{
    "1_1": "Video lectures Admin lecture: Practical arrangements. What will happen when? This lecture: Overview of content. What is the course about? General advice for viewing this and later lectures . . . (cid:73) Each lecture: several videos organized into a playlist. (Average 50-60 minutes.) (cid:73) Mix of videos from this year and last year. (cid:73) Captions: may sometimes correct small errors or omissions. (cid:73) Quiz questions within lectures: for your private use only. (cid:73) Do post questions or comments on the lectures to Piazza, mentioning lecture and slide number. Use fridayqs to submit questions for live Q+A sessions. IADS Lecture 1 Slide 2 ",
    "1_2": "What are algorithms and data structures? Informatics 2 \u2013 Introduction to Algorithms and Data Structures Algorithms are basically methods or recipes for solving various problems. To write a program to solve some problem, we first need to know a suitable algorithm. Data structures are ways of storing or representing data that make it easy to manipulate. Again, to write a program that works with certain data, we first need to decide how this data should be stored and structured. IADS Lecture 1 Slide 3 ",
    "1_3": "Tasks calling for algorithms How would you efficiently . . . (cid:73) Sort 1000000000 names into alphabetical order? (Databases) (cid:73) Visit all web pages reachable from a given starting page? (Search engines) (cid:73) Find the shortest/fastest/cheapest route from A to B? (SatNav) (cid:73) Find the longest substring shared by two (long) strings? (Genetics) (cid:73) Tell whether a 100-digit number is prime or not? (Cryptography) In some cases there\u2019s an \u2018obvious\u2019 method. Often there\u2019s a non-obvious method that\u2019s much more efficient. IADS Lecture 1 Slide 4 ",
    "1_4": "Problems, algorithms, programs IADS Lecture 1 Slide 5 ",
    "1_5": "There were algorithms before there were computers \u2018Algorithms\u2019 are so named after Muhammad al-Khw\u00afarizm\u00af\u0131, a 9th century Persian mathematician who wrote an important book on methods for arithmetic in decimal notation. (E.g. +, \u2212, long \u00d7, long /, Even earlier, there was Euclid\u2019s greatest common divisor algorithm: \u221a .) GCD (4851, 840) = GCD (840, 651) = GCD (651, 189) = GCD (189, 84) = GCD (84, 21) = 21. But now that we have computers, algorithms are everywhere! IADS Lecture 1 Slide 6 ",
    "1_6": "Quiz question Which of the following is not a true statement about algorithms? 1. Typically, the same algorithm can be implemented in many di\ufb00erent programming languages. 2. Some old algorithms are still considered to be very efficient. 3. Some algorithms work OK on small examples, but don\u2019t scale up to larger ones. 4. In order to design an algorithm for tackling some problem, you first need to write a program that solves it. IADS Lecture 1 Slide 7 ",
    "1_7": "Data structures How might you store a set of numbers (e.g. {5, 23, 3, 14, 2}) in a computer? In an array (sorted or not)? As a linked list? As a tree? Which of these make it easy . . . (cid:73) To test whether a given number (e.g. 11) is in the set? (cid:73) To add or remove a member of the set? Our choice of data structure may depend on which of these operations are most important for us. IADS Lecture 1 Slide 8 ",
    "1_8": "What makes computers go faster? Combination of . . . (cid:73) Hardware technology (e.g. processor speeds) (cid:73) Parallel processing (doing several things at once) (cid:73) Compiler techniques (e.g. optimization of machine code) (cid:73) Advances in algorithms / data structures. Even on \u2018old\u2019 algorithmic problems, new advances are still being made, sometimes leading to dramatic speed-ups on practical tasks. (cid:73) 2017: Major breakthrough in minwise hashing problem (from 1997) has led to order-of-magnitude improvement in near-duplicate detection, e.g. in search engines (Wang/Wang/Shrivastava/Ryu). (cid:73) 2018: Ideas of adaptive sampling give exponential speed-up on many optimization problems, e.g. taxi dispatch (Singer/Balkanski). In this sense, algorithms / data structures are a technology. People like GoogleTM certainly care about them, and ask questions about them at internship interviews! IADS Lecture 1 Slide 9 ",
    "1_9": "Rough outline of course (order slightly simplified) Approach via asymptotic analysis. (cid:73) Simple examples of efficient/inefficient algorithms. (cid:73) How can we measure how \u2018good\u2019 an algorithm is? (cid:73) Sorting algorithms: InsertSort, MergeSort, QuickSort, . . . (cid:73) Basic data structures: Ways of implementing lists, stacks, (cid:73) Algorithms on graphs: depth-first and breadth-first search, (cid:73) Dynamic programming: A way to avoid repeating work. (cid:73) Algorithms/data structures for language processing (e.g. of Java or Python source code). Grammars, syntax, parsing. (cid:73) What are the limits of algorithms and computation? Applications, e.g. seam carving for images. queues, sets, dictionaries, . . . topological sorting, shortest paths. Glance at complexity theory (intractable problems, P vs. NP) and computability theory (unsolvable problems, Turing machines, halting problem). IADS Lecture 1 Slide 10 ",
    "1_10": "Programming thread: Python The course will emphasize the relevance of algorithms/data struc- tures to practical programming. We\u2019ll be using Python as our programming language, and one of our goals is to teach you this. (Used in many later-year courses). We\u2019ll issue Python lab sheets, for you to work through at the computer at your own speed. Customized to this course: tie in closely with lecture material. Contain exercises for your own use (not submitted or marked). Online support available to help you with these sheets. Two of the three courseworks will involve Python programming. IADS Lecture 1 Slide 11 ",
    "1_11": "Other resources Learn page: reached from www.learn.ed.ac.uk. Everything will be made available here: lecture videos and slides, tutorial sheets, lab sheets, courseworks, link to discussion forum. Textbooks: (cid:73) Cormen, Leiserson, Rivest and Stein, Introduction to Algorithms (3rd edition). The classic. Also useful for Year 3 ADS; wealth of more advanced material. (cid:73) Goodrich, Goldwasser and Tamassia, Data Structures and Algorithms in Python. Programmer-oriented: starts with intro to Python! Closely matches first 2/3 of course. (cid:73) Sedgewick and Wayne, Algorithms (2016 edition). Java-based. (cid:73) Anything else you find helpful? Let us know! Reading for this lecture: CLRS Chapter 1. IADS Lecture 1 Slide 12 ",
    "1_12": "ENJOY THE COURSE! IADS Lecture 1 Slide 13 ",
    "2_1": "Inefficient vs. efficient algorithms Goal of lecture: Introduce some simple and important examples of algorithms, illustrating the di\ufb00erence between \u2018efficient\u2019 and \u2018inefficient\u2019 solutions. Problem 1 (toy example): Given a large decimal whole number n, compute n mod 9 (i.e. the remainder on dividing n by 9). Method A: Do division by school method; note the remainder. 4 1 6 4 7 3 5 2 5 3 7 7 2 8 9 9 ) 3 71458426631472248336965268087 rem 6 This is the \u2018obvious\u2019 method: no cleverness involved. IADS Lecture 2 Slide 2 ",
    "2_2": "Alternative method Method B: Add the digits of n to get a new number n(cid:48). Do the same to n(cid:48); repeat till we get down to a single digit d, If d = 9, answer is 0; otherwise answer is d. E.g. for n = 3748261728395607: 3+7+4+8+2+6+1+7+2+8+3+9+5+6+0+7 = 78 7 + 8 = 15 1 + 5 = 6 Why does this work? E.g. for a 4-digit number written abcd: 1000a + 100b + 10c + d = (999a + a) + (99b + b) + (9c + c) + d \u2261 a + b + c + d (mod 9) IADS Lecture 2 Slide 3 ",
    "2_3": "Comparing methods A and B Advantages of method B: (cid:73) Faster, at least for humans (though not spectacularly so). (cid:73) More \ufb02exible: can add digits in whatever order you like. (cid:73) Can be parallelized (you add first 8 digits, I\u2019ll add last 8). Moral: Using mathematical insight, improvements over the \u2018obvious\u2019 algorithm may be possible. Improved algorithm may be \u2018non-obvious\u2019 and may need justifying. IADS Lecture 2 Slide 4 ",
    "2_4": "Modular exponentiation Problem 2: Given (large) whole numbers a, n, m, compute an mod m. E.g. 210 mod 17 = 1024 mod 17 = 4. Believe it or not, this problem is absolutely fundamental to modern cryptosystems (e.g. RSA, as explained in DMP course). Method A: Literally compute an, then reduce modulo m. (cid:73) If e.g. a = 3, n = 123456789012345678901234, then an won\u2019t even fit in memory. (cid:73) In any case, working with very big numbers is time-consuming. IADS Lecture 2 Slide 5 ",
    "2_5": "Modular exponentiation, continued Method B: Start from a. Do (n \u2212 1) multiplications by a, but reduce mod m each time. Works because: (x \u00d7 y ) mod m = ((x mod m) \u00d7 (y mod m)) mod m E.g. for 210 mod 17: 2 \u00d7 2 = 4, 4 \u00d7 2 = 8, 8 \u00d7 2 = 16, 16 \u00d7 2 = 32 \u2261 15, 15 \u00d7 2 = 30 \u2261 13, 13 \u00d7 2 = 26 \u2261 9, 9 \u00d7 2 = 18 \u2261 1, 1 \u00d7 2 = 2, 2 \u00d7 2 = 4. (cid:73) Now numbers never get bigger than am. (cid:73) But still impractical if n = 123456789012345678901234. IADS Lecture 2 Slide 6 ",
    "2_6": "Fast modular exponentiation Method C: Notice that it\u2019s easy to compute e = an mod m if we\u2019ve already computed d = a(cid:98)n/2(cid:99) mod m: (cid:73) If n is even, take e = (d \u00d7 d) mod m. (cid:73) If n is odd, take e = (d \u00d7 d \u00d7 a) mod m. This suggests the following recursive algorithm: Expmod (a,n,m): # Computes an mod m if n=0 then return 1 else d = Expmod (a,(cid:98)n/2(cid:99),m) if n is even return (d \u00d7 d) mod m else return (d \u00d7 d \u00d7 a) mod m (Example of pseudocode: informal mix of programming constructs, math notation, and English. Useful for expressing algorithms in a readable way.) IADS Lecture 2 Slide 7 ",
    "2_7": "Example of Method C Expmod (a,n,m): if n=0 then return 1 else d = Expmod (a,(cid:98)n/2(cid:99),m) if n is even return (d \u00d7 d) mod m else return (d \u00d7 d \u00d7 a) mod m Imagine each evaluation of Expmod is done by a di\ufb00erent \u2018person\u2019: Us to A: A to B: B to C: C to D: D to E: E to D: D to C: C to B: B to A: A to us: What\u2019s Expmod (2,10,17) ? What\u2019s Expmod (2,5,17) ? What\u2019s Expmod (2,2,17) ? What\u2019s Expmod (2,1,17) ? What\u2019s Expmod (2,0,17)? 1 1 \u00d7 1 \u00d7 2 mod 17 = 2 2 \u00d7 2 mod 17 = 4 4 \u00d7 4 \u00d7 2 mod 17 = 15 15 \u00d7 15 mod 17 = 4. This is feasible even when a, n, m are large (say \u223c 1000 digits). IADS Lecture 2 Slide 8 ",
    "2_8": "Some Python experiments Time in milliseconds to compute 3n mod 2n (on my laptop): n Method A Method B Method C min(A,B)/C 10 100 1000 10000 100000 1000000 10000000 100000000 1000000000 .0027 .0041 .0092 .133 2.81 101. 3986. 149000 crashed 0.49 0.58 1.21 13.3 176. 5941. 68700. 587000. 9680000. \u2014 .0051 .0134 .1229 2.41 10.46 127. 1168. 11150 213000 \u2014 .0055 .0071 .0076 .010 .016 .017 .017 .019 .022 .656 10100 \u2014 Key idea: It\u2019s not just that my Python program for C is the best. Rather, the algorithm itself is vastly, fundamentally superior. How do we make this idea precise? IADS Lecture 2 Slide 9 ",
    "2_9": "Digression: Primality testing Fermat\u2019s little theorem: If n is prime and 0 < a < n, then an\u22121 mod n = 1. [Proved in DMP.] Application: Let n be the following 270-digit number. 412023436986659543855531365332575948179811699844327982845455 626433876445565248426198098870423161841879261420247188869492 560931776375033421130982397485150944909106910269861031862704 114880866970564902903653658867433731720813104105190864254793 282601391257624033946373269391 My Python \u2018Program C\u2019 takes <5 ms to discover that 2n\u22121 mod n = \u00b7\u00b7\u00b7 (cid:54)= 1. Conclusion: n is not prime. So what are its factors? If you knew, you\u2019d be (slightly) famous. This is RSA-896, not yet cracked. ($75,000 prize sadly withdrawn!) Note: This isn\u2019t a perfect primality test: a few non-primes (e.g. 561) masquerade as primes. But for big numbers, error probability is very small \u2014 and by refining the test, can be made even smaller (Miller-Rabin test). IADS Lecture 2 Slide 10 ",
    "2_10": "Insert-sort Problem 3: Given an array A containing n whole numbers, construct an array B containing the same n numbers in non-decreasing order. Method A (\u2018obvious\u2019): Go through elements of A one by one. Copy them into B, filling B from left to right, and inserting each element in its correct position. IADS Lecture 2 Slide 11 ",
    "2_11": "Insert-sort: in-place version Actually, don\u2019t need a separate array B: can do everything within A itself (in-place sorting). Just need to be able to hold one number \u2018in our hand\u2019 at any given time. In pseudocode: InsertSort(A): for i = 1 to |A|\u22121 # |A| means size of A x = A[i] j = i\u22121 while j \u2265 0 and A[j] > x A[j+1] = A[j] j = j\u22121 A[j+1] = x IADS Lecture 2 Slide 12 ",
    "2_12": "Merge-sort Method B (less obvious). Split A into two halves. Sort these separately, then merge the results. Merge (B,C): allocate D of size |B| + |C| i = j = 0 for k = 0 to |D|\u22121 if B[i] < C[j] # Convention: \u221e if index out of range D[k] = B[i], i = i+1 D[k] = C[j], j = j+1 else return D IADS Lecture 2 Slide 13 ",
    "2_13": "Merge-sort: Recursive application of Merge IADS Lecture 2 Slide 14 ",
    "2_14": "Merge-sort continued A recursive sorting algorithm: MergeSort (A,m,n): if n\u2212m = 1 # sorts A[m], A[m+1], . . . , A[n\u22121] # returning result in an array D of size n\u2212m else return [ A(m) ] p = (cid:98)(m+n)/2(cid:99) B = MergeSort (A,m,p) C = MergeSort (A,p,n) D = Merge (B,C) return D MergeSortAll (A): return MergeSort (A,0,|A|) IADS Lecture 2 Slide 15 ",
    "2_15": "Python again Time in milliseconds to sort a list of length n. (Entries were random whole numbers < n2.) n 10 100 1000 10000 100000 1000000 10000000 100000000 Insert-sort Merge-sort .023 .97 69.8 8210. 906000. \u2013 \u2013 \u2013 .068 .74 7.9 76.2 1080. 13300. 158000. 2619000. Speedup 0.34 1.31 8.84 107. 839. So merge-sort seems fundamentally superior (as regards runtime). Again, how can we make this precise? And why is merge-sort so much better? What\u2019s going on here? Will explore this next time. IADS Lecture 2 Slide 16 ",
    "2_16": "Reading (cid:73) Insert sort: CLRS 2.1 (cid:73) Merge sort: CLRS 2.3 (cid:73) Modular exponentiation: CLRS 31.6, second half (cid:73) [RSA challenge numbers: good Wikipedia pages.] Today\u2019s music: Fryderyk Chopin, Minute Waltz IADS Lecture 2 Slide 17 ",
    "3_1": "Outline Goal of Lectures 3,4,5: (cid:73) Introduce asymptotic analysis, the core mathematical theory used in this course. Centres around a certain \u2018Gang of Five\u2019: o O \u0398 \u2126 \u03c9 (cid:73) Apply this theory to InsertSort and MergeSort. Purpose of the theory: Way of making precise, quantitative statements about efficiency properties of algorithms themselves. (E.g. What do all implementations of MergeSort have in common?) Note: These ideas may take a while to master \u2013 don\u2019t worry! This lecture: In what sense is MergeSort \u2018fundamentally faster\u2019 than InsertSort? o and \u03c9. IADS Lecture 3 Slide 2 ",
    "3_2": "Comparing runtimes for InsertSort and MergeSort Take some specific implementations of InsertSort and MergeSort. Broadly, we want to consider . . . TI (n) = time taken by InsertSort on a list of length n (in ms) TM (n) = time taken by MergeSort on a list of length n Which list of length n? Time may vary widely between lists! Will come back to this. For now, take TI (n), TM (n) to be the worst-case (i.e. maximum) times for a list of length n. Could then plot a graph (schematic only): IADS Lecture 3 Slide 3 ",
    "3_3": "Comparing TI and TM How can we capture our intuition \u2018TI grows much faster than TM \u2019 ? Attempt 1: \u2200n. TM (n) < TI (n). Not true! We\u2019ve seen that for small n, InsertSort is faster. Really want to say that MergeSort is eventually faster. Attempt 2: \u2203N.\u2200n \u2265 N. TM (n) < TI (n). True. E.g. N = 100 would do here. But doesn\u2019t capture the essential di\ufb00erence . . . IADS Lecture 3 Slide 4 ",
    "3_4": "Comparing growth rates Attempt 3: Idea is that we expect that any impl of MergeSort will eventually beat any impl of InsertSort. E.g. suppose we gave InsertSort an unfair advantage by running it on a machine 100 times faster. Even TI (n)/100 would eventually overtake TM (n): In symbols: \u2203N.\u2200n \u2265 N. TM (n) < 0.01TI (n). (E.g. N = 100000 would do here.) Question: What if we replaced 0.01 by 0.0001? Or by 0.000001? IADS Lecture 3 Slide 5 ",
    "3_5": "Growth rates and \u2018little o\u2019 Intuition (will justify later): For any handicap factor c, however close to zero, cTI (n) will eventually break out and overtake TM : \u2200c > 0. \u2203N. \u2200n \u2265 N. TM (n) < cTI (n) We express this by saying TM is o(TI ). Can read this as: \u2018TM is slower-growing than\u2019 or \u2018asymptotically smaller than TI \u2019. In general, we say f is o(g ) if \u2200c > 0. \u2203N. \u2200n \u2265 N. f (n) < cg (n) (Here f , g : N \u2192 R\u22650, c ranges over R, and N, n range over N.) Equivalent to saying g (n)/f (n) \u2192 \u221e as n \u2192 \u221e (if f : N \u2192 R>0). IADS Lecture 3 Slide 6 ",
    "3_6": "o-notation: Simple examples Will come back to InsertSort and MergeSort later. Meanwhile, some simpler examples of o. Example 1: Is it true that n2 is o(n3)? YES! Informal justification: The ratio n3/n2 is n, which (trivially!) tends to \u221e as n tends to \u221e. Rigorous justification: Want to show that the o formula is satisfied: \u2200c > 0. \u2203N. \u2200n \u2265 N. n2 < cn3 Suppose we\u2019re given some c > 0. Need to pick a suitable N. Take any N > 1/c. Then for all n \u2265 N, we have cn3 = cn.n2 \u2265 cN.n2 > c(1/c)n2 = n2 (Idea: If n > 1/c, the extra factor n will compensate for the c.) IADS Lecture 3 Slide 7 ",
    "3_7": "Examples of o-notation, continued \u221a Example 2: Is it true that 100 n is o(n)? \u221a Informal justification: The ratio n/(100 to \u221e as n tends to \u221e. Rigorous justification: Want to show that the o formula is satisfied: n/100, which tends YES! n) is \u221a \u221a \u2200c > 0. \u2203N. \u2200n \u2265 N. 100 n < cn Suppose we\u2019re given some c > 0. Need to pick a suitable N. Take any N > 10000/c 2. Then for all n \u2265 N, we have \u221a \u221a cn = c n \u221a n \u2265 c \u221a \u221a n > c(100/c) N \u221a n = 100 n How did we pick that 10000/c 2? \u221a E.g. by working backwards from the requirement n/(100 n) > 1/c. IADS Lecture 3 Slide 8 ",
    "3_8": "Examples of o-notation, continued Example 3: Is it true that n + 1000000 is o(6n)? NO! Informal justification: Even though the ratio 6n/(n + 1000000) continues to increase as n tends to \u221e, it never exceeds 6, so doesn\u2019t tend to \u221e. Rigorous justification: Want to show the negation of the o formula: \u00ac (\u2200c > 0. \u2203N. \u2200n \u2265 N. n + 1000000 < c.6n) which is equivalent to \u2203c > 0. \u2200N. \u2203n \u2265 N. n + 1000000 \u2265 c.6n We can take c = 1/7. It\u2019s then true for any n \u2265 0 that n + 1000000 > n \u2265 6n/7 = c.6n So it\u2019s clear that \u2200N.\u2203n \u2265 N. n + 1000000 \u2265 c.6n (given N, can just take n = N). IADS Lecture 3 Slide 9 ",
    "3_9": "What is \u2018o(g )\u2019 officially? Officially, o(g ) is a set: namely, the set of all f that \u2018are o(g )\u2019. o(g ) = {f : N \u2192 R\u22650 | \u2200c > 0. \u2203N. \u2200n \u2265 N. f (n) < cg (n)} So, \u2018f is o(g )\u2019 technically means f \u2208 o(g ). Common convention: Write \u2018o(g )\u2019 to mean \u2018some (unspecified) function in the set o(g )\u2019. E.g. f = o(g ) , f (n) = 3n2 + o(n) Needs care: e.g. n = o(n2) and 2n = o(n2) don\u2019t imply n = 2n ! But many useful laws are valid, e.g. o(g ) + o(g ) = o(g ) which strictly means \u2018if f \u2208 o(g ) and f (cid:48) \u2208 o(g ), then f +f (cid:48) \u2208 o(g )\u2019. (Exercise if you like maths: Prove this from the definition of o.) IADS Lecture 3 Slide 10 ",
    "3_10": "Reducing clutter using o Asymptotic notation is useful when we\u2019re only interested in the broad headlines of how some function behaves. E.g. Can read 3n2 + o(n) as \u20183n2 plus small change.\u2019 Reduces clutter and simplifies calculations! Example: How does the following behave for large n? \u221a n / lg n) + 12) \u221a (3n + 5 n + 17 lg n) (4n + ( (In this course, lg means logarithm to base 2.) Rather than expanding this in full, can reason as follows: (3n + o(n) + o(n))(4n + o(n) + o(n)) = (3n + o(n))(4n + o(n)) = 12n2 + o(3n2) + o(4n2) + o(n2) = 12n2 + o(n2) (where every step can be rigorously justified). IADS Lecture 3 Slide 11 ",
    "3_11": "Some key points (cid:73) Saying f = o(g ) gives just the main headlines of how f and g are related: \u2018In the limit, f is vanishingly small relative to g \u2019. Often, this is all we care about. (cid:73) f = o(g ) makes a robust statement about f , g . E.g. una\ufb00ected by scaling: f = o(g ) \u21d4 3f = o(0.2g ). implementations of MergeSort/InsertSort. (cid:73) So can expect that e.g. \u2018TM = o(TI )\u2019 will remain true for any (cid:73) Use of o can reduce clutter and simplify calculations. (cid:73) But without sacrificing mathematical rigour: \u2018f = o(g )\u2019 has a precisely defined meaning. General advice: Sketch graphs to understand what\u2019s going on! IADS Lecture 3 Slide 12 ",
    "3_12": "And finally: \u03c9 \u03c9 is dual to o. Recall that f = o(g ) means: \u2200c > 0. \u2203N. \u2200n \u2265 N. f (n) < cg (n) (\u2018f is asymptotically smaller than / grows slower than g \u2019). By contrast, read f = \u03c9(g ) as saying: \u2018f is asympotically larger than / grows faster than g \u2019). Formal definition: f is \u03c9(g ) if \u2200C > 0. \u2203N. \u2200n \u2265 N. f (n) > Cg (n) (\u2018However much we scale g up by, f will eventually overtake it.\u2019) For purpose of comparing f and g , scaling g \u2018up\u2019 by C has same e\ufb00ect as scaling f \u2018down\u2019 by c = 1/C . So easy to show: f = \u03c9(g ) if and only if g = o(f ) (Compare: x > y if and only if y < x.) We\u2019ll tend to use o more than \u03c9. IADS Lecture 3 Slide 13 ",
    "3_13": "Next time: O, \u2126, \u0398. (Most presentations start with these!) Reading for Lectures 3 and 4: CLRS Chapter 3, GGT Sections 3.3, 3.4 Today\u2019s music: J.S. Bach, Toccata in D minor. IADS Lecture 3 Slide 14 ",
    "4_1": "Where we\u2019re heading . . . Recall our runtime functions TI , TM for InsertSort, MergeSort. We\u2019ve seen that TM grows slowly relative to TI : TM = o(TI ). Can we place growth rates of TI , TM on some absolute scale? E.g. consider the following hierarchy of \u2018simple\u2019 functions: \u221a f2(n) = n f5(n) = n2 f8(n) = 22n f1(n) = lg n f4(n) = n lg n f7(n) = 2n f0(n) = 1 f3(n) = n f6(n) = n3 \u00b7\u00b7\u00b7 Here f0 \u2208 o(f1), f1 \u2208 o(f2), . . . Which of the above functions do TI and TM most closely \u2018resemble\u2019 in their essential growth rate? IADS Lecture 4 Slide 2 ",
    "4_2": "The big guys: O, \u2126, \u0398 We\u2019re going to define a relation f is \u0398(g ) Read as \u2018f has same essential growth rate as g \u2019. Often used to classify \u2018complicated\u2019 functions via \u2018simple\u2019 ones. E.g. it will turn out that TI is \u0398(n2), and TM is \u0398(n lg n). Approach: First define f is O(g ) f is \u2126(g ) \u2018f grows no faster than g \u2019 \u2018f grows no slower than g \u2019 Then say: f is \u0398(g ) \u21d0\u21d2 f is O(g ) and f is \u2126(g ). IADS Lecture 4 Slide 3 ",
    "4_3": "Big O \u2018small\u2019 values of n, The spirit of asymptotics is that: (cid:73) we only care about behaviour \u2018in the limit\u2019 \u2014 can discard (cid:73) constant scaling factors are washed out. So let\u2019s say f grows no faster than g , if f is eventually bounded above by some (sufficiently large) multiple Cg of g : \u2203C > 0. \u2203N. \u2200n \u2265 N. f (n) \u2264 Cg (n) Write as f is O(g ), and call g an asymptotic upper bound for f . IADS Lecture 4 Slide 4 ",
    "4_4": "Big O: an example Suppose f (n) = 3n + \u221a n and g (n) = n. Claim: f is O(g ). Or more simply, f is O(n). Proof: Need to show \u2203C . \u2203N. \u2200n \u2265 N. 3n + \u221a n \u2264 Cn Take C = 4, N = 1. Then for all n \u2265 N = 1, we have Intuition: 3n is the \u2018dominant\u2019 term; \u221a n is \u2018small change\u2019. \u221a n \u2264 n, so n \u2264 4n = Cn \u221a 3n + IADS Lecture 4 Slide 5 ",
    "4_5": "Comparing o and O We\u2019ve defined: f is o(g ) means \u2200c > 0. \u2203N. \u2200n \u2265 N. f (n) < cg (n) f is O(g ) means \u2203C > 0. \u2203N. \u2200n \u2265 N. f (n) \u2264 Cg (n) (cid:73) For o we require that any multiple of g eventually overtakes f . (cid:73) For O it\u2019s enough that some multiple of g does. So f = o(g ) implies f = O(g ). But not conversely: e.g. f = O(f ) for any f , but f is never o(f ). Loosely, can think of o as like <, O as like \u2264. Notation: Again, O(g ) is officially a set: O(g ) = {f | \u2203C \u2265 0. \u2203N. \u2200n \u2265 N. f (n) \u2264 Cg (n)} But common to write e.g. f = O(g ) for f \u2208 O(g ). IADS Lecture 4 Slide 6 ",
    "4_6": "Big O: more examples Example 1: Let f (n) = (5n + 4)(7n + 100). Is f = O(n2)? YES! Informal justification: The dominant term is 35n2; the rest is small change that is clearly o(n2). So f is O(n2). Rigorous justification: Want to show: \u2203C . \u2203N. \u2200n \u2265 N. (5n + 4)(7n + 100) \u2264 Cn2 Note that (cid:73) 5n + 4 \u2264 6n once n \u2265 4 (cid:73) 7n + 100 \u2264 8n once n \u2265 100. So for all n \u2265 100, we have f (n) \u2264 48n2. In other words, C = 48, N = 100 will work. IADS Lecture 4 Slide 7 ",
    "4_7": "A bit of freedom here . . . We wanted to show \u2203C . \u2203N. \u2200n \u2265 N. (5n + 4)(7n + 100) \u2264 Cn2 We did this by picking C = 48, N = 100. There\u2019s some freedom of choice here. By picking a larger C , can often get away with a smaller N. E.g. once n \u2265 4, have 5n + 4 \u2264 6n and 7n + 100 \u2264 32n. So could equally well take C = 6 \u00d7 32 = 192, N = 4. Advice: Make life easy for yourself! IADS Lecture 4 Slide 8 ",
    "4_8": "More examples Example 2: Let f (n) = (5n + 4)(7n + 100). Is f = O(n3)? YES! We\u2019ve already shown So certainly \u2200n \u2265 100. f (n) \u2264 48n2 \u2200n \u2265 100. f (n) \u2264 48n3 Here we say O(n3) is an asymptotic upper bound for f , though not a tight upper bound. We\u2019d write f = \u0398(n3) to mean n3 was an asymptotic upper and lower bound (hence tight). Not true here! Some authors are less precise in distinguishing O and \u0398 (see CLRS, end of Chapter 3). IADS Lecture 4 Slide 9 ",
    "4_9": "More examples Example 3: Is 22n = O(2n)? NO! Informal justification: The ratio 22n/2n is 2n, which tends to \u221e and so will eventually exceed any given constant C . In fact, 22n = \u03c9(2n). Rigorous justification: Want to show: \u00ac (\u2203C > 0. \u2203N. \u2200n \u2265 N. 22n \u2264 C .2n) in other words \u2200C > 0. \u2200N. \u2203n \u2265 N. 22n > C .2n Given any C > 0 and N, take any n > max(N, lg C ). Then 2n > C , so 22n > C .2n. Moral: Do \u2018constant factors\u2019 matter? Depends where they occur! IADS Lecture 4 Slide 10 ",
    "4_10": "Big O: final example Example 4: Is lg(n7) = O(lg n)? YES! Note that lg(n7) = 7 lg n. So C = 7, N = 1 will do. IADS Lecture 4 Slide 11 ",
    "4_11": "Big \u2126 \u2126 is dual to O. Read f is \u2126(g ) as: \u2018f grows no slower than g \u2019, or \u2018g is an asymptotic lower bound for f \u2019. E.g. for some runtime function T (n): (cid:73) T (n) = O(g ) says runtime is not essentially worse than g (n), (cid:73) T (n) = \u2126(g ) says runtime is not essentially better than g (n). f = \u2126(g ) says f is eventually bounded below by some (sufficiently small) multiple cg of g : \u2203c > 0. \u2203N. \u2200n \u2265 N. cg (n) \u2264 f (n) Not hard to show f = \u2126(g ) \u21d0\u21d2 g = O(f ). IADS Lecture 4 Slide 12 ",
    "4_12": "Big \u2126: example Is it true that n \u2212 \u221a is large. So growth rate of n \u2212 \u221a Informal justification: n is \u2126(n)? \u221a YES! n becomes negligible relative to n when n n is essentially that of n. Rigorous justification: Want to show: \u2203c. \u2203N. \u2200n \u2265 N. cn \u2264 n \u2212 \u221a n Take c = 1/2, N = 4. Then for all n \u2265 N = 4, we have \u221a n \u2264 n/2, so n \u2212 \u221a n \u2265 n \u2212 n/2 = n/2 = cn IADS Lecture 4 Slide 13 ",
    "4_13": "Big \u0398 Can now capture the idea that f and g have \u2018essentially the same growth rate\u2019. Say f is \u0398(g ) (or g is an asymptotically tight bound for f ) if both f \u2208 O(g ) and f \u2208 \u2126(g ). Equivalently, f \u2208 \u0398(g ) if and only if \u2203c1, c2 > 0. \u2203N. \u2200n \u2265 N. c1g (n) \u2264 f (n) \u2264 c2g (n) Note also that f = \u0398(g ) \u21d0\u21d2 g = \u0398(f ). IADS Lecture 4 Slide 14 ",
    "4_14": "Examples of \u0398 For each of the following functions f , identify some \u2018simple\u2019 g such that f = \u0398(g ). Example 1: f (n) = 3n2 \u2212 2n + 19. The dominant term is 3n2, the rest is small change. So f (n) will eventually be sandwiched between 2n2 and 4n2. (Specifically, can take e.g. c1 = 2, c2 = 4, N = 5.) Example 2: f (n) = 5 \u2212 4/n. That is, we\u2019re taking our \u2018g \u2019 to be the constant function g (n) = 1. Then for any n \u2265 1, we have Answer: f (n) = \u0398(n2). Answer: f (n) = \u0398(1). 1.g (n) = 1 \u2264 5 \u2212 4/n \u2264 5 = 5.g (n) So taking c1 = 1, c2 = 5, N = 1 will work. IADS Lecture 4 Slide 15 ",
    "4_15": "Harder example Identify some simple g such that f = \u0398(g ). i=11/i. Example 3: f (n) = \u03a3n E.g. f (4) = 1 + 1/2 + 1/3 + 1/4 = 2 1 12 . Answer: f (n) = \u0398(ln n). Idea: f (n) is close to(cid:82) n 1 (1/x)dx, which is ln n. E.g. for n = 4: IADS Lecture 4 Slide 16 ",
    "4_16": "Growth rates and algorithms Let\u2019s return to an earlier question. Suppose each implementation J of (say) MergeSort yields some runtime function TJ . Question: What do we expect all these TJ to have in common? Answer: Same growth rate! \u2200J, J(cid:48) implementing MergeSort. TJ = \u0398(TJ(cid:48)) Will justify this next time, and furthermore see that \u2200J implementing MergeSort. TJ = \u0398(n lg n) Idea: Asymptotic notation can crisply express essential properties of algorithms, abstracting away from implementation detail. Of the Gang of Five, we\u2019ll meet O and \u0398 most often. IADS Lecture 4 Slide 17 ",
    "4_17": "Reading (same as last time): CLRS Chapter 3, GGT Sections 3.3, 3.4 Today\u2019s music: Richard Strauss, Also sprach Zarathustra. IADS Lecture 4 Slide 18 ",
    "5_1": "Algorithms and cost models We\u2019re interested in the cost of various algorithms, for various notions of cost. (Runtime, memory use, disk operations, . . . ). To analyse this, need some cost model: i.e. some definition of how we intend cost to be measured. Di\ufb00erent cost models are useful for di\ufb00erent purposes. We\u2019ll initially consider runtime cost. But even here, di\ufb00erent cost models are possible. E.g. for sorting algorithms, might measure. . . 1. number of comparisons (<) between items, 2. number of \u2018basic steps\u2019 performed \u2013 e.g. \u2018machine instructions\u2019 for some (idealized) machine model. This lecture: Start with 1, then move towards 2. IADS Lecture 5 Slide 2 ",
    "5_2": "Best case, worst case, average case Often want to estimate runtime on \u2018inputs of size n\u2019. (E.g. time taken to sort lists of length n.) Typically, di\ufb00erent inputs of size n give di\ufb00erent runtimes! However, the best case, worst case and (sometimes) average case times give well-defined functions we can talk about. IADS Lecture 5 Slide 3 ",
    "5_3": "Best/worst case and asymptotic bounds that manifest this bad behaviour. Informally . . . (cid:73) Tworst = O(g ) says the worst-case runtime (hence any runtime) is essentially no worse than g . Can then just say \u2018runtime is O(g )\u2019. (cid:73) Tworst = \u2126(g ) says runtime can be as bad as g , i.e. there are inputs (cid:73) Tworst = \u0398(g ) says both these things. (cid:73) Tbest = \u2126(g ) says best-case runtime (hence any runtime) is essentially no better than g . Can then say \u2018runtime is \u2126(g )\u2019. (cid:73) Tbest = O(g ) says runtime can be as good as g . (cid:73) Tbest = \u0398(g ) says both these things. Moral: Don\u2019t confuse \u2018O/\u2126/\u0398\u2019 with \u2018best/worst/average\u2019 ! (See CLRS pages 48-49.) IADS Lecture 5 Slide 4 ",
    "5_4": "InsertSort: \u2018number of comparisons\u2019 analysis Pseudocode for InsertSort again (line numbers added): 0 1 2 3 4 5 6 7 InsertSort(A): for i = 1 to n\u22121 # write n for size of A x = A[i] j = i\u22121 while j \u2265 0 and A[j] > x A[j+1] = A[j] j = j\u22121 A[j+1] = x How many times is the \u2018>\u2019 on line 4 invoked? (cid:73) For each value of i, may consider execution of lines 2\u20137. This invokes > at most i times. (Loop starts at j = i\u22121 and stops at j = \u22121, if not before). (cid:73) But i itself runs from 1 to n\u22121 (line 1). (cid:73) So total number of \u2018>\u2019 ops is at most \u03a3n\u22121 i=1 i = O(n2). IADS Lecture 5 Slide 5 ",
    "5_5": "\u2018Worst case\u2019 number of comparisons We\u2019ve seen that \u03a3n\u22121 of comparisons. So can say InsertSort does O(n2) comparisons. i=1 i = n(n \u2212 1)/2 is an upper bound for number But is it ever actually this bad? I.e. is this upper bound attained? For any n, consider how InsertSort will behave on the input [n, n \u2212 1, . . . , 2, 1]. Recall the inner loop: 4 5 6 while j \u2265 0 and A[j] > x A[j+1] = A[j] j = j\u22121 Not hard to see that all the comparisons A[j] > x will yield True. So for each i, this \u2018j-loop\u2019 will run until j = \u22121. So there will be exactly n(n \u2212 1)/2 comparisons. Headline is that the worst-case time is \u2126(n2), hence \u0398(n2). IADS Lecture 5 Slide 6 ",
    "5_6": "What about \u2018best case\u2019 ? What\u2019s the smallest number of comparisons that InsertSort could possibly perform on a size n input? j = i\u22121 while j \u2265 0 and A[j] > x 3 4 5 6 A[j+1] = A[j] j = j\u22121 For each value of i, this j-loop will do at least one comparison (first time round, when j = i\u22121). So the whole program performs at least n\u22121 comparisons. Is this lower bound attained? Yes: when input is already sorted! So best-case number of comparisons is \u0398(n). Can now say: number of comparisons performed by InsertSort on size n inputs is O(n2) and \u2126(n). IADS Lecture 5 Slide 7 ",
    "5_7": "\u2018Average case\u2019 for InsertSort We\u2019ve looked at worst and best cases. But how many comparisons will InsertSort perform on average? For simplicity, assume input A is some permutation of n distinct elements x0 < . . . < xn\u22121, with all n! permutations \u2018equally likely\u2019. Let Tav (n) = average number of comparisons for these n! inputs. That is, if Pn is the set of n! orderings of {x0, x1, . . . , xn\u22121}, then ((cid:93) comparisons on input [p(0), . . . , p(n \u2212 1)]) (cid:88) Tav (n) = 1 n! p\u2208Pn It can be shown that Tav (n) = n2/4 \u2212 O(n) (compare Tworst(n) = n2/2 \u2212 O(n)). Anyway, Tav (n) = \u0398(n2). IADS Lecture 5 Slide 8 ",
    "5_8": "MergeSort: comparison analysis First recall the Merge operation for merging two already sorted arrays B and C , of combined length m (i.e. |B| + |C| = m). How many comparisons does this perform? (Assume we stop doing comparisons once B or C is used up.) Reasoning informally . . . (cid:73) At most m \u2212 1. Every comparison yields a new element for the result list D, and the very last element gets put in without a comparison. (cid:73) At least min(|B|,|C|). And if |B| and |C| di\ufb00er by at most 1 (as they will in MergeSort), this is m/2 \u2212 O(1). So can say that number of comparisons done by Merge (within MergeSort) is \u0398(m). Now what about MergeSort itself? IADS Lecture 5 Slide 9 ",
    "5_9": "Analysis of MergeSort, ctd. For simplicity, suppose first that we\u2019re sorting a list of size n = 2k . We\u2019ll reason informally. Recall our diagram: All the comparisons/merging happen in the bottom half. (cid:73) On each \u2018level\u2019, total number of elements is n. (cid:73) And for each merge, (cid:93)comparisons < (cid:93)elements involved. (cid:73) So total (cid:93)comparisons for merges on each level is < n. (cid:73) And there are lg n = k levels. So total (cid:93)comparisons is < n lg n. Even if n isn\u2019t a power of 2, can show with a little care that (cid:93)comparisons is < n(cid:100)lg n(cid:101), which is certainly O(n lg n). IADS Lecture 5 Slide 10 ",
    "5_10": "MergeSort: worst, best and average case What about a lower bound? We\u2019ve seen that on sorted lists of total size m, di\ufb00ering in size by at most 1, Merge requires \u2126(m) comparisons. Using this, can show that on lists of length n, MergeSort requires \u2126(n lg n) comparisons. (Requires some care!) So Tworst = O(n lg n), Tbest = \u2126(n lg n). Can immediately conclude that Tworst, Tbest, Tav are all \u0398(n lg n). (Shown without deriving exact formulae for Tworst, Tbest, Tav !) Summary: In worst and average cases, MergeSort is asymptotically better than InsertSort (n lg n = o(n2)). But InsertSort does better in best case (n = o(n lg n)). IADS Lecture 5 Slide 11 ",
    "5_11": "Measuring \u2018overall runtime\u2019 Consider again InsertSort (for integer arrays): # write n for size of A for i = 1 to n\u22121 0 InsertSort(A): 1 2 3 4 5 6 7 x = A[i] j = i\u22121 while j \u2265 0 and A[j] > x A[j+1] = A[j] j = j\u22121 A[j+1] = x Common to take number of line executions as measure of runtime. Broad justification: Think about how the pseudocode would be imple- mented on a typical Random Access Machine (RAM). (Think 32-bit or 64-bit computer . . . except that word size/number of words may be taken as large as required for the given input.) Claim: Each line execution takes \u0398(1) time (i.e. between two positive constants t < t(cid:48)). This implies that, for any such impl, total execution time = \u0398(number of line executions) Warning! Applies only when each line execution does just a bounded amount of work. E.g. What if \u2018>\u2019 is comparison for strings? IADS Lecture 5 Slide 12 ",
    "5_12": "Overall runtime of InsertSort and MergeSort Can do \u2018line execution\u2019 analyses of InsertSort and MergeSort using same ideas as before. E.g. In InsertSort, for each value of i, execution of lines 2\u20137 takes \u2264 3i + 3 line executions . . . In this case, this tells the same story as our previous analyses: InsertSort MergeSort \u0398(n lg n) \u0398(n lg n) \u0398(n lg n) Best \u0398(n) Worst \u0398(n2) Average \u0398(n2) IADS Lecture 5 Slide 13 ",
    "5_13": "Space complexity Let\u2019s also look brie\ufb02y at the memory requirements of our algorithms on size n inputs. Sensible to ignore the space occupied by the input array A, and consider the extra space needed. Easy to see that . . . (cid:73) \u2018External\u2019 InsertSort (putting result in new array B) requires \u0398(n) extra space. (cid:73) In-place InsertSort requires only \u0398(1) extra space (just i,j,x). (cid:73) MergeSort apparently requires \u0398(n log n) space (total size of arrays created). (cid:73) However, with more careful memory management, MergeSort can be implemented using just \u0398(n) extra space. Idea is that after doing D = Merge (B,C), space occupied by \u2018temporary\u2019 arrays B,C can be reclaimed. Headline: In-place InsertSort wins on space efficiency. IADS Lecture 5 Slide 14 ",
    "5_14": "\u2018End of Part 1\u2019 That completes our introduction to the general concepts that underpin this course. Next time: Start on data structures \u2014 beginning with how program data is organized in memory. Today\u2019s reading: CLRS 2.2. IADS Lecture 5 Slide 15 ",
    "6_1": "Representations of data Having seen a few algorithms, we now turn to data structures: i.e. ways of representing/structuring data in memory. In due course, we\u2019ll see how to implement our own data structures. But we start at the bottom, with the \u2018primitive\u2019 data structures that programming languages typically provide as built-in. Actually, we\u2019ll begin one step further back: How, in general, is program data organized in memory? Picture is broadly similar for most modern programming languages (Java, Python, Haskell, . . . ): Stack and Heap. Remember: simplifying a bit, we think of memory as consisting of words each with an address (i.e. location). Any address can be itself be stored in a single word \u2014 though there may be fewer addresses than word values. IADS Lecture 6 Slide 2 ",
    "6_2": "Typical organization of program data in memory IADS Lecture 6 Slide 3 ",
    "6_3": "Program memory: summary (cid:73) Values of program variables are stored on a stack, which grows and shrinks as variables come in and out of scope. (cid:73) Stack items are contiguously arranged in memory, so must have fixed size (e.g. 1 or 2 words). (cid:73) Typically, a stack item contains either a basic value (e.g. 561, True) or a reference to something on the heap. (cid:73) Heap objects can live anywhere in memory, be of any size, and may contain references to other heap objects. (cid:73) When heap objects are created (allocated), the memory manager will decide where to put them. But references to other objects can be changed later \u2013 so we can end up with a real mess! (cid:73) As execution proceeds, some heap objects may become unreachable. In many languages (e.g. Java, Python, Haskell), a garbage collector (i.e. memory recycler) detects this and reclaims the space. IADS Lecture 6 Slide 4 ",
    "6_4": "Details may vary . . . Our picture is mostly \u2018Java-like\u2019 (except for the mixed-type array). (cid:73) In Java, anything of reference type (including \u2018objects\u2019 and \u2018arrays\u2019) lives on the heap. In C, built-in \u2018arrays\u2019 live on the stack, and their size is static: an array variable A has an associated size fixed throughout its lifetime. (cid:73) Python also o\ufb00ers \u2018lists\u2019 and \u2018arrays\u2019, both implemented much like the heap array in our picture. (Di\ufb00erence: \u2018lists\u2019 allow mixed types.) In functional programming languages (Haskell, ML, Lisp), \u2018lists\u2019 are more typically implemented as linked lists. Java o\ufb00ers many classes for \u2018lists\u2019, e.g. ArrayList, LinkedList. (cid:73) In Java, all reference types have special value null (\u2018pointer to nowhere\u2019). Default initial value for any variable/field of ref type. Closest Python equivalent is None \u2013 this is actually a reference to a specific object, with no fields or methods. Idea: Once we have the basic picture, we have a framework for understanding such di\ufb00erences. IADS Lecture 6 Slide 5 ",
    "6_5": "Assignment by reference In Python or Java, assignment statements have the form variable = expression E.g. s = \"smiles\"[1:5] # assigns ref to s What does an expression actually evaluate to? Either a basic value or a reference to a heap object. (Or null in Java.) List example: This matters! Think what happens when we do L1[2] = 5. A heap object will be \u2018copied\u2019 only if we request it (e.g. using \u2018[:]\u2019 for lists in Python). Similarly in Java (copying often done by a clone() method). IADS Lecture 6 Slide 6 ",
    "6_6": "Shallow vs. deep cloning In Python, \u2018[:]\u2019 makes only a shallow clone: copies \u2018top level only\u2019. E.g. think about lists of lists: Again, think what happens with L1[2] = 5. For a deep clone (fresh copy of entire structure, with no sharing), we could in this case write L6 = [L1[:],L1[:],L1[:]]. In general, may need to write a (possibly recursive) program to deep-clone the data structures in question. IADS Lecture 6 Slide 7 ",
    "6_7": "Equality testing Equality testing can be . . . (cid:73) by reference (\u2018are the addresses the same?\u2019), or (cid:73) by content (\u2018is what we find at those addresses \u201cthe same\u201d?\u2019) In Python, is means reference eq, == means (deep) content eq. E.g. after commands in column 1, what does column 2 give? L1 = [1,2,3] L2 = L1 L3 = [L1,L1,L1] L4 = L1[:] L5 = L3[:] L6 = [L1[:],L1[:],L1[:]] L2 is L1 L4 is L1 L2 == L1 L4 == L1 L6 is L3 L6 == L3 # True # False # True # True # False # True Warning: In Java, == means reference equality! For content equality, typically use an .equals method. Strings are especially slippery. Use content equality! IADS Lecture 6 Slide 8 ",
    "6_8": "About those NullPointerExceptions Consider a Java expression of form expr .fieldname. E.g. X.age. The expr evaluates to a reference, or perhaps to null. But when we pass the \u2018.\u2019, we follow the reference to reach what it points to (dereferencing) . . . and so risk a NullPointerException if value of expr was null! Same goes for the \u2018.\u2019 in expr .methodname(arguments). E.g. X.name.length(). Understanding this (and drawing pictures) can go a long way towards rooting out those pesky NullPointerExceptions. IADS Lecture 6 Slide 9 ",
    "6_9": "Basic operations The following operations (among others) may all be assumed to work within constant time (i.e. \u0398(1) time): (cid:73) Reading and writing values of program variables (basic or ref type). n n = 341 X Y = X (cid:73) Accessing or updating a field in a given object (involves deref). X.age X.age = 51 X.name = s (cid:73) Accessing or updating an entry in a given array (may involve deref). (cid:73) Allocating a new object (e.g. of a given class) on the heap (not A[42] A[42] = 51 counting initialization of fields). (cid:73) Allocating a new array on the heap, not counting initialization of all X = new Person() its entries. IADS Lecture 6 Slide 10 ",
    "6_10": "Linked lists In Java/Python, Linked list cells would typically be simple objects, e.g. of class Cell, with fields called key, next and maybe prev. E.g. a doubly linked list: In functional languages, singly-linked lists are everywhere, but presentation may look more abstract. E.g. L.key written as L.next written as new Cell(x,L) written as null written as head L tail L cons(x,L) or x:L nil or [] Anyway, find nth element of a linked list L takes time \u0398(n). Reading: CLRS chapter 10, especially 10.2 and 10.3. IADS Lecture 6 Slide 11 ",
    "6_11": "IADS Lecture 6 Slide 12 ",
    "6_12": "Music: George Harrison, Here comes the sun Arranged and played by John Longley Photograph: Glen Etive from path to Lairig Gartain Kyriakos Kalorkoti (by kind permission) IADS Lecture 6 Slide 13 ",
    "7_1": "\u2018Lists\u2019 in general . . . We\u2019ve seen that \u2018lists\u2019 can be implemented in several ways, e.g. via arrays or linked lists. How might we compare these? Start by listing the operations we\u2019d like any impl to support. E.g. for (unsorted) lists of items of type X , might want operations get set cons : int \u2192 X : int \u2217 X \u2192 void : X \u2192 void append : X \u2192 void : int \u2217 X \u2192 void insert delete : int \u2192 void length : void \u2192 int Much like an interface in Java. # read item at given pos # write item at given pos # add item at start # add item at end IADS Lecture 7 Slide 2 ",
    "7_2": "Abstract interfaces, concrete implementations As in Java, we can consider various concrete implementations of this abstract interface. Further points: (cid:73) For some purposes, could consider an interface with fewer operations, or with more: e.g. reverse : void \u2192 void. (cid:73) May be other operations that make sense for specific impl\u2019s. E.g. for linked lists, \u2018insert/delete at current position\u2019 is useful. (cid:73) Some of our operations will be definable from others: e.g. cons(x) \u2261 insert(0, x) But may wish to include cons in its own right: might be implementable more efficiently than general insert. IADS Lecture 7 Slide 3 ",
    "7_3": "Implementation 1: Fixed-size arrays Use an array A of some fixed size m. Can store a list L = x0, . . . , xn\u22121 (where n \u2264 m) in the first n cells of A (so A[i] = xi for each i < n). Also want an integer variable n to store the value of n. List operations are easy to implement. E.g. get(i): return A[i] append(x): A[n] = x n = n+1 insert (i,x): for j = n\u22121 downto i A[j+1] = A[j] A[i] = x n = n+1 (cid:73) length, get, set and append (when it works) take \u0398(1) time. (cid:73) cons, insert, delete require \u0398(n) time in worst case. IADS Lecture 7 Slide 4 ",
    "7_4": "Lists via fixed-size arrays, ctd. Fixed-size arrays have some strengths . . . (cid:73) Fast get and set operations \u2013 especially if we can keep the array on the stack! (cid:73) Fixed, predictable size good for memory management. (If on stack, can reclaim space immediately on expiry.) . . . but a major weakness . . . (cid:73) Can\u2019t cope with lists longer than pre-set limit m! (cid:73) If a computation involves a lot of lists, of unpredictable sizes, very likely we\u2019ll either under-cater (some array will over\ufb02ow) or over-cater (many arrays will contain a lot of wasted space). So not a good choice for \u2018general-purpose\u2019 lists. IADS Lecture 7 Slide 5 ",
    "7_5": "Implementation 2: Extensible arrays Idea is simple: if array A over\ufb02ows, replace it by a bigger one! (cid:73) If memory space \u2018after\u2019 A happens to be free, cheap to do. (cid:73) But if not, may have to allocate a fresh array B, and copy contents of A into it. E.g. for some real number r > 1: append (x): if n = |A| B = new array ((cid:100) n \u00d7 r (cid:101)) copy contents of A into B (n items) A = B # Now do ordinary append: A[n] = x n = n+1 So a \u2018normal\u2019 append takes \u0398(1) time \u2013 but occasionally we may get a bad one, taking \u0398(n). Might seem \u2018dirty\u2019, but widely used in practice. Runtime analysis is interesting . . . IADS Lecture 7 Slide 6 ",
    "7_6": "Amortized cost Perhaps in some apps, even one bad append day could be fatal. But often, we\u2019re happy if over any long run of appends, the average time is reasonable. A bad one may be acceptable if we regard its cost as amortized (\u2018spread out\u2019) over the next 100 good ones \u2013 i.e. if invested e\ufb00ort \u2018pays for itself\u2019 over time. Does it? Suppose array has initial capacity a, and starting from nil we do m appends in succession, expanding by factor r > 1 when need be. Array size grows as a, ar , ar 2, ar 3, . . .. How many steps to reach m? Solving \u2018ar s = m\u2019 yields s = logr (m/a) for the number of steps. An item may get copied this many times! Since potential number of copyings of an item grows with m, might suspect \u2018average cost per append\u2019 also grows with m . . . ?? Let\u2019s do the sums. IADS Lecture 7 Slide 7 ",
    "7_7": "Calculating amortized cost of append Example: Suppose a = 100, r = 1.1, m = 5000. Note that 1.141a < m < 1.142a. So will need 42 expansions. Ignoring \u2018rounding\u2019, number of copyings (B[i] = A[i]) is basically 100 \u00d7 (1 + 1.1 + 1.12 + \u00b7\u00b7\u00b7 + 1.141) By \u2018sum of geometric progression\u2019 formula, this is 100 \u00d7 (1.142 \u2212 1)/(1.1 \u2212 1) < 1.1m/0.1 So although some items get copied 42 times, average no. of copyings per item stays below 1.1/0.1 = 11. In general, total number of copyings is basically at most m(r /(r\u22121)). So average no. of copyings per item stays below r /(r \u2212 1). IADS Lecture 7 Slide 8 ",
    "7_8": "More conceptual argument Again suppose a = 100, r = 1.1. Imagine a copying costs 1p. Each time we do an append, we pay 11p into a pension fund to pay for future copyings. Suppose we\u2019ve just done our first expansion. Array now has 110 cells, with 100 filled. Next 10 appends pay for second expansion (110 copyings). After second expansion, array has 121 cells, 110 filled. Next 11 appends pay for third expansion (121 copyings) . . . So each append incurs a constant cost of 11 copyings. IADS Lecture 7 Slide 9 ",
    "7_9": "Amortized cost: conclusion So total time taken by expansion/copying is O(m). But time taken by ordinary appends is also clearly O(m). So may say the amortized cost of append is O(1) per operation. (cid:73) Lists in Python are implemented like this, essentially with r = 9/8. Underlying arrays may also be shrunk if proportion in use dips below 1/2. (For analysis, see CLRS 17.4.) (cid:73) Java class ArrayList also works like this. Precise expansion policy not prescribed, but it\u2019s required that amortized cost over a long run must be O(1) per operation. Of course, cons, insert, delete still take time \u0398(n) in worst case (even amortized). IADS Lecture 7 Slide 10 ",
    "7_10": "Implementation 3: Linked lists We can also represent the lists over X using linked lists, where each cell contains a key of type X . Clearly, for a list of length n: (cid:73) get and set have \u0398(n) worst-case time (but with small \u2018C \u2019) (cid:73) cons takes \u0398(1) time, always. (cid:73) insert(i,x), delete(i) have \u0398(n) worst-case time (or \u0398(1) if we\u2019ve already located the cell at position i \u2212 1). Linked lists also naturally allow for sharing (unlike arrays): O\ufb04ine exercise: Show how the list of all 2n binary lists of length n can be stored in \u0398(2n) space with linked list impl. (Would take \u0398(n.2n) with arrays.) IADS Lecture 7 Slide 11 ",
    "7_11": "List implementations: summary Upper bounds on runtimes (where n is length of list): Operation Array impl get O(1) set O(1) cons append insert delete O(n) * O(n) * O(n) (amortized O(1)) O(n) (can make it O(1)) * O(n) Linked-list impl O(n) O(n) O(1) O(n) O(n) Operations marked * may fail for fixed-array implementations, or trigger expansion for extensible-array ones. So arrays o\ufb00er fast get/set; linked lists o\ufb00er fast cons/append and insert/delete at given position, plus sharing. ?? Is there some impl of lists for which all the above are \u2018fast\u2019 ?? Find out in Lecture 9! IADS Lecture 7 Slide 12 ",
    "7_12": "Stacks and queues Sometimes, we know that some list will only be manipulated in certain restricted ways, e.g. . . . (cid:73) Elements only ever added/read/removed at front of list (stack or Last-in-first-out bu\ufb00er) (cid:73) Elements added at back, read/removed at front of list (queue or First-in-first-out bu\ufb00er) Knowing this may a\ufb00ect our choice of implementation. Interfaces for stacks and queues (of items of type X ): STACKS: : void \u2192 bool empty push : X \u2192 void peek : void \u2192 X pop : void \u2192 X QUEUES: : void \u2192 bool empty enqueue : X \u2192 void peek : void \u2192 X dequeue : void \u2192 X IADS Lecture 7 Slide 13 ",
    "7_13": "Implementing stacks In principle, any impl of lists yields an impl of stacks: But two obvious candidates: (cid:73) arrays (growing at end) (cid:73) linked lists (growing at start) Operation Extensible array impl empty O(1) Linked list impl O(1) * O(n) (amortized O(1)) O(1) O(1) O(1) push peek O(1) pop O(1) IADS Lecture 7 Slide 14 ",
    "7_14": "Implementing queues Impl 1: Wraparound array bu\ufb00er (fixed-size/extensible) Impl 2: Linked list with references to first and last cells IADS Lecture 7 Slide 15 ",
    "7_15": "Implementing queues, ctd. How would e.g. enqueue look in each case? Wraparound array: enqueue(x): j = (j+1) mod |A| if j = i fail (or expand) else A[j] = x Linked list: enqueue(x): last.next = new Cell(x,null) last = last.next For further details, see Python Lab Sheet 3. Situation similar to stacks: Operation Wraparound array impl enqueue Linked-list impl peek O(1) dequeue O(1) * O(n) (amortized O(1)) O(1) O(1) O(1) IADS Lecture 7 Slide 16 ",
    "7_16": "Reading Stacks and queues: CLRS chapter 10. Table expansion / amortized analysis: CLRS section 17.4. IADS Lecture 7 Slide 17 ",
    "8_1": "Sets and dictionaries Two important datatypes . . . (cid:73) (Finite) sets of items of a given type X . E.g. {3, 5} = {5, 3} : X \u2192 bool contains : X \u2192 void insert delete : X \u2192 void : void \u2192 bool isEmpty (cid:73) Dictionaries (i.e. lookup tables) mapping keys of type X to values of type Y . lookup : X \u2192 Y insert delete : X \u2192 void isEmpty : X \u2217 Y \u2192 void : void \u2192 bool IADS Lecture 8 Slide 2 ",
    "8_2": "Sets and dictionaries in Python Beatles = {\u2019John\u2019, \u2019Paul\u2019, \u2019George\u2019, \u2019Ringo\u2019} \u2019George\u2019 in Beatles # returns True BeatlesYearsOfBirth = {\u2019John\u2019:1940, \u2019Paul\u2019:1942, \u2019George\u2019:1943, \u2019Ringo\u2019:1940} BeatlesYearsOfBirth[\u2019George\u2019] # returns 1943 IADS Lecture 8 Slide 3 ",
    "8_3": "Sets and dictionaries via sorted arrays Could implement sets/dictionaries via (any impl of) lists: Beatles Rep = [\u2019John\u2019, \u2019Paul\u2019, \u2019George\u2019, \u2019Ringo\u2019] BeatlesYearsOfBirth Rep = [(\u2019John\u2019,1940), (\u2019Paul\u2019,1942), ....] But average-case time for contains/lookup will be \u0398(n) (terrible!) Much better if arrays are sorted (by key). Can then use binary search. E.g. for dictionaries: binarySearch(A,key,i,j): if j\u22121 = i # searches A[i], ..., A[j\u22121] else if A[i].key = key then return A[i].value else FAIL k = (cid:98) i+j/2 (cid:99) if key < A[k].key then return binarySearch(A,key,i,k) else return binarySearch(A,key,k,j) Using this, contains/lookup have worst-case time \u0398(lg n). But insert/delete still costly. Can we do better? IADS Lecture 8 Slide 4 ",
    "8_4": "Hash tables Suppose our keys are strings (e.g. people\u2019s names). Number K of potential keys is vast \u2014 number n of actual keys \u2018currently in use\u2019 much smaller. Really silly idea: Give a way of converting strings s to integers \u0131(s) (E.g. treat ASCII characters as digits to base 128). Then store value associated with s in a big array at position \u0131(s). Impractical: K normally far too large, and most of the array would be unused. More sensible idea: Choose some hash function # mapping potential keys s to integers 0, . . . , m \u2212 1 (hash codes), where m \u223c n. Want # to be easy to compute. E.g. we might define: #(s) = \u0131(s) mod m Then try to use an array A of size m, storing the entry for key s at position #(s) in A. IADS Lecture 8 Slide 5 ",
    "8_5": "Hashes and clashes Problem: What if #(s) = #(t) for two keys s, t? How likely are clashes to arise? E.g. if we took e.g. m \u223c 5n (and accepted the space wastage), would clashes be improbable? Example: Keys are people, m = 366, #(p) = birthday of p. How many people must there be for probability of shared birthday to be > 1/2? (Assume uniform distrib.) Answer: Just 23! (Sometimes called the birthday paradox.) See CLRS 5.4.1 for analysis (if you\u2019re interested). Question: In a class of 347 (assuming uniform distrib), what would be the probability of a birthday shared by 2 people? By 3 people? By 4, 5, 6, 7, . . . ? 2 5 6 7 > (100 \u2212 10\u2212123)% > 99.9999% > 99.8% 66% 15% 2% 3 4 IADS Lecture 8 Slide 6 ",
    "8_6": "The Great Inf2 Birthday Experiment (totally optional!) Curious about shared birthdays? Email j.r.longley@ed.ac.uk to take part! Subject line: birthday MMDD (E.g. birthday 0216 for 16 Feb; birthday 1006 for 6 Oct.) Please observe this format: lowercase birthday then single space. Body text: Your name if you\u2019d like to be put in touch with any others with same birthday. Otherwise blank. Closing date: 12 noon (BST) on Friday 22 October. All emails/records will be deleted by Friday 29 October. I\u2019ll announce the results on Piazza: number of participants, dates of any shared birthdays, and number of people sharing them. I won\u2019t announce any names. IADS Lecture 8 Slide 7 ",
    "8_7": "Dealing with clashes So we must accept clashes (a.k.a. collisions) as a fact of life. Solution 1: Store a list of entries (or bucket) for each hash value. (Omit value components if it\u2019s just a set.) Write n for number of entries, m for array size. The ratio \u03b1 = n/m is called the load on the hash table: may be \u2264 1 or > 1. If we\u2019ve decided on a desired load \u03b1, can \u2018expand-and-rehash\u2019 any time n gets too large (amortized cost is reasonable). IADS Lecture 8 Slide 8 ",
    "8_8": "Bucket-list hash tables: some analysis Recall: n table entries, m hash codes, \u03b1 = n/m. Write bi for number of entries in ith bucket. Let\u2019s analyse average time for an unsuccessful lookup. Assume that for k not in the table, #(k) equally likely to be any of the m hash codes. If #(k) = i, lookup will do bi key comparisons if unsuccessful. So average number of key comparisons is m\u22121(cid:88) i=0 1 m bi = n/m = \u03b1 If computing #(k) itself takes O(1) time, conclude that average time for unsuccessful lookup is \u0398(\u03b1). (Thinking of \u03b1 \u2192 \u221e.) Can also show the same for successful lookup, assuming all keys present in table are equally likely. See CLRS 11.2. IADS Lecture 8 Slide 9 ",
    "8_9": "Making a proper hash of it Rarely true that all potential keys (e.g. strings) \u2018equally probable\u2019. But in the interests of \u2018balancing\u2019 our hash table, we\u2019d like the hash codes 0, . . . , m \u2212 1 to be all equally likely. Bad choice: #(s) = \u0131(s) mod 128. E\ufb00ectively just last character of s. So avoid powers of two! Also not great: #(s) = \u0131(s) mod 127. Gives #(s) = #(t) whenever s, t are anagrams. So #(\u2018algorithms\u2019) = #(\u2018logarithms\u2019). Better: #(s) = \u0131(s) mod 97. Primes not too close to powers of two are reasonable. Just the start of the delicate art of hash function design. . . But whatever we do, worst case (all keys hashing to same code) is always terrible. A malicious user who knew your hash function could force this to happen . . . IADS Lecture 8 Slide 10 ",
    "8_10": "Open addressing and probing Solution 2: Rather than keeping bucket lists outside the hash table, store all items within the table itself (open addressing). To deal with clashes, we use not just a simple hash function #(k), but a function #(k, i) where 0 \u2264 i < m. For a key k: (cid:73) #(k, 0) is our first choice of hash value, (cid:73) #(k, 1) is our second choice, etc. so that #(k, 0), #(k, 1), . . . , #(k, m \u2212 1) is a permutation of 0, . . . , m \u2212 1. (Ideally, for a randomly chosen k, all m! permutations should be equally likely.) To insert an item e with key k, probe A[#(k, 0)], A[#(k, 1)], . . . until we find a free slot A[#(k, i)], then put e there. To lookup an item with key k, probe A[#(k, 0)], A[#(k, 1)], . . . until we find either an item with key k, or free cell (lookup failed). IADS Lecture 8 Slide 11 ",
    "8_11": "Probing: example Let\u2019s use an array A of size m = 10 to store a set of integers. 1 2 3 4 5 6 7 0 58 8 28 9 49 Probe function: #(k, i) = (k + i) mod 10. insert(49). #(49, 0) = 9: free. insert(28). #(28, 0) = 8: free. insert(58). #(58, 0) = 8: taken. #(58, 1) = 9: taken. #(58, 2) = 0: free. contains(28). #(28, 0) = 8, A[8] = 28. So true. contains(58). #(58, 0) = 8, A[8] = 28 (cid:54)= 58. #(58, 1) = 9, A[9] = 49 (cid:54)= 58. #(58, 2) = 0: A[0] = 58. So true. contains(39). #(39, 0) = 9, A[9] = 49 (cid:54)= 39. #(39, 1) = 0, A[0] = 58 (cid:54)= 39. #(39, 2) = 1, A[1] free. So false. IADS Lecture 8 Slide 12 ",
    "8_12": "Probing: pros and cons (cid:73) Expected number of probes for insert (and hence for lookup) stays low until table is nearly full. (Can show it\u2019s 1/(1 \u2212 \u03b1) for unsuccessful lookup; less for successful one.) (cid:73) No need for pointers. The memory this saves can be \u2018spent\u2019 on increasing table size m and so decreasing load \u03b1 . . . So compared to bucket lists, get faster lookup for same amount of memory. (cid:73) However, delete is a pain for the probing approach. (cid:73) Design of probing functions is again a delicate art (linear probing, quadratic probing, double hashing, . . . ). See CLRS 11.4 for more details. IADS Lecture 8 Slide 13 ",
    "8_13": "Radical alternative: Perfect hashing (cid:73) All the approaches we\u2019ve mentioned are bad in the worst case: size of bucket/sequence of probes can be of length n. (cid:73) Even in typical cases, probably some buckets will be large relative to \u03b1. (Birthday paradox!) If we could avoid clashes altogether, these problems would vanish! Would get worst-case \u0398(1) lookup. If set of keys is static (no insert/delete required), may be worth finding a perfect hash function (no clashes) for this set of keys. As part of Coursework 1, we\u2019ll explore a state-of-the-art approach to perfect hashing. IADS Lecture 8 Slide 14 ",
    "8_14": "Reading: CLRS Chapter 11. You can omit the theorems and their proofs, except for Theorem 11.1 which corresponds to slide 9. Today\u2019s music: Nikolai Rimsky-Korsakov, Flight of the bumblebee. (Piano version by Sergei Rachmaninov.) IADS Lecture 8 Slide 15 ",
    "9_1": "Tackling that \u2018worst case\u2019 (cid:73) We\u2019ve considered hash table implementations of sets/dictionaries in which lookup/insert/delete are usually fast \u2013 but worst case time for all operations is \u0398(n). (cid:73) For lists (a.k.a. vectors): some operations have worst-case time \u0398(1), but insert/delete are \u0398(n) even in average case. ??? Can we find implementations of sets/dictionaries/lists for which all operations have acceptable worst-case times ??? This lecture: We\u2019ll see that \u2018balanced trees\u2019 (e.g. red-black trees) achieve this: all ops have worst-case and average time \u0398(lg n). Will do sets/dictionaries here; ideas can also be applied to lists. IADS Lecture 9 Slide 2 ",
    "9_2": "Representing sets by trees Consider binary trees: each node x has a left and a right branch, each of which may be null or a pointer to a child node. (Implementation detail: should use doubly linked tree structures.) Write L(x), R(x) for left and right subtrees at x (may be empty). Label nodes with keys (e.g. integers or strings) in such a way that for every node x we have \u2200y \u2208 L(x). y .key < x.key , \u2200z \u2208 R(x). x.key < z.key Can use such trees to represent sets of keys. (For dictionaries, just add value component to each node.) IADS Lecture 9 Slide 3 ",
    "9_3": "Implementing contains/lookup This is easy. Let a node x stand for the tree rooted at x. contains\u2019(x,k): if x = null then return False else if x.key = k then return True else if k < x.key then return contains\u2019(x.left,k) else return contains\u2019(x.right,k) contains(k): return contains\u2019(root,k) Suppose the tree has n nodes and is perfectly balanced, i.e. all non-leaf nodes have 2 children, and all leaf nodes are at the same depth d. (Possible only if n = 2d+1 \u2212 1.) Then d = (cid:98)lg n(cid:99), so contains will take time O(lg n). More generally, for trees that are \u2018not too unbalanced\u2019 (say max depth \u2264 2(cid:100)lg n(cid:101)), can say contains take O(lg n) time. However, worst case is still \u0398(n)! IADS Lecture 9 Slide 4 ",
    "9_4": "Insert on binary trees This too is easy: walk down tree to find where k wants to go, and create a new leaf node for it. insert\u2019(x,k): if x.key = k then return KeyAlreadyPresent else if k < x.key then if x.left = null then x.left = new Node(k) else insert\u2019(x.left,k) else if x.right = null then x.right = new Node(k) else insert\u2019(x.right,k) insert(k): if root = null then root = new Node(k) else return insert\u2019(root,k) Again, O(lg n) time if tree not too unbalanced, \u0398(n) in worst case. NB. Nothing here to guard against tree becoming unbalanced! IADS Lecture 9 Slide 5 ",
    "9_5": "Delete on binary trees A bit more subtle. To perform delete(j): (cid:73) Locate the node y bearing j (assume there is one). (cid:73) If y has no children, can just delete it. (cid:73) If y has one child, easy to elide the node y (Fig. 1). (cid:73) If y has two children: (cid:73) Locate leftmost node in R(y ), i.e. starting at y , turn right, then left as often as possible. This finds the node z bearing the smallest key in R(y ) (call it k). (cid:73) Copy z.key to y .key. (cid:73) If z has a right child, elide z, otherwise just delete z. (Fig. 2). Same runtime characteristics. IADS Lecture 9 Slide 6 ",
    "9_6": "Balanced tree representations General strategy: (cid:73) Work with some special class of trees (red-black trees) that are guaranteed to be \u2018not too unbalanced\u2019, so that all operations will take time O(lg n). (cid:73) Whenever an insert/delete threatens to take us outside this class, do some \u2018re-balancing\u2019 work to restore it. Clever bit: Can arrange that this re-balancing work also takes just O(lg n) time! This leads to worst-case O(lg n) time for all operations. This broad strategy works for several classes of trees: red-black trees, AVL trees, 2-3 trees, . . . We choose red-black trees as they\u2019re covered in detail in CLRS. IADS Lecture 9 Slide 7 ",
    "9_7": "Small preliminary: adding trivial nodes For mathematical convenience, extend our trees so that original null branches now point to trivial nodes, with no children and bearing no key. Original nodes are proper nodes. Call this an extended tree. Just makes rules easier to state. Wouldn\u2019t need these trivial nodes in an implementation. IADS Lecture 9 Slide 8 ",
    "9_8": "Red-black trees Work with extended trees as above. In a red-black tree, every node is coloured red or black. Tree rules: (cid:73) Root and all (trivial) leaves are black. (cid:73) All paths root \u2192 leaf contain same number b of blacks. (cid:73) On a path root \u2192 leaf, never have two reds in a row. So min possible path length is b, and max is 2b \u2212 1. Red-black trees are not too unbalanced: Can show b \u2264 lg(n + 1) + 1, so all path lengths \u2264 2 lg(n + 1) + 1. So contains works as usual with worst-case time \u0398(lg n). IADS Lecture 9 Slide 9 ",
    "9_9": "Insert for red-black trees Can insert a key-bearing node as usual (adding two trivial leaves). Colour it red. This all takes O(lg n) time. Problem: Resulting tree might no longer be a legal red-black tree: (cid:73) New red node might have red parent (2 reds in succession), or (cid:73) (Trivial case) New red node might be root (should be black). So need to apply a fix-up operation to restore red-black-ness. Main ingredient is the red-uncle rule: (Just colour-\ufb02ipping: fast. No rewiring involved!) IADS Lecture 9 Slide 10 ",
    "9_10": "Insert fix-up, continued Applying the red-uncle rule pushes a red upward, so may result in another double-red higher up. So we apply the red-uncle rule as often as possible (will be at most O(lg n) times). We\u2019ll then be in one of three endgame scenarios: 1. Problem cured: tree now legal. 2. Red pushed to root: turn it black. Adds 1 to all black-lengths. 3. Have some configuration involving a black with 4 \u2018nearest black descendants\u2019. Replace by obvious \u2018balanced\u2019 version: =\u21d2 O(1) amount of rewiring. Note order of constituents is preserved: AaBbCcD. (Subtrees A,B,C,D may be empty.) IADS Lecture 9 Slide 11 ",
    "9_11": "Delete for red-black trees Just the main ideas: won\u2019t give full details. Do delete as usual: this involves removing some proper node z. Problem: All paths must have same black-length. So if z was black, want to remove z but keep the \u2018blackness\u2019. Easy case: Node it haunts is now red: can just turn it black. Wandering black rule: apply this as often as possible (will be O(lg n) times). IADS Lecture 9 Slide 12 ",
    "9_12": "Delete for red-black trees: the endgame Finitely many endgame scenarios, each fixable in O(1) time. E.g. (cid:73) Floating black haunts a red node: turns it black. (cid:73) Floating black reaches root: just remove it. (cid:73) We\u2019re in some other fixable scenario, e.g. Blue square and green triangle are colour variables. (cid:73) 4 other scenarios like this: see CLRS 13 for full details. IADS Lecture 9 Slide 13 ",
    "9_13": "Balanced trees: conclusion (cid:73) Balanced trees o\ufb00er a way of implementing sets/dictionaries so that all operations have worst-case time O(lg n). (Idea can be applied to lists too.) (cid:73) Not much to choose between red-black and AVL trees. AVL are \u2018more balanced\u2019 (better for lookup); red-blacks possibly have faster insert/delete. (cid:73) Red-black trees used in practice: (cid:73) Linux completely fair scheduler (cid:73) Java 8 HashMap class: dictionary via bucket-style hash table, but each bucket is a red-black tree rather than a linked list. Retains excellent typical-case performance of hash tables, but kills o\ufb00 the nasty \u2018worst cases\u2019. Reading: CLRS 12.1-12.3, 13.1-13.3 Today\u2019s music: John Williams, Hedwig\u2019s Theme (from the Harry Potter films). IADS Lecture 9 Slide 14 ",
    "10_1": "Data structures: re\ufb02ection We\u2019ve looked at . . . (cid:73) some classic abstract datatypes (lists, stacks, queues, sets, (cid:73) various concrete implementations of them (via extensible dictionaries) arrays, linked lists, hash tables, red-black trees . . . ) We\u2019ve analysed their pros/cons in terms of asymptotic runtimes for operations. (Measured as number of line executions, paying attention to what\u2019s allowed as a \u0398(1) time basic memory operation.) The above datatypes are used frequently in programming \u2013 and many other algorithms build on them. Most of these data structures already provided in standard libraries (e.g. Java API classes). But understanding of runtime characteristics can help in (cid:73) writing efficient programs (cid:73) constructing efficient database queries. IADS Lecture 10 Slide 2 ",
    "10_2": "Recursion: a recurring theme As we\u2019ve seen, many algorithms can be presented as recursive: i.e. they involve subcall to (one or more instances of) same problem. Examples: (cid:73) Expmod(a,n,m) involves call to Expmod(a,(cid:98)n/2(cid:99),m). (cid:73) Mergesort(A,m,n) calls Mergesort(A,m,p) and Mergesort(A,p,n). (cid:73) Insert(x,k) (for binary trees) may call Insert(x.left,k) or Insert(x.right,k). Common pattern: (cid:73) \u2018Simple\u2019 (e.g. small) instances can be dealt with directly. (cid:73) For larger instances, may do work before/during/after the recursive call(s): we divide into subproblems, conquer these, combine results. E.g. for Mergesort: (cid:73) divide is simply checking n \u2212 m > 1 and computing (cid:98)(m + n)/2(cid:99). (cid:73) combine is merging the two lists returned by the recursive calls. IADS Lecture 10 Slide 3 ",
    "10_3": "Recurrence relations How can we calculate the (asymptotic) runtime for a recursive algorithm? Let T (n) be the worst-case runtime e.g. for Mergesort on array segments of size n. Whatever the function T is, can say that T (n) = T ((cid:98)n/2(cid:99)) + T ((cid:100)n/2(cid:101)) + F (n) for all n > 1 , where F (n) is the worst-case time for the divide and combine phases on inputs of size n. Can also say T (1) is a constant C . This is an example of a recurrence relation. If we know C and F , can compute T (n) for a specific n, e.g. T (4) = 2T (2) + F (4) = 2(2T (1) + F (2)) + F (4) = 4C + 2F (2) + F (4) But can we \u2018solve\u2019 the rec. rel. to find an explicit formula for T (n)? Or at least, for its asymptotic growth rate? IADS Lecture 10 Slide 4 ",
    "10_4": "Recurrence relations for growth rates (cid:26) C T (n) = if n = 1 T ((cid:98)n/2(cid:99)) + T ((cid:100)n/2(cid:101)) + F (n) otherwise Actually, if we only want the growth rate of T , don\u2019t need to know F precisely \u2014 knowing its growth rate is enough. E.g. in Mergesort example, have F (n) = \u0398(n) (time for Merge on lists of length n/2). Leads to the concept of an asymptotic recurrence relation. E.g. (cid:26) \u0398(1) T (n) = 2T (n/2) + \u0398(n) otherwise if n = 1 Solution we\u2019re seeking isn\u2019t a precise function, but a growth rate. (Omission of (cid:98)\u2212(cid:99) and (cid:100)\u2212(cid:101) a bit sloppy . . . but can be shown these \u2018don\u2019t a\ufb00ect asymptotic solution\u2019 in cases like this.) IADS Lecture 10 Slide 5 ",
    "10_5": "Recurrence relations ctd. Asymp. rec. relation for Mergesort again: (cid:26) \u0398(1) T (n) = 2T (n/2) + \u0398(n) otherwise if n = 1 In Lecture 5 we saw informally that in this case T (n) = \u0398(n lg n). Other examples: (cid:73) Runtime of Expmod(a,n,m) for fixed a,m: T (n) = T (n/2) + \u0398(1) for n>1 (cid:73) Runtime of Exp(a,n) for fixed a (Expmod without the mod): T (n) = T (n/2) + \u0398(n2) for n>1 (cid:63) Can we solve such recurrences systematically? Is there a general pattern here? IADS Lecture 10 Slide 6 ",
    "10_6": "How do we come up with solutions? Approach 1: Use intuition/experience/numerical data to \u2018guess\u2019 a solution, then verify it using induction. Usual concept of induction needs extending a bit. We\u2019ll use MergeSort as an example. Ordinary induction: \u2018Log induction\u2019: Here, log induction on k (cid:39) ordinary induction on (cid:100)lg k(cid:101) (recursion depth for MergeSort on array segments of size k). IADS Lecture 10 Slide 7 ",
    "10_7": "Proof by induction on recursion depth Claim: If n \u2212 m = k \u2265 1, number of comparisons performed by MergeSort (A,m,n) is \u2264 k(cid:100)lg k(cid:101). Proof by complete induction on (cid:100)lg k(cid:101): Base case: (cid:100)lg k(cid:101) = 0, i.e. k = 1. Then MergeSort immediately returns [A(m)] (0 comparisons). Induction step: Suppose claim holds whenever (cid:100)lg k(cid:101) \u2264 q, and now consider some k such that (cid:100)lg k(cid:101) = q + 1. Let p = (cid:98)(m + n)/2(cid:99), k1 = p \u2212 m, k2 = n \u2212 p. (So either k1 = k2 = k/2 or k1 = (cid:98)k/2(cid:99), k2 = (cid:100)k/2(cid:101).) In any case, we have (cid:100)lg(k1)(cid:101),(cid:100)lg(k2)(cid:101) \u2264 q. So by induction hypothesis: (cid:73) MergeSort (A, m, p) takes \u2264 k1(cid:100)lg(k1)(cid:101) \u2264 k1q comparisons, (cid:73) MergeSort (A, p, n) takes \u2264 k2(cid:100)lg(k2)(cid:101) \u2264 k2q comparisons, (cid:73) the final Merge (B, C ) takes \u2264 k1 + k2 = k comparisons. So total number of comparisons is \u2264 k1q + k2q + k = kq + k = k(q + 1) = k(cid:100)lg k(cid:101) as required. (cid:3) IADS Lecture 10 Slide 8 ",
    "10_8": "The Master Theorem Approach 2: If our recurrence happens to be of the form . . . (cid:26) \u0398(1) T (n) = aT (n/b) + \u0398(nk ) if n \u2264 n0 if n > n0 . . . then there\u2019s a Master Theorem that simply gives us the answer. (Also works with \u2018\ufb02oors and ceilings\u2019 around.) The answer depends on how a compares with bk (will explain!). Equivalently, how e = logb a compares with k. \uf8f1\uf8f2\uf8f3 \u0398(ne) \u0398(nk lg n) \u0398(nk ) if e > k if e = k if e < k T (n) = This applies in many (not all) commonly arising situations. (CLRS 4.5 gives a more general version of the theorem.) IADS Lecture 10 Slide 9 ",
    "10_9": "Master Theorem: informal intuition Think about total work done by all divide / combine phases at each recursion level. Does this increase or decrease as we go down? (cid:73) Larger a (no. of subproblems) means more work as we descend. (cid:73) But larger b means each subproblem is smaller. If divide/combine work is F (n) = \u0398(nk ), then reducing problem size by factor b will reduce this work by bk . (cid:73) So break-even point is when a = bk . In this case, amount of work is \u2018essentially the same\u2019 for all levels. IADS Lecture 10 Slide 10 ",
    "10_10": "Optional slide (not covered in video lecture) A bit more detail for those interested . . . (cid:73) If a < bk , then the most work is done at the top level. Thereafter, amount of work roughly decreases in geometric progression, by factor r = a/bk < 1. So total work will be roughly top-level work (\u0398(nk )) times 1 + r + r 2 + \u00b7\u00b7\u00b7 \u2264 1/(1 \u2212 r ) (constant). Still \u0398(nk ). (cid:73) If a > bk , work increases by r = a/bk > 1 as we descend. Around logb(n) levels. So bottom-level exceeds top-level by r logb(n) = blogb(r ). logb(n) = blogb(n). logb(a/bk ) = ne\u2212k So total work comes out as \u0398(nk ).\u0398(ne\u2212k ) = \u0398(ne). (cid:73) If a = bk , all levels are \u2018essentially the same\u2019. So work is roughly (top-level work \u00d7 number of levels), i.e. \u0398(nk lg n). IADS Lecture 10 Slide 11 ",
    "10_11": "Master Theorem in action (cid:73) Mergesort recurrence again: (cid:26) \u0398(1) T (n) = 2T (n/2) + \u0398(n) otherwise if n = 1 Here a = 2, b = 2, k = 1. So e = logb a = 1 and e = k. So we\u2019re in the middle case: \u0398(n log n). (cid:73) Exp(a,n) for fixed a: T (n) = T (n/2) + \u0398(n2) if n>1 Here a = 1, b = 2, k = 2. So e = logb a = 0 and e < k. Work at top-level dominates: solution is \u0398(n2). More examples coming in Tutorial Sheet 4 (for Week 8). Reading: CLRS Chapter 4 intro and 4.3\u20134.5. Today\u2019s music: Trad. spiritual, This little light of mine IADS Lecture 10 Slide 12 ",
    "10_12": "Roundup of sorting algorithms For long lists, Insertsort and Bubblesort aren\u2019t serious contenders: both take \u0398(n2) time on average. Mergesort Average-case time \u0398(n lg n) Worst-case time \u0398(n lg n) Heapsort \u0398(n lg n) \u0398(n lg n) Best-case time \u0398(n lg n) \u0398(n) (when input Quicksort \u0398(n lg n) \u0398(n2) \u0398(n lg n) already sorted) Hidden constants Extra space needed Stable? Memory locality medium large? small \u0398(n) Yes ok? \u0398(1) (in-place) \u0398(1) (in-place) No poor No good Theorem: Any general, comparison-based sorting algorithm has worst-case time \u2126(n lg n). [Proved in CLRS 8.1: boils down to lg(n!) = \u2126(n lg n).] However, Countsort, Radixsort, Bucketsort run in O(n) time on more specialized kinds of inputs. [CLRS 8.2, 8.3, 8.4] IADS Lecture 10 Slide 13 ",
    "11_1": "Remainder of semester 1 Hello everyone! I will take over the teaching for the remainder of semester 1, and also a large part of semester 2. Plan for (rest of) semester 1: 11. The Heap data structure 12. BuildHeap and HeapSort: running-time 13. QuickSort 14. Graphs I: graph data structures, Breadth-first search 15. Graphs II: DFS, connected components,TopSort",
    "11_2": "The Heap Definition A (max) heap is a \u201cnearly complete\u201d binary tree structure storing items in nodes, where every node is greater than or equal to each of its child nodes. (cid:73) The rule for parent/child key values is weaker over the tree as a whole than what we have for red-black trees, 2-3-4 trees or AVL trees (in those cases the tree encodes a total-ordering on the keys in the nodes). (cid:73) But . . . the topology of a heap is more restricted than for those other tree structures - we have a binary tree with leaves appearing at depth h and depth h \u2212 1, and all depth-h leaves grouped together to the left. (cid:73) The heap does not (readily) carry total-order information, but is ideally set-up to efficiently answer \u201cmax\u201d questions (suitable for priority queues). (cid:73) Neat structure of the topology means we can store the heap in an array.",
    "11_3": "Example heap Direct mapping: k-th element of heap stored in index k \u2212 1. Can use (2i \u2212 1) + j \u2212 1 for index of jth element on level i. (depends on \u201cAlmost-complete\u201d property).",
    "11_4": "Heaps: height and size A heap is an almost-complete binary tree: (cid:73) All leaves are either at depth h \u2212 1 or depth h (where h is height). (cid:73) The depth-h leaves all appear consecutively from left-to-right. . . . A heap of height h has between 2h and 2h+1 \u2212 1 nodes. 2h \u2264 n \u2264 2h+1 \u2212 1. Hence taking lg across this inequality, we see h \u2264 lg(n) < h + 1. This will put h in the range [lg(n) \u2212 1, lg(n)], ie \u0398(lg(n)). Lots of our Heap algorithms have worst-case running-time directly related to the height of the Heap.",
    "11_5": "Main operations on a Heap We imagine that the heap is stored in the array A. Heap-Maximum Returns the max element of a Heap - \u0398(1) time. Max-Heapify Runs in O(lg(n)) time and is used to maintain the (max) Heap property whenever some node/index i has violated the heap rule (but left subtree, right subtree are each legal Max Heaps). Heap-Extract-Max Can return (and delete) the maximum item of a Heap in O(lg(n)) time. Max-Heap-Insert Can insert a new item (and maintain the heap property) in O(lg(n)) time. Same for Heap-Increase-Key. Build-Max-Heap Special one called Build-Max-Heap will run in O(n) time to build a Heap from scratch from an unordered input array.",
    "11_6": "Max-Heapify and the other operations The Max-Heapify operation (called at i) is used to \u201cfix-up\u201d a Heap where the left-subtree Left(i) is a Heap, and so is the right-subtree Right(i) . . . but the value at i violates the Heap property. (cid:73) We will show that Max-Heapify can be implemented in time O(h) for the height h of the heap, which is O(lg(n)). (well, specifically, the height of the Heap rooted at i) (cid:73) We can then implement Heap-Extract-Max via the trick of just . . . (cid:73) Swapping A[0] (the max element) with A[A.heap size \u2212 1] (the last (cid:73) Then calling Max-Heapify(0) on the Heap to \u201cfix\u201d the error at the item in the array, and decrementing A.heap size. root. (cid:73) Max-Heapify is also key to the implementation of Build-Max-Heap.",
    "11_7": "Heap-Extract-Max The main work is not returning the max element (\u0398(1) time) but removing the max from the tree. We copy over the \u201clast node\u201d onto the root, then call Max-Heapify to fix things.",
    "11_8": "Max-Heapify We assume that the \u201cleft-heap\u201d Left(i) and the \u201cright-heap\u201d Left(i) are both accurate. Then Max-Heapify(i) will \u201cpatch-up\u201d the heap from i. Algorithm Max-Heapify(A, i) (cid:96)\u2190 Left(i) r \u2190 Right(i) largest\u2190 i largest\u2190 (cid:96) largest\u2190 r if largest (cid:54)= i 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. if (cid:96) < A.heap size and A[(cid:96)] > A[i] if r < A.heap size and A[r ] > A[largest] exchange A[i] with A[largest] Max-Heapify(A, largest)",
    "11_9": "Max-Heapify We are calling Max-Heapify from the root node. Max child of root is 48 on right, need to swap, and then recursively call Max-Heapify on 30 as the child (as in line 10. of the Algorithm).",
    "11_10": "Max-Heapify . . . Max child of 30 is 45 on left, need to swap, and then call heapify on 30 as the child.",
    "11_11": "Max-Heapify . . . Max child of 30 is 4, less than 30. ok. Finish.",
    "11_12": "Max-Heap-Insert Algorithm Max-Heap-Insert(A, k) 1. A.heap size\u2190 A.heap size + 1 2. A[heap size \u2212 1]\u2190 k. j\u2190 heap size \u2212 1 j\u2190 Parent(j) 3. 4. while (j (cid:54)= 0 and A[j] > A[Parent(j)]) do 5. 6. exchange A[j] and A[Parent(j)] \u201cBubble\u201d the item up the tree. Basically swap k with A[Parent(j)] if k is bigger. Why is this correct?? Takes \u0398(1) for adding new last node (initially), and \u0398(1) for every swap. Hence \u0398(lg n) worst-case in total.",
    "11_13": "Max-Heap-Insert Max-Heap-Insert(48), first add at \u201clast node\u201d. Need to swap 48 with parent 30, because 48 > 30.",
    "11_14": "Max-Heap-Insert 48 has now moved-up Now need to swap 48 with parent 45, because 48 > 45.",
    "11_15": "Max-Heap-Insert Done. 48 is less than root 88, no swap needed.",
    "11_16": "Priority Queues A Priority queue is a Data Structure for storing collections of elements. They di\ufb00er in their access policy compared to Lists, Stacks and Queues: (cid:73) Every element is associated with a key, which is taken from some linearly ordered set, such as the integers. (cid:73) Keys represent priorities: A larger key means a higher priority. Classic application is for access to resources like printers, when di\ufb00erent users may have varying priority levels.",
    "11_17": "Priority Queue operations Methods of PriorityQueue: (cid:73) insertItem(k, e): Insert element e with key k. (cid:73) maxElement(): Return an element with maximum key; an error occurs if the priority queue is empty. (cid:73) removeMax(): Return and remove an element with maximum key; an error occurs if the priority queue is empty. (cid:73) isEmpty(): Return true if the priority queue is empty and false otherwise. No findElement(k) or removeItem(k) methods.",
    "11_18": "Implementations of Priority Queues Observation: The maximum key in a binary search tree (like a Red-Black tree) is always stored in the rightmost leaf. Therefore, all Priority Queue methods can be implemented on an Red-Black tree with running time \u0398(lg(n)) (except isEmpty which is \u0398(1)). However, using a Max Heap we can implement maxElement with Heap-Maximum in \u0398(1) time, while still having insertItem (via Max-Heap-Insert) and removeMax (via Heap-Extract-Max) in \u0398(lg(n)) time. Note Balanced Search trees can be \u201ctweaked\u201d to maintain a direct pointer to the rightmost leaf, to give \u0398(1) for maxElement.",
    "11_19": "Reading Material This lecture used content from sections 6.1, 6.2 and 6.3 of [CLRS]: (cid:73) I did Max-Heap-Insert more directly than the book. (cid:73) I didn\u2019t write the details of Parent, Left, Right on slides (tutorial qn). In lecture 12, I will cover: (cid:73) The method Build-Heap (cid:73) The asymptotic analysis of the running-time of the Heap algorithms (6.1-6.3 of [CLRS]) (cid:73) Heapsort and its running time (6.4 of [CLRS])",
    "13_1": "Holyrood Park last year IADS \u2013 Lecture 13 \u2013 slide 2 ",
    "13_2": "QuickSort Invented by British computer scientist Tony Hoare in 1960 while studying in Moscow, published in 1961. Divide-and-Conquer algorithm: 1. If the input array has < two elements, do nothing. Otherwise, call Partition: Pick a pivot key and use it to divide the array into two: \u2264 pivot pivot \u2265 pivot 2. Sort the two subarrays recursively. IADS \u2013 Lecture 13 \u2013 slide 3 ",
    "13_3": "QuickSort if p < r then split\u2190 Partition(A, p, r ) Algorithm QuickSort(A, p, r ) 1. 2. 3. 4. QuickSort(A, p, split \u2212 1) QuickSort(A, split + 1, r ) IADS \u2013 Lecture 13 \u2013 slide 4 ",
    "13_4": "Partition Algorithm Partition(A, p, r ) 1. pivot\u2190 A[r ].key i \u2190 p \u2212 1 for j\u2190 p to r \u2212 1 do i \u2190 i + 1 2. 3. 4. 5. 6. 7. exchange A[i + 1] and A[r ] 8. return i + 1 if A[j] \u2264 pivot exchange A[i] and A[j] Invariant: i is 1 less than the leftmost > pivot value in the range p . . . j (or is j \u2212 1 if no > pivot is there). IADS \u2013 Lecture 13 \u2013 slide 5 ",
    "13_5": "Partition example (done on video) IADS \u2013 Lecture 13 \u2013 slide 6 ",
    "13_6": "Correctness of Partition Assume we are part-way through Partition and for this j (and this i), we have the Invariant (all A[i + 1 . . . j \u2212 1] are > pivot). (cid:73) If A[j].key > pivot, the algorithm changes nothing. Induction step: And the range of > pivot cells expands by 1 (as j gets incremented). (cid:73) If A[j].key \u2264 pivot, we will swap A[j] with A[i + 1]. A[i + 1].key either . . . (cid:73) was \u2264 pivot (and i = j \u2212 1), in which case i (cid:48) = i + 1 is j and we swap A[j] with itself, and then have the same pattern after j increments, or (cid:73) was > pivot, in which case . . . (cid:73) After loop, Invariant implies that the first > pivot (if any) is in A[i + 1]. Hence swapping A[r ] and A[i + 1] gives a \u201cpartition\u201d of the desired form. IADS \u2013 Lecture 13 \u2013 slide 7 ",
    "13_7": "Results from Partition (cid:73) We might get a fairly balanced partition, with \u201cpivot\u201d lying near the middle (\u201csplit\u201d roughly halfway between p and r ). (cid:73) When this happens, the Divide-and-Conquer balance is like MergeSort (and also, we have \u0398(n) work at the \u201ctop level\u201d) (cid:73) Alternatively, we could get a very unbalanced partition, with one side empty or very small (cid:73) Then we are doing linear work to reduce the size of the problem to be solved only a tiny bit. More like BubbleSort. (cid:73) Or anything in between, depends on the original arrangement of keys (and what \u201crank\u201d pivot has among the keys in the array). IADS \u2013 Lecture 13 \u2013 slide 8 ",
    "13_8": "Running Time of QuickSort Tpartition(n) = \u0398(n) Partition QuickSort TquickSort(n) \u2208 = max1\u2264s\u2264n\u22121 Implies max1\u2264s\u2264n\u22121 + Tpartition(n) + \u0398(1) (cid:0)TquickSort(s) + TquickSort(n \u2212 s \u2212 1)(cid:1) (cid:0)TquickSort(s) + TquickSort(n \u2212 s \u2212 1)(cid:1) + \u0398(n). TquickSort(n) \u2208 \u0398(n2) To show \u2126(n2) you need a specific structured input (not too hard). IADS \u2013 Lecture 13 \u2013 slide 9 ",
    "13_9": "QuickSort The average-case running-time of a sorting algorithm is the average number of computational steps (comparisons) carried out on a uniform random permutation of the keys {1, . . . , n}. (cid:73) We don\u2019t say \u201camortized\u201d as we have a single computation, and are comparing input of exactly the same size. (cid:73) For sorting-algorithms, typically the running-time can be captured by the number of \u201ccomparisons\u201d (these measures tend to be asymptotically equivalent). (cid:73) Uniform random permutation means all permutations arise with same probability. The average-case running time of QuickSort is \u0398(n lg(n)). IADS \u2013 Lecture 13 \u2013 slide 10 ",
    "13_10": "QuickSort (cid:73) QuickSort can be very fast in practice. (cid:73) But performs badly \u2014 \u0398(n2) \u2014 on sorted and almost sorted arrays. Practical Improvements (cid:73) Di\ufb00erent choice of pivot (key of middle item, random) (cid:73) Refined partitioning routine (cid:73) Use InsertionSort for small arrays (similar to \u201cTimSort\u201d) RandomQuickSort (cid:73) The \u0398(n lg(n)) result for average-case can be shown to carry over characterize (expected) running-time for RandomQuickSort (choose the pivot randomly). IADS \u2013 Lecture 13 \u2013 slide 11 ",
    "13_11": "Sorting in Python The default sorting algorithm in python is \u201cTimsort\u201d, an optimized version of MergeSort developed by Tim Peters, a major contributor to the development of CPython. (cid:73) Does a pre-processing step looking for \u201cruns\u201d of strictly decreasing or (non-strict) increasing items. (cid:73) We understand how to handle \u201cruns\u201d (without sorting). (cid:73) Then merge the sorted runs, using InsertSort for short subarrays, and MergeSort for bigger subarrays. IADS \u2013 Lecture 13 \u2013 slide 12 ",
    "13_12": "Reading Material Personal Reading: (cid:73) QuickSort and its analysis, sections 7.1, 7.2 and 7.4 of [CLRS] (cid:73) \u201cQuickShort\u201d interview with Tony Hoare can be viewed at http://anothercasualcoder.blogspot.com/2015/03/ my-quickshort-interview-with-sir-tony.html (cid:73) To read about \u201cTimsort\u201d . . . read the listsort.txt file in the Objects directory from the python download. Source code (eg version 3.7.4) can be downloaded https://www.python.org/downloads/source/ IADS \u2013 Lecture 13 \u2013 slide 13 ",
    "14_1": "Directed and Undirected Graphs (cid:73) A graph is a mathematical structure consisting of a set of vertices and a set of edges connecting the vertices. Formally: G = (V , E ), where V is a set and E \u2286 V \u00d7 V . (cid:73) G = (V , E ) undirected if for all v , w \u2208 V : (v , w ) \u2208 E \u21d0\u21d2 (w , v ) \u2208 E . Otherwise directed. Directed \u223c arrows (one-way) Undirected \u223c lines (two-way) IADS \u2013 Lecture 14 \u2013 slide 2 ",
    "14_2": "A directed graph (cid:9) (cid:8) (cid:8) G = (V , E ) with vertex set V = 0, 1, 2, 3, 4, 5, 6 and edge set E = (0, 2), (0, 4), (0, 5), (1, 0), (2, 1), (2, 5), (3, 1), (3, 6), (4, 0), (4, 5), (6, 3), (6, 5) . (cid:9) IADS \u2013 Lecture 14 \u2013 slide 3 0236541",
    "14_3": "An undirected graph IADS \u2013 Lecture 14 \u2013 slide 4 badefgc",
    "14_4": "Examples of graphs in \u201creal life\u201d Road Maps. Edges represent streets and vertices represent crossings. IADS \u2013 Lecture 14 \u2013 slide 5 ",
    "14_5": "Examples (cont\u2019d) Computer Networks. Vertices represent computers and edges represent network connections (cables) between them. The World Wide Web. Vertices represent webpages, and edges represent hyperlinks. IADS \u2013 Lecture 14 \u2013 slide 6 ",
    "14_6": "Adjacency matrices Let G = (V , E ) be a graph with n vertices, with vertices numbered 0, . . . , n \u2212 1. The adjacency matrix of G is the n \u00d7 n matrix A = (aij )0\u2264i ,j\u2264n\u22121 with (cid:14) aij = 1 if there is an edge from vertex i to vertex j 0 otherwise. Python arrays are a bit strange to work with, being set up as \u201clists of lists\u201d. Alternatives are: (cid:73) Import NumPy, and use their true 2D arrays (cid:73) Define a mapping (i , j)\u2192 i \u2217 n + j (where n is the number of vertices) and work with a n \u2217 n or n \u2217 (n + 1)/2 sized 1D array in python. IADS \u2013 Lecture 14 \u2013 slide 7 ",
    "14_7": "Adjacency matrix (Example) \uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 \uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 0 0 0 1 0 0 0 IADS \u2013 Lecture 14 \u2013 slide 8 0236541",
    "14_8": "Adjacency lists Array with one entry for each vertex v , which is a list of all vertices adjacent to v . Example IADS \u2013 Lecture 14 \u2013 slide 9 02365412455103550610125634",
    "14_9": "Lists or matrices? Given: graph G = (V , E ), with n = |V |, m = |E |. For v \u2208 V , we write in(v ) for in-degree, out(v ) for out-degree. Questions to think about:? 1. Which data structure has faster (asymptotic) worst-case running-time, for checking if w is adjacent to v , for a given pair of vertices? 2. Which data structure has faster (asymptotic) worst-case running-time, for visiting all vertices w adjacent to v , for a given vertex v ? IADS \u2013 Lecture 14 \u2013 slide 10 ",
    "14_10": "Adjacency Matrices vs Adjacency Lists adjacency matrix adjacency list Space \u0398(n2) \u0398(n + m) Time to check if w adjacent to v Time to visit all w adjacent to v . \u0398(1) \u0398(out(v )) \u0398(n) \u0398(out(v )) Time to visit all edges \u0398(n2) \u0398(n + m) IADS \u2013 Lecture 14 \u2013 slide 11 ",
    "14_11": "Sparse and dense graphs G = (V , E ) graph with n vertices and m edges Observation: m \u2264 n2 (cid:73) G dense if m close to n2 (cid:73) G sparse if m much smaller than n2 What about planar graphs? \u201cKuratowski\u2019s criterion\u201d for Planar graphs has the Corollary that |E | \u2264 3|V | \u2212 6. IADS \u2013 Lecture 14 \u2013 slide 12 ",
    "14_12": "Graph traversals A traversal is a strategy for visiting all vertices of a graph. BFS = breadth-first search DFS = depth-first search General strategy: 1. Let v be an arbitrary vertex 2. Visit all vertices reachable from v 3. If there are vertices that have not been visited, let v be such a vertex and go back to 2. IADS \u2013 Lecture 14 \u2013 slide 13 ",
    "14_13": "BFS Visit all vertices reachable from v in the following order: (cid:73) v (cid:73) all neighbours of v (cid:73) all neighbours of neighbours of v that have not been visited yet (cid:73) all neighbours of neighbours of neighbours of v that have not been visited yet (cid:73) etc. IADS \u2013 Lecture 14 \u2013 slide 14 ",
    "14_14": "BFS (using a Queue) Algorithm bfs(G ) 1. 2. 3. 4. 5. Initialise Boolean array visited , setting all entries to false. Initialise Queue Q for all v \u2208 V do if visited [v ] = false then bfsFromVertex(G , v ) IADS \u2013 Lecture 14 \u2013 slide 15 ",
    "14_15": "BFS (using a Queue) Algorithm bfsFromVertex(G , v ) u\u2190 Q.dequeue() 1. visited [v ] = true 2. Q.enqueue(v ) 3. while not Q.isEmpty() do 4. 5. 6. 7. 8. visited [w ] = true Q.enqueue(w ) for all w adjacent to u do if visited [w ] = false then IADS \u2013 Lecture 14 \u2013 slide 16 ",
    "14_16": "Example for Breadth-First Search The video recording shows a demonstration of bfs on the graph below: IADS \u2013 Lecture 14 \u2013 slide 17 ",
    "14_17": "DFS Visit all vertices reachable from v in the following order: (cid:73) v (cid:73) some neighbor w of v that has not been visited yet (cid:73) some neighbor x of w that has not been visited yet (cid:73) etc., until the current vertex has no neighbour that has not been visited yet (cid:73) Backtrack to the first vertex that has a yet unvisited neighbour v (cid:48). (cid:73) Continue with v (cid:48), a neighbour, a neighbour of the neighbour, etc., backtrack, etc. IADS \u2013 Lecture 14 \u2013 slide 18 ",
    "14_18": "DFS (using a stack) Algorithm dfs(G ) 1. 2. 3. 4. 5. Initialise Boolean array visited , setting all to false Initialise Stack S for all v \u2208 V do if visited [v ] = false then dfsFromVertex(G , v ) IADS \u2013 Lecture 14 \u2013 slide 19 ",
    "14_19": "DFS (using a stack) Algorithm dfsFromVertex(G , v ) u\u2190 S.pop() 1. S.push(v ) 2. while not S.isEmpty() do 3. 4. 5. 6. 7. if visited [u] = false then visited [u] = true for all w adjacent to u do S.push(w ) IADS \u2013 Lecture 14 \u2013 slide 20 ",
    "14_20": "Example for Depth-First Search The video recording shows a demonstration of dfs on the graph below: IADS \u2013 Lecture 14 \u2013 slide 21 ",
    "14_21": "Reading Today\u2019s lecture: (cid:73) Graph representations in Section 22.1 (cid:73) Breadth-first search Section 22.2, Depth-first search Section 22.3 Next week: (cid:73) Showing why Depth-first search (and Breadth-first search) run in \u0398(n + m) time. (cid:73) Computing connected components and topological sort Section 22.4 IADS \u2013 Lecture 14 \u2013 slide 22 ",
    "15_1": "Recursive DFS (no explicit Stack) Algorithm dfs(G ) 1. Initialise Boolean array visited , setting all to false 2. for all v \u2208 V do 3. 4. if visited [v ] = false then dfsFromVertex(G , v ) Algorithm dfsFromVertex(G , v ) 1. visited [v ]\u2190 true for all w adjacent to v do if visited [w ] = false then dfsFromVertex(G , w ) 2. 3. 4. (cid:73) The recursive set-up eliminates the need for an explicit stack; (cid:73) We will have reversed prioritisation of the vertices adjacent to v . IADS \u2013 Lecture 15 \u2013 slide 2 ",
    "15_2": "Analysis of DFS Lemma During dfs(G ), dfsFromVertex(G , v ) is invoked exactly once for each vertex v . Proof. At least once: (cid:73) visited [v ] can only become true when dfsFromVertex(G , v ) is executed. (cid:73) If visited [v ] remains false, dfsFromVertex(G , v ) will eventually be called by line 4 of dfs(G ). At most once: (cid:73) First call of dfsFromVertex(G , v ) sets visited [v ] to true. (cid:73) After visited [v ] is true, dfsFromVertex(G , v ) is never called again. (\u201cAt most once\u201d is also true for Stack dfs, but \u201cat least once\u201d is not. dfsFromVertex\u201d is more to \u201dstart a component\u201d in the Stack version) IADS \u2013 Lecture 15 \u2013 slide 3 ",
    "15_3": "Analysis of DFS (cont\u2019d) (cid:80) (cid:80) Lemma For a directed graph, For an undirected graph, v\u2208V out-degree(v ) = m. v\u2208V deg(v ) = 2m. Proof. Every edge is counted exactly once on both sides of the equation (for directed). For the undirected case, every edge is counted twice on the lhs. IADS \u2013 Lecture 15 \u2013 slide 4 ",
    "15_4": "Analysis of recursive DFS G = (V , E ) graph with n vertices and m edges Algorithm dfs(G ) 1. Initialise Boolean array visited , setting all to false 2. for all v \u2208 V do 3. 4. (cid:73) dfs(G ): Ignoring calls to dfsFromVertex, total time \u0398(n) (cid:73) dfsFromVertex(v ) is called at most once for every vertex v , and does if visited [v ] = false then dfsFromVertex(G , v ) \u0398(out-degree(v )) work, excluding recursive calls. Overall time: (cid:80) (cid:80) (cid:16) T (n, m) = \u0398(n) + n + = \u0398 = \u0398(n + m) v\u2208V \u0398(out-degree(v )) v\u2208V out-degree(v ) (cid:17) IADS \u2013 Lecture 15 \u2013 slide 5 ",
    "15_5": "Adjacency List or Adjacency Matrix? We said each call to dfsFromVertex(v ) takes \u0398(out-degree(v )) time (excluding recursive calls). Algorithm dfsFromVertex(G , v ) 1. visited [v ]\u2190 true 2. 3. 4. for all w adjacent to v do if visited [w ] = false then dfsFromVertex(G , w ) If we are iterating over \u201call w adjacent to v \u201d in \u0398(out-degree(v )) time, then we must be using an Adjacency list structure. IADS \u2013 Lecture 15 \u2013 slide 6 ",
    "15_6": "Analysis of original DFS Compare the two dfsFromVertex(G , v ) methods: Algorithm dfsFromVertex(G , v ) visited[v ]\u2190 true 1. 2. 3. 4. for all w adjacent to v do if visited[w ] = false then dfsFromVertex(G , w ) Algorithm dfsFromVertex(G , v ) u\u2190 S.pop() 1. S.push(v ) 2. while not S.isEmpty() do 3. 4. 5. 6. 7. if visited[u] = false then visited[u] = true for all w adjacent to u do S.push(w ) visited[v ]\u2190 true \u2194 S.pop(); visited[v ]\u2190 true dfsFromVertex(v ) \u2194 for all w adjacent to v ; S.push(w ); while Recursive: marks v as \u201cvisited\u201d, then calls dfsFromVertex for unvisited adjacent vertices Iterative: marks v as \u201cvisited\u201d after \u201cpopping\u201d it, then \u201cpushes\u201d all adjacent vertices. However, the number of Stack operations for v is bounded in terms of the number of edges into v \u21d2 the overall runtime for our original dfs is still \u0398(n + m). IADS \u2013 Lecture 15 \u2013 slide 7 ",
    "15_7": "DFS Forests A DFS traversing a graph builds up a forest whose vertices are all vertices of the graph and whose edges are all vertices traversed during the DFS. Definition A vertex w is a child of a vertex v in the DFS forest if dfsFromVertex(G , w ) is called from dfsFromVertex(G , v ). IADS \u2013 Lecture 15 \u2013 slide 8 ",
    "15_8": "DFS Forests Example Recall Q2 of tutorial sheet 5 on how the connected components can vary depending on the order in which we consider vertices at the top-level of dfs. IADS \u2013 Lecture 15 \u2013 slide 9 02365410245163",
    "15_9": "Topological Sorting Example: 10 tasks to be carried out. Some of them depend on others. (cid:73) Task 0 must be completed before Task 1 can be started. (cid:73) Task 1 and Task 2 must be done before Task 3 can start. (cid:73) Task 4 must be done before Task 0 or Task 2 can start. (cid:73) Task 5 must be done before Task 0 or Task 4 can start. (cid:73) Task 6 must be done before Task 4, 5 or 7 can start. (cid:73) Task 7 must be done before Task 0 or Task 9 can start. (cid:73) Task 8 must be done before Task 7 or Task 9 can start. (cid:73) Task 9 must be done before Task 2 or Task 3 can start. IADS \u2013 Lecture 15 \u2013 slide 10 ",
    "15_10": "Topological order Definition Let G = (V , E ) be a directed graph. A topological order of G is a total order \u227a of the vertex set V such that for all edges (v , w ) \u2208 E we have v \u227a w . (in some fields this is called a linear extension) IADS \u2013 Lecture 15 \u2013 slide 11 ",
    "15_11": "Tasks as a (directed) graph Does this graph have a topological order? Yes. One topological sort is: 8 \u227a 6 \u227a 7 \u227a 9 \u227a 5 \u227a 4 \u227a 2 \u227a 0 \u227a 1 \u227a 3. IADS \u2013 Lecture 15 \u2013 slide 12 5319476820",
    "15_12": "Topological order (cont\u2019d) A digraph that has a cycle does not have a topological order. Definition A DAG (directed acyclic graph) is a digraph without cycles. Theorem A digraph has a topological order if and only if it is a DAG. IADS \u2013 Lecture 15 \u2013 slide 13 ",
    "15_13": "Classification of vertices during recursive DFS G = (V , E ) graph, v \u2208 V . Consider dfs(G ). (cid:73) v is finished if dfsFromVertex(G , v ) has been completed. Vertices can be in the following states: (cid:73) not yet visited (let us call a vertex in this state white), (cid:73) visited, but not yet finished (grey). (cid:73) finished (black). IADS \u2013 Lecture 15 \u2013 slide 14 ",
    "15_14": "Classification of vertices during recursive DFS (cont\u2019d) Lemma Let G be a graph and v a vertex of G . Consider the moment during the execution of dfs(G ) when dfsFromVertex(G , v ) is started. Then for all vertices w we have: 1. If w is white and reachable from v , then w will be black before v . 2. If w is grey, then v is reachable from w . IADS \u2013 Lecture 15 \u2013 slide 15 ",
    "15_15": "Topological sorting G = (V , E ) digraph. Define order on V as follows: v \u227a w \u21d0\u21d2 w becomes black before v . Theorem If G is a DAG then \u227a is a topological order. Proof. Suppose (v , w ) \u2208 E . Consider dfsFromVertex(G , v ). (cid:73) If w is already black, then v \u227a w (and this is what we want). (cid:73) If w is white, then by Lemma part 1., w will be black before v . Thus v \u227a w . (cid:73) If w is grey, then by Lemma part 2. v is reachable from w . So there is a path p from w to v . Path p and edge (v , w ) together form a cycle. Contradiction! (G is acyclic . . .) IADS \u2013 Lecture 15 \u2013 slide 16 ",
    "15_16": "Topological sorting implemented Algorithm topSort(G ) 1. Initialise array state by setting all entries to white. 2. Initialise linked list L 3. for all v \u2208 V do 4. 5. 6. print L if state[v ] = white then sortFromVertex(G , v ) IADS \u2013 Lecture 15 \u2013 slide 17 ",
    "15_17": "Topological sorting implemented Algorithm sortFromVertex(G , v ) state[v ]\u2190 grey 1. 2. 3. 4. 5. 6. 7. 8. 9. L.insertFirst(v ) state[v ]\u2190 black for all w adjacent to v do if state[w ] = white then sortFromVertex(G , w ) else if state[w ] = grey then print \u201cG has a cycle\u201d halt Di\ufb00erence from dfs itself - the order the vertices get added to the list. Running-time is again \u0398(n + m) IADS \u2013 Lecture 15 \u2013 slide 18 ",
    "15_18": "Example Use the algorithm topSort to compute a topological sort of this graph (video). IADS \u2013 Lecture 15 \u2013 slide 19 5319476820",
    "15_19": "Connected components of an undirected graph G = (V , E ) undirected graph Definition (cid:73) A subset C of V is connected if for all v , w \u2208 C there is a path from v to w (if G is directed, say strongly connected). (cid:73) A connected component of G is a maximum connected subset C of V . (no connected subset C (cid:48) of V strictly contains C . (cid:73) G is connected if it only has one connected component, that is, if for all vertices v , w there is a path from v to w . IADS \u2013 Lecture 15 \u2013 slide 20 ",
    "15_20": "Connected components - undirected (cont\u2019d) (cid:73) Each vertex of an undirected graph is contained in exactly one connected component. (cid:73) For each vertex v of an undirected graph, the connected component that contains v is precisely the set of all vertices that are reachable from v . For an undirected graph G , dfsFromVertex(G , v ) visits exactly the vertices in the connected component of v . And the same is true for bfsFromVertex(G , v ) (either will do!) IADS \u2013 Lecture 15 \u2013 slide 21 ",
    "15_21": "Connected components - undirected (cont\u2019d) Algorithm connComp(G ) 1. Initialise Boolean array visited by setting all entries to false 2. for all v \u2208 V do 3. 4. 5. if visited [v ] = false then print \u201cNew Component\u201d ccFromVertex(G , v ) Algorithm ccFromVertex(G , v ) 1. visited [v ]\u2190 true 2. print v 3. 4. 5. for all w adjacent to v do if visited [w ] = false then ccFromVertex(G , w ) IADS \u2013 Lecture 15 \u2013 slide 22 ",
    "15_22": "Reading From [CLRS] as usual: (cid:73) Computing topological sort - Section 22.4 Hope you get a break over the holidays! And \u201csee\u201d you in 2022. IADS \u2013 Lecture 15 \u2013 slide 23 "
}