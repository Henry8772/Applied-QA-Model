{
    "1_1": {
        "content": "What is an Intelligent \nAgent?\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 1",
        "page": "1"
    },
    "1_2": {
        "content": "Structure\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 1",
        "page": "2"
    },
    "1_3": {
        "content": "Structure\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 1",
        "page": "3"
    },
    "1_4": {
        "content": "An agent\n\nPerceives its environment,\n\nThrough its sensors,\n\nThen achieves its goals,\n\nBy acting on its environment via actuators.\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 1",
        "page": "4"
    },
    "1_5": {
        "content": "Categorise agents\n\nEnvironment\n\nGoals\n\nPercepts\n\nActions\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 1",
        "page": "5"
    },
    "1_6": {
        "content": "Example: Mail sorting\n\nConveyor belt of letters\n\nRoute letter into correct bin\n\nArray of pixel intensities\n\nRoute letter into bin\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 1",
        "page": "6"
    },
    "1_7": {
        "content": "Smart Home\n\nINF2D: REASONING AND AGENTS\n\n",
        "lecture": " 1",
        "page": "7"
    },
    "1_8": {
        "content": "Example: Smart home\n\noccupants enter and leave house, occupants enter and \n\nleave rooms; daily variation in outside light and \n\ntemperature \n\noccupants warm, room lights are on when room is \n\noccupied, house energy efficient\n\nsignals from temperature sensors, movement sensors, \n\nclock, sound sensor, weather sensor\n\nroom heaters on/off, lights on/off\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 1",
        "page": "8"
    },
    "1_9": {
        "content": "Example: Autonomous car\n\nstreets, other vehicles, pedestrians, traffic \n\nsignals/lights/signs\n\nsafe, fast, legal trip\n\ncamera, GPS signals, speedometer, sonar\n\nsteer, accelerate, brake\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 1",
        "page": "9"
    },
    "1_10": {
        "content": "Type of Intelligent \nAgents\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 1",
        "page": "10"
    },
    "1_11": {
        "content": "Simple Reflex Agents\n\nAction depends only on immediate percepts.\n\nImplement by condition-action rules. \n\nExample: \n\n\u25e6 Agent: Mail sorting robot\n\n\u25e6 Environment: Conveyor belt of letters\n\n\u25e6 Rule: e.g. city=Edinburgh \u2192 put in Scotland bag\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 1",
        "page": "11"
    },
    "1_12": {
        "content": "Simple Reflex \nAgents\n\nfunction SIMPLE-REFLEX-AGENT(percept) \n\nreturns action\n\npersistent: rules (set of condition-action rules)\n\nstate \u2190 INTERPRET-INPUT(percept)\n\nrule \u2190 RULE-MATCH(state, rules)\n\naction \u2190 rule.ACTION\n\nreturn action\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 1",
        "page": "12"
    },
    "1_13": {
        "content": "Model-Based Reflex Agents\n\nAction may depend on history or unperceived aspects of the world.\n\nNeed to maintain internal world model.\n\nExample: \n\nAgent: robot vacuum cleaner\n\nEnvironment: dirty room, furniture\n\nModel: map of room, which areas already cleaned\n\nSensor/model tradeoff.\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 1",
        "page": "13"
    },
    "1_14": {
        "content": "Model-Based \nReflex Agents\n\nfunction REFLEX-AGENT-WITH-STATE(percept) \n\nreturns action\n\npersistent: state, description of current world state\n\nmodel, description of how the next state \n\ndepends on current state and action\n\nrules,  a set of condition-action rules\n\naction, the most recent action, initially none\n\nstate \u2190 UPDATE-STATE(state, action, percept, model)\n\nrule \u2190 RULE-MATCH(state, rules)\n\naction \u2190 rule.ACTION\nreturn action\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 1",
        "page": "14"
    },
    "1_15": {
        "content": "Goal-Based Agents\n\nAgents so far have fixed, implicit goals.\n\nWe want agents with variable goals.\n\nForming plans to achieve goals is a topic for later.\n\nExample: \n\n\u25e6 Agent: robot maid\n\n\u25e6 Environment: house & people.\n\n\u25e6 Goals: clean clothes, tidy room, table laid, etc\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 1",
        "page": "15"
    },
    "1_16": {
        "content": "Goal-\nBased \nAgents\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 1",
        "page": "16"
    },
    "1_17": {
        "content": "Utility-Based Agents\n\nAgents so far have had a single goal. \n\nAgents may have to juggle conflicting goals. \n\nNeed to optimise utility over a range of goals.\n\nUtility:  measure of goodness (a real number).\n\nCombine with probability of success to get expected utility. \n\nExample: \n\n\u25e6 Agent: autonomous car.\n\u25e6 Environment: roads, vehicles, signs, etc.\n\u25e6 Goals: stay safe, reach destination, be quick, obey law, save fuel, etc.\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 1",
        "page": "17"
    },
    "1_18": {
        "content": "Utility-\nBased \nAgents\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 1",
        "page": "18"
    },
    "1_19": {
        "content": "Learning Agents\n\nHow do agents improve their performance in the light of experience?\n\n\u25e6 Generate problems which will test performance.\n\n\u25e6 Perform activities according to rules, goals, model, utilities, etc.\n\n\u25e6 Monitor performance and identify non-optimal activity.\n\n\u25e6 Identify and implement improvements\n\nWe will not be covering learning agents, but this topic is dealt with in several honours-level \n\ncourses (see also R&N, Ch. 18-21).\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 1",
        "page": "19"
    },
    "1_20": {
        "content": "Exercise\n\nConsider a chess playing \nprogram. \n\nWhat sort of agent would it need \nto be?\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 1",
        "page": "20"
    },
    "1_21": {
        "content": "Solution\n\nSimple-reflex agent: but some actions require \nsome memory (e.g. castling)\n\nModel-based reflex agent: but needs to reason \nabout future\n\nGoal-based agent: but only has one goal\n\nUtility-based agent: might consider multiple goals \nwith limited lookahead\n\nLearning agent: learns from experience or self-play\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 1",
        "page": "21"
    },
    "1_22": {
        "content": "Types of \nEnvironments\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 1",
        "page": "22"
    },
    "1_23": {
        "content": "Observable?\n\nFULLY\n\nPARTIALLY\n\nSource\n\nINF2D: REASONING AND AGENTS\n\nSource\n\n24\n\n",
        "lecture": " 1",
        "page": "23"
    },
    "1_24": {
        "content": "Observable?\n\nFULLY\n\nPARTIALLY\n\nagent's sensors describe \n\nenvironment fully \n\nvs.\n\nsome parts of the environment \n\nnot visible; noisy sensors\n\nSource\n\nINF2D: REASONING AND AGENTS\n\nSource\n\n25\n\n",
        "lecture": " 1",
        "page": "24"
    },
    "1_25": {
        "content": "Deterministic?\n\nDETERMINISTIC\n\nSTOCHASTIC\n\nINF2D: REASONING AND AGENTS\n\n26\n\nSource\n\n",
        "lecture": " 1",
        "page": "25"
    },
    "1_26": {
        "content": "Deterministic?\n\nDETERMINISTIC\n\nSTOCHASTIC\n\nnext state fully determined by \ncurrent state and agent's actions\n\nvs.\n\nrandom changes - cannot be \n\npredicted exactly\n\nAn environment may appear stochastic if it is only partially observable.\n\nINF2D: REASONING AND AGENTS\n\n27\n\nSource\n\n",
        "lecture": " 1",
        "page": "26"
    },
    "1_27": {
        "content": "Sequential?\n\nEPISODIC\n\nSEQUENTIAL\n\nSource\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 1",
        "page": "27"
    },
    "1_28": {
        "content": "Sequential?\n\nEPISODIC\n\nSEQUENTIAL\n\nnext episode does not depend \n\non previous actions\n\nvs.\n\nactions affect the future\n\nSource\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 1",
        "page": "28"
    },
    "1_29": {
        "content": "Static?\n\nSTATIC\n\nDYNAMIC\n\nINF2D: REASONING AND AGENTS\n\n30\n\nSource\n\n",
        "lecture": " 1",
        "page": "29"
    },
    "1_30": {
        "content": "Static?\n\nSTATIC\n\nDYNAMIC\n\nenvironment unchanged while \n\nagent deliberates\n\nvs.\n\nenvironment can change at any \n\ntime or even continuously\n\nINF2D: REASONING AND AGENTS\n\n31\n\nSource\n\n",
        "lecture": " 1",
        "page": "30"
    },
    "1_31": {
        "content": "Discrete?\n\nDISCRETE\n\nCONTINUOUS\n\nSource\n\nSource\n\nINF2D: REASONING AND AGENTS\n\n32\n\n",
        "lecture": " 1",
        "page": "31"
    },
    "1_32": {
        "content": "Discrete?\n\nDISCRETE\n\nCONTINUOUS\n\npercepts, actions and episodes \n\nare discrete\n\nvs.\n\ncontinuous time flow\n\nSource\n\nSource\n\nINF2D: REASONING AND AGENTS\n\n33\n\n",
        "lecture": " 1",
        "page": "32"
    },
    "1_33": {
        "content": "How many agents?\n\nSINGLE\n\nMULTI-AGENT\n\nINF2D: REASONING AND AGENTS\n\n34\n\nSource\n\n",
        "lecture": " 1",
        "page": "33"
    },
    "1_34": {
        "content": "How many agents?\n\nSINGLE\n\nMULTI-AGENT\n\nConsider how many object must be modelled as agents.\n\nElement of choice over which objects are considered agents.\n\nINF2D: REASONING AND AGENTS\n\n35\n\nSource\n\n",
        "lecture": " 1",
        "page": "34"
    },
    "1_35": {
        "content": "Summary\n\nAn agent might have any combination of these properties: \n\u25e6 from \u201cbenign\u201d: i.e., fully observable, deterministic, episodic, static, discrete and single \n\nagent \n\n\u25e6 to \u201cchaotic\u201d: partially observable, stochastic, sequential, dynamic, continuous and multi-\n\nagent\n\nWhat are the properties of the environment that would be experienced by \n\u25e6 a mail-sorting robot? \n\n\u25e6 a smart home? \n\n\u25e6 a car-driving robot?\n\nINF2D: REASONING AND AGENTS\n\n36\n\n",
        "lecture": " 1",
        "page": "35"
    },
    "1_36": {
        "content": "Why?\n\n\u25e6 Understanding what is an agent and how it can be modelled.\n\n\u25e6 Understanding the environment and the assumptions or considerations that need to be \n\nmade.\n\n\u25e6 Making the right design decisions and choosing the right tools.\n\n\u25e6 Managing the complexity.\n\nINF2D: REASONING AND AGENTS\n\n37\n\n",
        "lecture": " 1",
        "page": "36"
    },
    "2_1": {
        "content": "Problem-solving \nAgents\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 2",
        "page": "1"
    },
    "2_2": {
        "content": "Problem-\nsolving \nagents\n\nfunction SIMPLE-PROBLEM-SOLVING-AGENT(percept) returns an action\n\npersistent:   seq, an action sequence, initially empty\n\nstate, some description of the current world state\ngoal, a goal, initially null\nproblem, a problem formulation\n\nstate \uf0df UPDATE-STATE(state, percept)\nif seq is empty then do\n\ngoal \uf0df FORMULATE-GOAL(state)\nproblem \uf0df FORMULATE-PROBLEM(state, goal)\nseq \uf0df SEARCH(problem)\nif seq = failure then return a null action\n\naction \uf0df FIRST(seq)\nseq \uf0df REST(seq)\nreturn action\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 2",
        "page": "2"
    },
    "2_3": {
        "content": "Example: \nRomania\n\nOn holiday in Romania.\n\nCurrently in Arad.\n\nFlight leaves tomorrow from \nBucharest.\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 2",
        "page": "3"
    },
    "2_4": {
        "content": "Example: Romania\n\nOn holiday in Romania; currently in Arad.\nFlight leaves tomorrow from Bucharest\nFormulate goal:\n\u25e6 be in Bucharest\n\nFormulate problem:\n\u25e6 states: various cities\n\u25e6 actions: drive between cities\n\nFind solution:\n\u25e6 sequence of cities, e.g., Arad, Sibiu, Fagaras, Bucharest\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 2",
        "page": "4"
    },
    "2_5": {
        "content": "Problem types\n\nDeterministic, fully observable \u2192 single-state problem\n\u25e6 Agent knows exactly which state it will be in; solution is a sequence\n\nNon-observable \u2192 sensorless problem (conformant problem)\n\u25e6 Agent may have no idea where it is; solution is a sequence\n\nNondeterministic and/or partially observable \u2192 contingency problem\n\u25e6 percepts provide new information about current state\n\u25e6 often interleave search, execution\n\nUnknown state space \u2192 exploration problem\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 2",
        "page": "5"
    },
    "2_6": {
        "content": "Example: \nvacuum world\n\nSingle-state:\nStart in 5 \nSolution?\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 2",
        "page": "6"
    },
    "2_7": {
        "content": "Example: \nvacuum world\n\nSingle-state:\nStart in 5\nSolution?\n[Right, Suck]\n\nSensorless:\nStart in {1,2,3,4,5,6,7,8}\ne.g. Right goes to {2,4,6,8} \nSolution?\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 2",
        "page": "7"
    },
    "2_8": {
        "content": "Example: \nvacuum world\n\nSingle-state:\nStart in 5\nSolution?\n[Right, Suck]\n\nSensorless:\nStart in {1,2,3,4,5,6,7,8}\ne.g. Right goes to {2,4,6,8} \nSolution?\n[Right, Suck, Left, Suck]\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 2",
        "page": "8"
    },
    "2_9": {
        "content": "Example: \nvacuum world\n\nContingency:\n\n\u25e6 Nondeterministic: Suck may \n\ndirty a clean carpet\n\n\u25e6 Partially observable: can \n\nonly see dirt at current \nlocation.\n\n\u25e6 Percept: [Left, Clean]\n\ni.e., start in 5 or 7\nSolution?\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 2",
        "page": "9"
    },
    "2_10": {
        "content": "Example: \nvacuum world\n\nContingency:\n\u25e6 Nondeterministic: Suck may \n\ndirty a clean carpet\n\n\u25e6 Partially observable: can \n\nonly see dirt at current \nlocation.\n\n\u25e6 Percept: [Left, Clean]\n\ni.e., start in 5 or 7\nSolution?\n[Right, if dirt then Suck]\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 2",
        "page": "10"
    },
    "2_11": {
        "content": "Problem Formulation\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 2",
        "page": "11"
    },
    "2_12": {
        "content": "Single-state problem formulation\n\nInitial State\n\n\u2022 e.g. \u201cin Arad\u201d\n\nActions or Successor function\n\n\u2022 S(x) = set of action\u2013state pairs \n\u2022 e.g. S(Arad) = {<Arad \u2192 Zerind, Zerind>, \u2026 }\n\nGoal test\n\n\u2022 explicit \n\u2022 implicit\n\ne.g. x = \u201cin Bucharest\"\ne.g. Checkmate(x)\n\nPath cost (additive)\n\n\u2022 e.g. sum of distances, number of actions executed, etc.\n\u2022 c(x,a,y) is the step cost of taking action a in state x to reach state y, assumed to be \u2265 0\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 2",
        "page": "12"
    },
    "2_13": {
        "content": "Single-state problem formulation\n\nInitial State\n\n\u2022 e.g. \u201cin Arad\u201d\n\nActions or Successor function\n\nA solution is a sequence of actions leading from the initial state to a goal \n\n\u2022 S(x) = set of action\u2013state pairs \n\u2022 e.g. S(Arad) = {<Arad \u2192 Zerind, Zerind>, \u2026 }\n\nstate, i.e. a state that succeeds the goal test.\n\nGoal test\n\n\u2022 explicit \n\u2022 implicit\n\ne.g. x = \u201cin Bucharest\"\ne.g. Checkmate(x)\n\nPath cost (additive)\n\n\u2022 e.g. sum of distances, number of actions executed, etc.\n\u2022 c(x,a,y) is the step cost of taking action a in state x to reach state y, assumed to be \u2265 0\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 2",
        "page": "13"
    },
    "2_14": {
        "content": "Selecting a state space\n\nReal world is absurdly complex \n\n\u2192 state space must be abstracted for problem solving\n\n(Abstract) state = set of real states\n\n(Abstract) action = complex combination of real actions\n\u25e6 e.g., \"Arad \u2192 Zerind\" represents a complex set of possible routes, detours, rest stops, \n\netc. \n\n\u25e6 For guaranteed realizability, any real state \"in Arad\u201c must get to some real state \"in \n\nZerind\"\n\n(Abstract) solution = set of real paths that are solutions in the real world\n\nEach abstract action should be \"easier\" than the original problem.\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 2",
        "page": "14"
    },
    "2_15": {
        "content": "Example: Vacuum world\n\nStates\n\nActions\n\nGoal test\n\nPath cost (additive)\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 2",
        "page": "15"
    },
    "2_16": {
        "content": "Example: Vacuum world\n\nStates\n\n\u2022 Pair of dirt and robot locations\n\nActions\n\n\u2022 Left, Right, Suck\n\nGoal test\n\n\u2022 No dirt at any location\n\nPath cost (additive)\n\n\u2022 1 per action\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 2",
        "page": "16"
    },
    "2_17": {
        "content": "Example: \nVacuum world\n\nStates\n\n\u2022 Pair of dirt and robot locations\n\nActions\n\n\u2022 Left, Right, Suck\n\nGoal test\n\n\u2022 No dirt at any location\n\nPath cost (additive)\n\n\u2022 1 per action\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 2",
        "page": "17"
    },
    "2_18": {
        "content": "Example: 8-puzzle\n\nStates\n\nActions\n\nGoal test\n\nPath cost (additive)\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 2",
        "page": "18"
    },
    "2_19": {
        "content": "Example: 8-puzzle\n\nStates\n\n\u2022 Integer location of tiles\n\nActions\n\n\u2022 Move blank left, right, up, down\n\nGoal test\n\n\u2022 = Goal state (given)\n\nPath cost (additive)\n\n\u2022 1 per move\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 2",
        "page": "19"
    },
    "2_20": {
        "content": "Example: 8-puzzle\n\nStates\n\n\u2022 Integer location of tiles\n\nActions\n\n\u2022 Move blank left, right, up, down\n\nGoal test\n\n\u2022 = Goal state (given)\n\nPath cost (additive)\n\n\u2022 1 per move\n\nNP-\nHard\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 2",
        "page": "20"
    },
    "2_21": {
        "content": "Example: Robotic assembly\n\nStates\n\n\u2022 Real-valued coordinates of robot joint angles\n\u2022 Parts of the object to be assembled\n\nActions\n\n\u2022 Continuous motions of robot joints\n\nGoal test\n\n\u2022 = complete assembly\n\nPath cost (additive)\n\n\u2022 Time to execute\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 2",
        "page": "21"
    },
    "2_22": {
        "content": "Searching for \nSolutions\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 2",
        "page": "22"
    },
    "2_23": {
        "content": "Tree search algorithms\n\nfunction TREE-SEARCH(problem) returns a solution, or failure\n\ninitialize the frontier using the initial state of problem\n\nloop do\n\nif the frontier is empty then return failure\n\nchoose a leaf node and remove it from the frontier\n\nif the node contains a goal state then return the corresponding solution\n\nexpand the chosen node, adding the resulting nodes to the frontier\n\nINF2D: REASONING AND AGENTS\n\n24\n\n",
        "lecture": " 2",
        "page": "23"
    },
    "2_24": {
        "content": "Tree search example\n\nINF2D: REASONING AND AGENTS\n\n25\n\n",
        "lecture": " 2",
        "page": "24"
    },
    "2_25": {
        "content": "Tree search example\n\nINF2D: REASONING AND AGENTS\n\n26\n\n",
        "lecture": " 2",
        "page": "25"
    },
    "2_26": {
        "content": "Tree search example\n\nINF2D: REASONING AND AGENTS\n\n27\n\n",
        "lecture": " 2",
        "page": "26"
    },
    "2_27": {
        "content": "Implementation: \nstates vs. nodes\n\nA state is a (representation of) a physical \nconfiguration\n\nA node is a book-keeping data structure \nconstituting part of a search tree; includes \nstate, parent node, action, path cost\n\nUsing these it is easy to compute the \ncomponents for a child node. \n(The CHILD-NODE function)\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 2",
        "page": "27"
    },
    "2_28": {
        "content": "Implementation: general tree search\n\nfunction TREE-SEARCH(problem) returns a solution, or failure\n\ninitialize the frontier using the initial state of problem\nloop do\n\nif the frontier is empty then return failure\nchoose a leaf node and remove it from the frontier\nif the node contains a goal state then return the corresponding solution\nexpand the chosen node, adding the resulting nodes to the frontier\n\nfunction CHILD-NODE(problem, parent, action) returns a node\n\nreturn a node with\n\nSTATE = problem.RESULT(parent.STATE, action),\nPARENT = parent, ACTION = action,\nPATH-COST = parent.PATH-COST + problem.STEP-COST(parent.STATE, action)\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 2",
        "page": "28"
    },
    "2_29": {
        "content": "Summary\n\nProblem formulation usually requires abstracting away real-world \ndetails to define a state space that can feasibly be explored.\n\nINF2D: REASONING AND AGENTS\n\n30\n\n",
        "lecture": " 2",
        "page": "29"
    },
    "2_30": {
        "content": "Why?\n\n\u25e6 Formulating problems in a way that a computer can understand.\n\n\u25e6 Breaking down the problem and its parameters.\n\n\u25e6 Clarifying the possible actions and assumptions about them.\n\n\u25e6 Creating structures where we can methodically and systematically search for solutions.\n\nINF2D: REASONING AND AGENTS\n\n31\n\n",
        "lecture": " 2",
        "page": "30"
    },
    "3_1": {
        "content": "Search strategies\n\nA search strategy is defined by picking the order of node expansion.\n\u25e6 Nodes are taken from the frontier.\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 3",
        "page": "1"
    },
    "3_2": {
        "content": "Evaluating search strategies\n\ncompleteness: does it always \nfind a solution if one exists?\n\ntime complexity: number of \nnodes generated / expanded\n\nspace complexity: maximum \nnumber of nodes in memory\n\noptimality: does it always \nfind a least-cost solution?\n\nTime and space complexity are \nmeasured in terms of: \n\u25e6 b: maximum branching factor of the \n\nsearch tree\n\n\u25e6 d: depth of the least-cost solution\n\n\u25e6 m: maximum depth of the state \n\nspace (may be \u221e)\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 3",
        "page": "2"
    },
    "3_3": {
        "content": "Recall: Tree \nSearch\n\nfunction TREE-SEARCH(problem) returns a solution, or failure\n\ninitialize the frontier using the initial state of problem\nloop do\n\nif the frontier is empty then return failure\nchoose a leaf node and remove it from the frontier\nif the node contains a goal state then return the corresponding solution\nexpand the chosen node, adding the resulting nodes to the frontier\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 3",
        "page": "3"
    },
    "3_4": {
        "content": "Repeated \nstates\n\nFailure to detect repeated \nstates can turn a linear\nproblem into an exponential \none!\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 3",
        "page": "4"
    },
    "3_5": {
        "content": "Graph \nsearch\n\nAugment TREE-SEARCH with a new \ndata-structure: \n\n\u2022 the explored set (closed \nlist), which remembers every \nexpanded node\n\n\u2022 newly expanded nodes \nalready in explored set are \ndiscarded\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 3",
        "page": "5"
    },
    "3_6": {
        "content": "Breadth-first search\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 3",
        "page": "6"
    },
    "3_7": {
        "content": "Breadth-first \nsearch\n\nOut\n\nA\n\nExpand shallowest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a FIFO queue, \ni.e., new successors go at \nend\n\nIn\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 3",
        "page": "7"
    },
    "3_8": {
        "content": "Breadth-first \nsearch\n\nOut\n\nA B C\n\nIn\n\nExpand shallowest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a FIFO queue, \ni.e., new successors go at \nend\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 3",
        "page": "8"
    },
    "3_9": {
        "content": "Out\n\nB C D E\n\nIn\n\nBreadth-first \nsearch\n\nExpand shallowest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a FIFO queue, \ni.e., new successors go at \nend\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 3",
        "page": "9"
    },
    "3_10": {
        "content": "Out\n\nIn\n\nC D E\n\nF G\n\nBreadth-first \nsearch\n\nExpand shallowest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a FIFO queue, \ni.e., new successors go at \nend\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 3",
        "page": "10"
    },
    "3_11": {
        "content": "Breadth-\nfirst \nsearch \nalgorithm\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 3",
        "page": "11"
    },
    "3_12": {
        "content": "Properties of breadth-first search\n\nComplete?\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 3",
        "page": "12"
    },
    "3_13": {
        "content": "Properties of breadth-first search\n\nComplete?\nYes (if b is finite)\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 3",
        "page": "13"
    },
    "3_14": {
        "content": "Properties of breadth-first search\n\nComplete?\nYes (if b is finite)\n\nTime complexity?\nb+b2+b3+\u2026 +bd = O(bd)  (worst-case)\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 3",
        "page": "14"
    },
    "3_15": {
        "content": "Properties of breadth-first search\n\nComplete?\nYes (if b is finite)\n\nTime complexity?\nb+b2+b3+\u2026 +bd = O(bd) (worst-case)\n\nSpace complexity?\nO(bd) (keeps every node in memory)\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 3",
        "page": "15"
    },
    "3_16": {
        "content": "Properties of breadth-first search\n\nComplete?\nYes (if b is finite)\n\nTime complexity?\nb+b2+b3+\u2026 +bd = O(bd) (worst-case)\n\nSpace complexity?\nO(bd) (keeps every node in memory)\n\nOptimal?\nYes (if cost = 1 per step)\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 3",
        "page": "16"
    },
    "3_17": {
        "content": "Properties of breadth-first search\n\nComplete?\nYes (if b is finite)\n\nTime complexity?\nb+b2+b3+\u2026 +bd = O(bd) (worst-case)\n\nSpace complexity?\nO(bd) (keeps every node in memory)\n\nOptimal?\nYes (if cost = 1 per step)\n\nthen optimal \n\nsolution is closest \n\nto start!\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 3",
        "page": "17"
    },
    "3_18": {
        "content": "Properties of breadth-first search\n\nComplete?\nYes (if b is finite)\n\nTime complexity?\nb+b2+b3+\u2026 +bd = O(bd) (worst-case)\n\nSpace complexity?\nO(bd) (keeps every node in memory)\n\nOptimal?\nYes (if cost = 1 per step)\n\nSpace is the bigger problem (more than time)\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 3",
        "page": "18"
    },
    "3_19": {
        "content": "Depth-first search\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 3",
        "page": "19"
    },
    "3_20": {
        "content": "In\n\nOut\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nA\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 3",
        "page": "20"
    },
    "3_21": {
        "content": "In\n\nOut\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nB C A\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 3",
        "page": "21"
    },
    "3_22": {
        "content": "In\n\nOut\n\nD E B C\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 3",
        "page": "22"
    },
    "3_23": {
        "content": "In\n\nOut\n\nH I D E\n\nC\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nINF2D: REASONING AND AGENTS\n\n24\n\n",
        "lecture": " 3",
        "page": "23"
    },
    "3_24": {
        "content": "In\n\nOut\n\nH I\n\nE\n\nC\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nINF2D: REASONING AND AGENTS\n\n25\n\n",
        "lecture": " 3",
        "page": "24"
    },
    "3_25": {
        "content": "In\n\nOut\n\nI\n\nE\n\nC\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nINF2D: REASONING AND AGENTS\n\n26\n\n",
        "lecture": " 3",
        "page": "25"
    },
    "3_26": {
        "content": "In\n\nOut\n\nJ\n\nK E\n\nC\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nINF2D: REASONING AND AGENTS\n\n27\n\n",
        "lecture": " 3",
        "page": "26"
    },
    "3_27": {
        "content": "In\n\nOut\n\nJ\n\nK\n\nC\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 3",
        "page": "27"
    },
    "3_28": {
        "content": "In\n\nOut\n\nK\n\nC\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 3",
        "page": "28"
    },
    "3_29": {
        "content": "In\n\nOut\n\nF G C\n\nDepth-first \nsearch\n\nExpand deepest\nunexpanded node\n\nImplementation:\n\n\u25e6 frontier is a LIFO queue, \ni.e., new successors go at \nfront\n\nINF2D: REASONING AND AGENTS\n\n30\n\n",
        "lecture": " 3",
        "page": "29"
    },
    "3_30": {
        "content": "Properties of depth-first search\n\nComplete?\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n31\n\n",
        "lecture": " 3",
        "page": "30"
    },
    "3_31": {
        "content": "Properties of depth-first search\n\nComplete?\nNo: fails in infinite-depth spaces, spaces with loops\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n32\n\n",
        "lecture": " 3",
        "page": "31"
    },
    "3_32": {
        "content": "Properties of depth-first search\n\nComplete?\nNo: fails in infinite-depth spaces, spaces with loops\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\navoid repeated \n\nstates along path; \ncomplete in finite \n\nspaces \n\nINF2D: REASONING AND AGENTS\n\n33\n\n",
        "lecture": " 3",
        "page": "32"
    },
    "3_33": {
        "content": "Properties of depth-first search\n\nComplete?\nNo: fails in infinite-depth spaces, spaces with loops\n\nTime complexity?\nO(bm): terrible if m is much larger than d\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n34\n\n",
        "lecture": " 3",
        "page": "33"
    },
    "3_34": {
        "content": "Properties of depth-first search\n\nComplete?\nNo: fails in infinite-depth spaces, spaces with loops\n\nTime complexity?\nO(bm): terrible if m is much larger than d\n\nSpace complexity?\n\nOptimal?\n\nIf solutions are \n\ndense, depth-first \nmay be much faster \nthan breadth-first!\n\nINF2D: REASONING AND AGENTS\n\n35\n\n",
        "lecture": " 3",
        "page": "34"
    },
    "3_35": {
        "content": "Properties of depth-first search\n\nComplete?\nNo: fails in infinite-depth spaces, spaces with loops\n\nTime complexity?\nO(bm): terrible if m is much larger than d\n\nSpace complexity?\nO(bm), i.e., linear space!\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n36\n\n",
        "lecture": " 3",
        "page": "35"
    },
    "3_36": {
        "content": "Properties of depth-first search\n\nComplete?\nNo: fails in infinite-depth spaces, spaces with loops\n\nTime complexity?\nO(bm): terrible if m is much larger than d\n\nSpace complexity?\nO(bm), i.e., linear space!\n\nOptimal?\nNo\n\nINF2D: REASONING AND AGENTS\n\n37\n\n",
        "lecture": " 3",
        "page": "36"
    },
    "3_37": {
        "content": "Mid-Lecture Exercise\n\nBREADTH-FIRST\n\nDEPTH-FIRST\n\nINF2D: REASONING AND AGENTS\n\n38\n\n",
        "lecture": " 3",
        "page": "37"
    },
    "3_38": {
        "content": "Mid-Lecture Exercise\n\nBREADTH-FIRST\n\nDEPTH-FIRST\n\n\u25e6 When completeness is important.\n\n\u25e6 When optimal solutions are important. \n\n\u25e6 When solutions are dense and low-cost \n\nis important, especially space costs. \n\nINF2D: REASONING AND AGENTS\n\n39\n\n",
        "lecture": " 3",
        "page": "38"
    },
    "3_39": {
        "content": "Iterative deepening \nsearch\n\n\u2026 or how to improve depth -first search\n\nINF2D: REASONING AND AGENTS\n\n40\n\n",
        "lecture": " 3",
        "page": "39"
    },
    "3_40": {
        "content": "Depth-limited \nsearch\n\nThis is depth-first search with \ndepth limit l, i.e., nodes at \ndepth l have no successors\n\nINF2D: REASONING AND AGENTS\n\n41\n\n",
        "lecture": " 3",
        "page": "40"
    },
    "3_41": {
        "content": "Iterative deepening search\n\nINF2D: REASONING AND AGENTS\n\n42\n\n",
        "lecture": " 3",
        "page": "41"
    },
    "3_42": {
        "content": "Iterative deepening search l =0\n\nINF2D: REASONING AND AGENTS\n\n43\n\n",
        "lecture": " 3",
        "page": "42"
    },
    "3_43": {
        "content": "Iterative deepening search l =1\n\nINF2D: REASONING AND AGENTS\n\n44\n\n",
        "lecture": " 3",
        "page": "43"
    },
    "3_44": {
        "content": "Iterative deepening search l =2\n\nINF2D: REASONING AND AGENTS\n\n45\n\n",
        "lecture": " 3",
        "page": "44"
    },
    "3_45": {
        "content": "Iterative deepening search l =3\n\nINF2D: REASONING AND AGENTS\n\n46\n\n",
        "lecture": " 3",
        "page": "45"
    },
    "3_46": {
        "content": "Iterative deepening search\n\nNumber of nodes generated in an iterative deepening search to \ndepth d with branching factor b: \n\nNIDS = (d)b + (d-1) b2 + \u2026 + (2)bd-1 + (1)bd\n\nSome cost associated with generating upper levels multiple times \n\nExample: For b = 10, d = 5,\n\u25e6 NBFS = 10 + 100 + 1,000 + 10,000 + 100,000 = 111,110\n\u25e6 NIDS = 50 + 400 + 3,000 + 20,000 + 100,000 = 123,450\n\nOverhead = (123,450 - 111,110)/111,110 = 11%  \n\nINF2D: REASONING AND AGENTS\n\n47\n\n",
        "lecture": " 3",
        "page": "46"
    },
    "3_47": {
        "content": "Properties of iterative deepening search\n\nComplete?\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n48\n\n",
        "lecture": " 3",
        "page": "47"
    },
    "3_48": {
        "content": "Properties of iterative deepening search\n\nComplete?\nYes\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n49\n\n",
        "lecture": " 3",
        "page": "48"
    },
    "3_49": {
        "content": "Properties of iterative deepening search\n\nComplete?\nYes\n\nTime complexity?\n(d)b + (d-1)b2 + \u2026 + (1)bd = O(bd)\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n50\n\n",
        "lecture": " 3",
        "page": "49"
    },
    "3_50": {
        "content": "Properties of iterative deepening search\n\nComplete?\nYes\n\nTime complexity?\n(d)b + (d-1)b2 + \u2026 + (1)bd = O(bd)\n\nSpace complexity?\nO(bd)\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n51\n\n",
        "lecture": " 3",
        "page": "50"
    },
    "3_51": {
        "content": "Properties of iterative deepening search\n\nComplete?\nYes\n\nTime complexity?\n(d)b + (d-1)b2 + \u2026 + (1)bd = O(bd)\n\nSpace complexity?\nO(bd)\n\nOptimal?\nYes, if step cost = 1\n\nINF2D: REASONING AND AGENTS\n\n52\n\n",
        "lecture": " 3",
        "page": "51"
    },
    "3_52": {
        "content": "Summary of algorithms\n\nINF2D: REASONING AND AGENTS\n\n53\n\n",
        "lecture": " 3",
        "page": "52"
    },
    "3_53": {
        "content": "Summary\n\nVariety of uninformed search strategies:\n\u25e6 breadth-first, depth-first, iterative deepening\n\nIterative deepening search uses only linear \nspace and not much more time than other \nuninformed algorithms\n\nINF2D: REASONING AND AGENTS\n\n54\n\n",
        "lecture": " 3",
        "page": "53"
    },
    "3_54": {
        "content": "Why?\n\n\u25e6 Very common algorithms.\n\n\u25e6 Used whenever we are looking for a path between 2 points in a tree or graph.\n\n\u25e6 Anywhere from games to programming languages.\n\n\u25e6 Properties matter! \n\n\u25e6 e.g. time or space complexity may be important depending on application.\n\n\u25e6 Understanding which algorithm to use in what circumstances.\n\nINF2D: REASONING AND AGENTS\n\n55\n\n",
        "lecture": " 3",
        "page": "54"
    },
    "4_1": {
        "content": "Review: Tree search\n\nfunction TREE-SEARCH(problem) returns a solution, or failure\n\ninitialize the frontier using the initial state of problem\n\nloop do\n\nif the frontier is empty then return failure\n\nchoose a leaf node and remove it from the frontier\n\nif the node contains a goal state then return the corresponding solution\n\nexpand the chosen node, adding the resulting nodes to the frontier\n\nA search strategy is defined by picking the order of node expansion from \nthe frontier.\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 4",
        "page": "1"
    },
    "4_2": {
        "content": "Review: Tree search\n\nfunction TREE-SEARCH(problem) returns a solution, or failure\n\ninitialize the frontier using the initial state of problem\n\nloop do\n\nif the frontier is empty then return failure\n\nchoose a leaf node and remove it from the frontier\n\nWhat if we order the \nnodes in the frontier \n\nby decreasing \n\ndesirability?\n\nif the node contains a goal state then return the corresponding solution\n\nexpand the chosen node, adding the resulting nodes to the frontier\n\nA search strategy is defined by picking the order of node expansion from \nthe frontier.\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 4",
        "page": "2"
    },
    "4_3": {
        "content": "Best-first search\n\nAn instance of general TREE-SEARCH or GRAPH-SEARCH\n\n\u2192 Use an evaluation function f(n) for each node n\n\n\u25e6 estimate of \"desirability\"\n\n\u2192 Expand most desirable unexpanded node, usually the node with the \nlowest evaluation\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 4",
        "page": "3"
    },
    "4_4": {
        "content": "Heuristics\n\nFrom the Greek \u00ab\u03b5\u03c5\u03c1\u03af\u03c3\u03ba\u03c9\u00bb (/e\u02c8v\u027ei.sko/), meaning to find, to discover.\n\nAny method that is believed or practically proven to be useful for the solution of a \ngiven problem.\n\u25e6 No guarantee that it will always work or lead to an optimal solution!\n\nWe use heuristics to guide tree search. \n\u25e6 This may not change the worst case complexity of the algorithm, but can help in the average \n\ncase.\n\nWe introduce conditions (admissibility, consistency) in order to identify good \nheuristics, i.e. those which actually lead to an improvement over uninformed search.\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 4",
        "page": "4"
    },
    "4_5": {
        "content": "Romania\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 4",
        "page": "5"
    },
    "4_6": {
        "content": "Greedy best-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 4",
        "page": "6"
    },
    "4_7": {
        "content": "Greedy best-first search\n\nEvaluation function f(n) = h(n) (heuristic)\n\n\u25e6 estimated cost of cheapest path from state at node n to a goal state\n\n\u25e6 e.g., hSLD(n) = straight-line distance from n to Bucharest\n\nGreedy best-first search expands the node that appears to be closest to \ngoal\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 4",
        "page": "7"
    },
    "4_8": {
        "content": "Arad\n\nBest-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 4",
        "page": "8"
    },
    "4_9": {
        "content": "Sibiu\n\nTimiso-\n\nara\n\nZerind\n\nBest-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 4",
        "page": "9"
    },
    "4_10": {
        "content": "Sibiu\n\nTimiso-\n\nara\n\nZerind\n\nBest-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 4",
        "page": "10"
    },
    "4_11": {
        "content": "Timiso-\n\nara\n\nZerind\n\nArad\n\nFagaras Oradea\n\nRV\n\nBest-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 4",
        "page": "11"
    },
    "4_12": {
        "content": "Fagaras\n\nRV\n\nTimiso-\n\nara\n\nArad\n\nZerind\n\nOradea\n\nBest-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 4",
        "page": "12"
    },
    "4_13": {
        "content": "Fagaras\n\nRV\n\nTimiso-\n\nara\n\nArad\n\nZerind Oradea\n\nBest-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 4",
        "page": "13"
    },
    "4_14": {
        "content": "Fagaras\n\nRV\n\nTimiso-\n\nara\n\nArad\n\nZerind Oradea\n\nSibiu\n\nBucha-\n\nrest\n\nBest-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 4",
        "page": "14"
    },
    "4_15": {
        "content": "Properties of best-first search\n\nComplete?\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 4",
        "page": "15"
    },
    "4_16": {
        "content": "Properties of best-first search\n\nComplete?\nNo! Can get stuck in loops.\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 4",
        "page": "16"
    },
    "4_17": {
        "content": "Properties of best-first search\n\nComplete?\nNo! Can get stuck in loops.\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nGRAPH-SEARCH \n\nis complete in \nfinite spaces.\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 4",
        "page": "17"
    },
    "4_18": {
        "content": "Properties of best-first search\n\nComplete?\nNo! Can get stuck in loops.\n\nTime complexity?\nO(bm) for tree version\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 4",
        "page": "18"
    },
    "4_19": {
        "content": "Properties of best-first search\n\nComplete?\nNo! Can get stuck in loops.\n\nTime complexity?\nO(bm) for tree version\n\nSpace complexity?\n\nOptimal?\n\nA good heuristic \n\ncan lead to \n\ndramatic \n\nimprovement!\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 4",
        "page": "19"
    },
    "4_20": {
        "content": "Properties of best-first search\n\nComplete?\nNo! Can get stuck in loops.\n\nTime complexity?\nO(bm) for tree version\n\nSpace complexity?\nO(bm) \u2013 keeps all nodes in memory\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 4",
        "page": "20"
    },
    "4_21": {
        "content": "Properties of best-first search\n\nComplete?\nNo! Can get stuck in loops.\n\nTime complexity?\nO(bm) for tree version\n\nSpace complexity?\nO(bm) \u2013 keeps all nodes in memory\n\nOptimal?\nNo\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 4",
        "page": "21"
    },
    "4_22": {
        "content": "A* Search\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 4",
        "page": "22"
    },
    "4_23": {
        "content": "A* search\n\nEvaluation function f(n) = g(n) + h(n)\n\n\u25e6 g(n) = cost so far to reach n\n\n\u25e6 h(n) = estimated cost from n to goal\n\n\u25e6 f(n) = estimated total cost of path through n to goal\n\nAvoid expanding paths that are already expensive\n\nINF2D: REASONING AND AGENTS\n\n24\n\n",
        "lecture": " 4",
        "page": "23"
    },
    "4_24": {
        "content": "Best-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n25\n\n",
        "lecture": " 4",
        "page": "24"
    },
    "4_25": {
        "content": "Best-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n26\n\n",
        "lecture": " 4",
        "page": "25"
    },
    "4_26": {
        "content": "Best-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n27\n\n",
        "lecture": " 4",
        "page": "26"
    },
    "4_27": {
        "content": "Best-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 4",
        "page": "27"
    },
    "4_28": {
        "content": "Best-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 4",
        "page": "28"
    },
    "4_29": {
        "content": "Best-first \nsearch\n\nINF2D: REASONING AND AGENTS\n\n30\n\n",
        "lecture": " 4",
        "page": "29"
    },
    "4_30": {
        "content": "Heuristics\n\nINF2D: REASONING AND AGENTS\n\n31\n\n",
        "lecture": " 4",
        "page": "30"
    },
    "4_31": {
        "content": "Admissible heuristics\n\nA heuristic h(n) is admissible if for every node n:\n\nh(n) \u2264 h*(n)\n\nwhere h*(n) is the true cost to reach the goal state from n.\n\nAn admissible heuristic never overestimates the cost to reach the \ngoal, i.e., it is optimistic\n\nExample: hSLD(n) (never overestimates the actual road distance)\n\nINF2D: REASONING AND AGENTS\n\n32\n\n",
        "lecture": " 4",
        "page": "31"
    },
    "4_32": {
        "content": "Admissible heuristic = optimal A*\n\nh(n) never overestimates the cost to reach the goal\n\nThus, f(n) = g(n) + h(n) never overestimates the true cost of a \nsolution\n\nTHEOREM\n\nIf h(n) is admissible, A* using TREE-SEARCH is optimal\n\nINF2D: REASONING AND AGENTS\n\n33\n\n",
        "lecture": " 4",
        "page": "32"
    },
    "4_33": {
        "content": "Proof: Optimality of A*\n\nSuppose some suboptimal goal G2 has been \ngenerated and is in the frontier. \n\nLet n be an unexpanded node in the frontier such that \nn is on a shortest path to an optimal goal G.\n\nf(G2)  = g(G2)\n\ng(G2) > g(G) \n\nsince h(G2) = 0 \n\nsince G2 is suboptimal \n\nf(G)   = g(G)\n\nsince h(G) = 0 \n\nf(G2)  > f(G)\n\nfrom above \n\nINF2D: REASONING AND AGENTS\n\n34\n\n",
        "lecture": " 4",
        "page": "33"
    },
    "4_34": {
        "content": "Proof: Optimality of A*\n\nSuppose some suboptimal goal G2 has been \ngenerated and is in the frontier. \n\nLet n be an unexpanded node in the frontier such that \nn is on a shortest path to an optimal goal G.\n\nf(G)\n\n< f(G2) \n\nfrom above \n\nh(n)\n\n\u2264 h*(n)\n\nsince h is admissible\n\ng(n) + h(n) \u2264 g(n) + h*(n)\n\nf(n) \u2264 f(G)\n\nHence f(n) < f(G2), and A* will never select G2 for expansion\n\nINF2D: REASONING AND AGENTS\n\n35\n\n",
        "lecture": " 4",
        "page": "34"
    },
    "4_35": {
        "content": "Consistent heuristics\n\nA heuristic h(n) is consistent if for every node n, every successor n' of n generated by any \naction a, \n\nh(n) \u2264 c(n, a, n') + h(n')\n\nIf h is consistent, we have\n\nf(n') = g(n') + h(n') \n\n= g(n) + c(n, a, n') + h(n') \n\n\u2265 g(n) + h(n) \n\n\u2265 f(n)\n\ni.e., f(n) is non-decreasing along any path.\n\nTHEOREM\n\nIf h(n) is consistent, A* using GRAPH-SEARCH is optimal\n\nINF2D: REASONING AND AGENTS\n\n36\n\n",
        "lecture": " 4",
        "page": "35"
    },
    "4_36": {
        "content": "Optimality of \nA*\n\nA* expands nodes in order \nof increasing f value\n\nGradually adds \"f-\ncontours\" of nodes \n\nContour i has all nodes \nwith f=fi, where fi < fi+1\n\nINF2D: REASONING AND AGENTS\n\n37\n\n",
        "lecture": " 4",
        "page": "36"
    },
    "4_37": {
        "content": "Properties of A*\n\nComplete?\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n38\n\n",
        "lecture": " 4",
        "page": "37"
    },
    "4_38": {
        "content": "Properties of A*\n\nComplete?\nYes (unless there are infinitely many nodes with f \u2264 f(G)\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n39\n\n",
        "lecture": " 4",
        "page": "38"
    },
    "4_39": {
        "content": "Properties of A*\n\nComplete?\nYes (unless there are infinitely many nodes with f \u2264 f(G)\n\nTime complexity?\nExponential\n\nSpace complexity?\nKeeps all nodes in memory\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n40\n\n",
        "lecture": " 4",
        "page": "39"
    },
    "4_40": {
        "content": "Properties of A*\n\nComplete?\nYes (unless there are infinitely many nodes with f \u2264 f(G)\n\nTime complexity?\nExponential\n\nSpace complexity?\nKeeps all nodes in memory\n\nOptimal?\nYes\n\nINF2D: REASONING AND AGENTS\n\n41\n\n",
        "lecture": " 4",
        "page": "40"
    },
    "4_41": {
        "content": "Admissible heuristics\n\nExample: 8-puzzle:\n\u25e6 h1(n) = number of misplaced tiles\n\u25e6 h2(n) = total Manhattan distance\n\n(i.e., no. of squares from desired location of each tile)\n\nh1(S) = ? \n\nh2(S) = ?\n\nINF2D: REASONING AND AGENTS\n\n42\n\n",
        "lecture": " 4",
        "page": "41"
    },
    "4_42": {
        "content": "Dominance\n\nIf h2(n) \u2265 h1(n) for all n (both admissible) then\n\u25e6 h2 dominates h1 \n\u25e6 h2 is better for search\n\nTypical search costs (average number of nodes expanded):\n\u25e6 d=12\n\nIDS = 3,644,035 nodes\nA*(h1) = 227 nodes \nA*(h2) = 73 nodes \n\n\u25e6 d=24 \n\nIDS \u2248 54,000,000,000 nodes\nA*(h1) = 39,135 nodes \nA*(h2) = 1,641 nodes \n\nINF2D: REASONING AND AGENTS\n\n43\n\n",
        "lecture": " 4",
        "page": "42"
    },
    "4_43": {
        "content": "Relaxed problems\n\nA problem with fewer restrictions on the actions is called a relaxed problem.\n\nThe cost of an optimal solution to a relaxed problem is an admissible heuristic\nfor the original problem.\n\nIf the rules of the 8-puzzle are relaxed so that a tile can move anywhere, \n\u25e6 then h1(n) gives the shortest solution\n\nIf the rules are relaxed so that a tile can move to any adjacent square, \n\u25e6 then h2(n) gives the shortest solution\n\nUse relaxation to automatically generate admissible heuristics!\n\nINF2D: REASONING AND AGENTS\n\n44\n\n",
        "lecture": " 4",
        "page": "43"
    },
    "4_44": {
        "content": "Summary\n\nSmart search based on heuristic scores.\n\n\u25e6 Best-first search\n\n\u25e6 Greedy best-first search\n\n\u25e6 A* search\n\n\u25e6 Admissible heuristics and optimality.\n\nINF2D: REASONING AND AGENTS\n\n45\n\n",
        "lecture": " 4",
        "page": "44"
    },
    "4_45": {
        "content": "Why?\n\nInformed search allows us to use domain knowledge to our advantage.\n\nOptimality over some utility can often be the top priority.\n\nA* is very popular! \n\u25e6 e.g. pathfinding\n\nA* is simple, yet very efficient.\n\nA* is too good sometimes (e.g. in games).\n\nINF2D: REASONING AND AGENTS\n\n46\n\n",
        "lecture": " 4",
        "page": "45"
    },
    "5_1": {
        "content": "Constraint satisfaction problems (CSPs)\n\nState\n\n\u2022 Set of variables Xi with values from domain Di\n\nActions\n\n\u2022 Assign a value to a variable\n\nGoal test\n\n\u2022 A set of constraints specifying allowable combinations of values for subsets of variables\n\nPath cost\n\n\u2022 None \n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 5",
        "page": "1"
    },
    "5_2": {
        "content": "Constraint satisfaction problems (CSPs)\n\nState\n\n\u2022 Set of variables Xi with values from domain Di\n\nActions\n\n\u2022 Assign a value to a variable\n\nGoal test\n\n\u2022 A set of constraints specifying allowable combinations of values for subsets of variables\n\nPath cost\n\nSimple example of a formal representation language.\n\n\u2022 None \n\nAllows useful general-purpose algorithms with more power than standard \n\nsearch algorithms.\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 5",
        "page": "2"
    },
    "5_3": {
        "content": "Structure of a CSP\n\n\u27a2 A set of variables: \n\n\u27a2 A set of domains:\n\nX={X1,\u2026 Xn}\n\nD={D1,\u2026 Dn}\n\n\u2022 each domain Di is a set of possible values for variable Xi\n\n\u27a2 A set of constraints C that specify acceptable combinations of values.\n\n\u2022 Each c \u2208 C consists of:\n\n\u27a2 a scope \u2013 tuple of variables (neighbours) involved in the constraint\n\n\u27a2 a relation that defines the values that the variables can take\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 5",
        "page": "3"
    },
    "5_4": {
        "content": "Example: Map-Colouring\n\nVariables: {WA, NT, Q, NSW, V, SA, T}\n\nDomains: Di = {red, green, blue}\n\nConstraints: adjacent regions must have different \ncolours,\n\u25e6 e.g. WA \u2260 NT, \n\n\u25e6 or (WA,NT) \u2208 {(red, green), (red, blue), (green, red), \n\n(green, blue), (blue, red), (blue, green)}.\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 5",
        "page": "4"
    },
    "5_5": {
        "content": "Example: Map-Colouring\n\nSolutions are complete and consistent assignments, \n\u25e6 e.g. WA = red, NT = green, Q = red, \n\nNSW = green, V = red, SA = blue, T = green.\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 5",
        "page": "5"
    },
    "5_6": {
        "content": "Constraint graph\n\nBinary CSP: each constraint relates two \nvariables.\n\nConstraint graph: nodes are variables, arcs are \nconstraints.\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 5",
        "page": "6"
    },
    "5_7": {
        "content": "Constraint graph\n\nBinary CSP: each constraint relates two \nvariables.\n\nConstraint graph: nodes are variables, arcs are \nconstraints.\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 5",
        "page": "7"
    },
    "5_8": {
        "content": "Varieties \nof CSPs\n\nDiscrete variables:\n\u25e6 finite domains:\n\n\u25e6 n variables, domain size d \u2192O(dn), complete assignments.\n\n\u25e6 e.g. Boolean CSPs, incl. Boolean satisfiability (NP-complete).\n\n\u25e6 infinite domains:\n\n\u25e6 integers, strings, etc.\n\n\u25e6 e.g. job scheduling, variables are start/end days for each job.\n\n\u25e6 need a constraint language, e.g. StartJob1+ 5 \u2264 StartJob3.\n\nContinuous variables:\n\u25e6 e.g. start/end times for Hubble Space Telescope \n\nobservations.\n\n\u25e6 linear constraints solvable in polynomial time by linear \n\nprogramming.\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 5",
        "page": "8"
    },
    "5_9": {
        "content": "Varieties of constraints\n\nUnary constraints involve a single variable, \n\u25e6 e.g. SA \u2260 green.\n\nBinary constraints involve pairs of variables,\n\u25e6 e.g. SA \u2260 WA.\n\nHigher-order constraints involve 3 or more variables,\n\u25e6 e.g. crypt-arithmetic column constraints.\n\nGlobal constraints involve an arbitrary number of variables\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 5",
        "page": "9"
    },
    "5_10": {
        "content": "Example: Crypt-arithmetic\n\nVariables: F T U W R O X1 X2 X3.\n\nconstraint\nhypergraph\n\nDomains: {0,1,2,3,4,5,6,7,8,9}.\n\nHypernode\n\n(n-ary constraint)\n\nConstraints: \n\u25e6 Alldiff (F,T,U,W,R,O)\n\n\u25e6 O + O = R + 10 \u00b7 X1\n\u25e6 X1 + W + W = U + 10 \u00b7 X2\n\u25e6 X2 + T + T = O + 10 \u00b7 X3\n\u25e6 X3 = F, T \u2260 0, F \u2260 0\n\nGlobal constraint\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 5",
        "page": "10"
    },
    "5_11": {
        "content": "Real-world CSPs\n\nAssignment problems\n\nTimetabling problems\n\nTransportation \n\nscheduling\n\nFactory scheduling\n\ne.g. who teaches what class.\n\ne.g. which class is offered when \n\nand where.\n\nNotice that many real-world problems involve real-valued variables.\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 5",
        "page": "11"
    },
    "5_12": {
        "content": "Search in CSPs\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 5",
        "page": "12"
    },
    "5_13": {
        "content": "Standard search formulation \n(incremental)\n\nLet's start with the straightforward approach, then adapt it.\n\nStates are defined by the values assigned so far.\n\nInitial state: the empty assignment { }.\n\nSuccessor function: assign a value to an unassigned variable that does not conflict \nwith current assignment\n\u2192 fail if no legal assignments.\n\nGoal test: the current assignment is complete.\n\n\u27a2 For a CSP with n variables, every solution appears at depth n\n\n\u2192 use depth-first search!\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 5",
        "page": "13"
    },
    "5_14": {
        "content": "Backtracking search\n\nVariable assignments are commutative, \n\u25e6 e.g. [ WA = red then NT = green ] same as [ NT = green then WA = red ].\n\nOnly need to consider assignments to a single variable at each node\n\nb = d and there are dn leaves\n\nDepth-first search for CSPs with single-variable assignments is called backtracking\nsearch.\n\nBacktracking search is the basic uninformed algorithm for CSPs.\n\nCan solve n-queens for n \u2248 25.\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 5",
        "page": "14"
    },
    "5_15": {
        "content": "Backtracking \nsearch\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 5",
        "page": "15"
    },
    "5_16": {
        "content": "Backtracking \nexample\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 5",
        "page": "16"
    },
    "5_17": {
        "content": "Backtracking \nexample\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 5",
        "page": "17"
    },
    "5_18": {
        "content": "Backtracking \nexample\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 5",
        "page": "18"
    },
    "5_19": {
        "content": "Backtracking \nexample\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 5",
        "page": "19"
    },
    "5_20": {
        "content": "Can we eliminate some \n\nsymmetrical nodes?\n\nBacktracking \nexample\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 5",
        "page": "20"
    },
    "5_21": {
        "content": "Can we eliminate some \n\nsymmetrical nodes?\n\nBacktracking \nexample\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 5",
        "page": "21"
    },
    "5_22": {
        "content": "Smart Search in CSPs\n\n\u2026 or how to improve from backtracking\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 5",
        "page": "22"
    },
    "5_23": {
        "content": "Improving backtracking \nefficiency\n\nGeneral-purpose methods can give huge gains in \nspeed:\n\n\u25e6 Which variable should be assigned next?\n\n\u25e6 SELECT-UNASSIGNED-VARIABLE\n\n\u25e6 Then, in what order should its values be tried?\n\n\u25e6 ORDER-DOMAIN-VALUES\n\n\u25e6 What inferences should be performed at each \n\nstep of the search?\n\n\u25e6 INFERENCE\n\n\u25e6 Can we detect inevitable failure early?\n\nINF2D: REASONING AND AGENTS\n\n24\n\n",
        "lecture": " 5",
        "page": "23"
    },
    "5_24": {
        "content": "Most constrained variable\n\nvar \uf0ac SELECT-UNASSIGNED-VARIABLE(csp)\n\nMost constrained variable:\n\u25e6 choose the variable with the fewest legal values.\n\na.k.a. minimum-remaining-values (MRV) heuristic.\n\nINF2D: REASONING AND AGENTS\n\n25\n\n",
        "lecture": " 5",
        "page": "24"
    },
    "5_25": {
        "content": "Most constraining variable\n\nTie-breaker among most constrained variables.\n\nMost constraining variable:\n\u25e6 choose the variable with the most constraints on remaining variables \u2013 thus reducing \n\nbranching.\n\na.k.a. degree heuristic\n\nINF2D: REASONING AND AGENTS\n\n26\n\n",
        "lecture": " 5",
        "page": "25"
    },
    "5_26": {
        "content": "Least constraining value\n\nfor value in ORDER-DOMAIN-VALUES(var, assignment, csp)\n\nLeast constraining value:\n\u25e6 given a variable, choose the value that rules out the fewest values in the remaining variables.\n\nCombining these heuristics makes 1000 queens feasible!\n\nINF2D: REASONING AND AGENTS\n\n1 value left \n\nfor SA\n\n0 values left \n\nfor SA\n\n27\n\n",
        "lecture": " 5",
        "page": "26"
    },
    "5_27": {
        "content": "Inference: Forward checking\nIdea:\n\u25e6 Keep track of remaining legal values for unassigned variables.\n\n\u25e6 Terminate search when any variable has no legal values.\n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nT\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 5",
        "page": "27"
    },
    "5_28": {
        "content": "Inference: Forward checking\nIdea:\n\u25e6 Keep track of remaining legal values for unassigned variables.\n\n\u25e6 Terminate search when any variable has no legal values.\n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nT\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 5",
        "page": "28"
    },
    "5_29": {
        "content": "Inference: Forward checking\nIdea:\n\u25e6 Keep track of remaining legal values for unassigned variables.\n\n\u25e6 Terminate search when any variable has no legal values.\n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nT\n\nINF2D: REASONING AND AGENTS\n\n30\n\n",
        "lecture": " 5",
        "page": "29"
    },
    "5_30": {
        "content": "Inference: Forward checking\nIdea:\n\u25e6 Keep track of remaining legal values for unassigned variables.\n\n\u25e6 Terminate search when any variable has no legal values.\n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nT\n\nINF2D: REASONING AND AGENTS\n\n31\n\n",
        "lecture": " 5",
        "page": "30"
    },
    "5_31": {
        "content": "Constraint propagation\nForward checking propagates information from assigned to unassigned \nvariables, but doesn't provide early detection for all failures:\n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nT\n\nNT and SA cannot both be blue!\n\nConstraint propagation repeatedly enforces constraints locally.\n\nINF2D: REASONING AND AGENTS\n\n32\n\n",
        "lecture": " 5",
        "page": "31"
    },
    "5_32": {
        "content": "Arc consistency\n\nSimplest form of propagation makes each arc consistent.\n\nX \u2192Y is consistent iff for every value x of in the domain of X \nthere is some allowed y in the domain of Y.\n\nIs there a value for X that makes the domain of Y empty?\n\nCan be run as a preprocessor or after each assignment.\n\nStart with all directed arcs from the graph (18 here):\n\nWA\u2192NT, WA\u2192SA, NT\u2192WA, NT\u2192SA, NT\u2192Q, Q\u2192NT, Q\u2192SA, \nQ\u2192NSW, SA\u2192WA, SA\u2192NT, SA\u2192Q, SA\u2192NSW, SA\u2192V, \nNSW\u2192Q, NSW\u2192SA, NSW\u2192V, V\u2192SA, V\u2192NSW\n\nINF2D: REASONING AND AGENTS\n\n33\n\n",
        "lecture": " 5",
        "page": "32"
    },
    "5_33": {
        "content": "Arc consistency\n\nX\u2192Y : Is there a value for X that makes the domain of Y empty?\n\ne.g. NSW \u2192 SA\n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nT\n\nINF2D: REASONING AND AGENTS\n\n34\n\n",
        "lecture": " 5",
        "page": "33"
    },
    "5_34": {
        "content": "Arc consistency\n\nX\u2192Y : Is there a value for X that makes the domain of Y empty?\n\ne.g. NSW \u2192 SA\n\nOnce a value is removed, add all arcs pointing to X back in the queue!\n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nDomain of NSW \n\nT\n\nbecame smaller, so \nsome arcs may have \nbecome inconsistent!\n\nINF2D: REASONING AND AGENTS\n\n35\n\n",
        "lecture": " 5",
        "page": "34"
    },
    "5_35": {
        "content": "Arc consistency\n\nX\u2192Y : Is there a value for X that makes the domain of Y empty?\n\ne.g. NSW \u2192 SA\n\nOnce a value is removed, add all arcs pointing to X back in the queue!\n\nAdd: \n\nV\u2192NSW \nSA\u2192NSW \nQ\u2192NSW \n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nT\n\nINF2D: REASONING AND AGENTS\n\n36\n\n",
        "lecture": " 5",
        "page": "35"
    },
    "5_36": {
        "content": "Arc consistency\n\nX\u2192Y : Is there a value for X that makes the domain of Y empty?\n\nEventually check SA\u2192NT\n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nT\n\nINF2D: REASONING AND AGENTS\n\n37\n\n",
        "lecture": " 5",
        "page": "36"
    },
    "5_37": {
        "content": "Arc consistency\n\nX\u2192Y : Is there a value for X that makes the domain of Y empty?\n\nEventually check SA\u2192NT\n\nWA\n\nNT\n\nQ\n\nNSW\n\nV\n\nSA\n\nFail!\n\nT\n\nArc consistency detects failure earlier than forward checking.\n\nINF2D: REASONING AND AGENTS\n\n38\n\n",
        "lecture": " 5",
        "page": "37"
    },
    "5_38": {
        "content": "Arc consistency \nalgorithm AC-3\n\nMake Xi arc-consistent with respect to Xj\nNo consistent value left for Xi so fail\n\nSince revision occurred, add all \nneighbours of Xi for consideration \n(or reconsideration)\n\nTime complexity: O(cd3), \nwhere d is maximum size of \neach domain and c is the \nnumber of binary constraints \n(arcs).\n\nINF2D: REASONING AND AGENTS\n\n39\n\n",
        "lecture": " 5",
        "page": "38"
    },
    "5_39": {
        "content": "Summary\n\nCSPs are a special kind of problem:\n\u25e6 states defined by values of a fixed set of variables\n\u25e6 goal test defined by constraints on variable values\n\nBacktracking = depth-first search with one variable assigned per node\n\nVariable ordering and value selection heuristics help significantly\n\nForward checking prevents assignments that guarantee later failure\n\nConstraint propagation (e.g. arc consistency) does additional work to constrain values \nand detect inconsistencies\n\nINF2D: REASONING AND AGENTS\n\n40\n\n",
        "lecture": " 5",
        "page": "39"
    },
    "5_40": {
        "content": "Why?\n\nCSPs are prevalent in modern computation.\n\nExamples mentioned in this lecture.\n\nParticularly: resource allocation, planning & scheduling, automated configuration, \npuzzles/games.\n\nMore complex problem formulations exist: e.g. Distributed Constraint Optimisation \nProblems (DCOPs).\n\nOther solutions exist too: e.g. genetic algorithms, optimization\n\nINF2D: REASONING AND AGENTS\n\n41\n\n",
        "lecture": " 5",
        "page": "40"
    },
    "8_1": {
        "content": "Games vs. search problems\n\n\"Unpredictable\" opponent \u2192 solution is a strategy / policy\n\u25e6 Specify a move for every possible opponent reply\n\nTime limits \u2192 unlikely to find goal, must approximate\n\nDiscrete!\n\nTYPES OF GAMES\n\ndeterministic\n\nchance\n\nperfect information\n\nchess, checkers\n\nbackgammon, \n\nmonopoly\n\nimperfect information\n\nbattleships, stratego\n\nbridge, poker, scrabble\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 8",
        "page": "1"
    },
    "8_2": {
        "content": "Games vs. search problems\n\nWe are interested in zero-sum games of perfect information:\n\n\u25e6 Deterministic, fully observable\n\n\u25e6 Agents act alternately\n\n\u25e6 Utilities at end of game are equal and opposite (adding up to 0)\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 8",
        "page": "2"
    },
    "8_3": {
        "content": "Game tree \n(2-player, \ndeterministic, \nturns)\n\n\u2022 2 players: MAX and MIN\n\n\u2022 MAX moves first\n\n\u2022 Tree built from MAX\u2019s POV\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 8",
        "page": "3"
    },
    "8_4": {
        "content": "Optimal Decisions\n\nNormal search: \n\u25e6 optimal decision is a sequence of actions leading to a goal state \n\n(i.e. a winning terminal state)\n\nAdversarial search: \n\u25e6 MIN has a say in game\n\n\u25e6 MAX needs to find a contingent strategy which specifies:\n\n\u27a2 MAX\u2019s move in initial state then\u2026\n\n\u27a2 MAX\u2019s moves in states resulting from every response by MIN to the move then\u2026\n\n\u27a2 MAX\u2019s moves in states resulting from every response by MIN to all those moves, etc\u2026\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 8",
        "page": "4"
    },
    "8_5": {
        "content": "Minimax value\n\nminimax value of a node = utility for MAX of being in corresponding state:\n\nUTILITY(s)                                             \n\nif TERMINAL-TEST(s)\n\nMINIMAX(s) =\n\nmaxa\uf0ceActions(s) MINIMAX(RESULT(s,a)) \n\nif PLAYER(s) = MAX\n\nmina\uf0ceActions(s) MINIMAX(RESULT(s,a)) \n\nif PLAYER(s) = MIN\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 8",
        "page": "5"
    },
    "8_6": {
        "content": "Minimax\n\nPerfect play for \ndeterministic, perfect-\ninformation games\n\nIdea: choose move to \nposition with highest \nminimax value \n\n= best achievable payoff \nagainst best play\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 8",
        "page": "6"
    },
    "8_7": {
        "content": "Minimax\n\nPerfect play for \ndeterministic, perfect-\ninformation games\n\nIdea: choose move to \nposition with highest \nminimax value \n\n= best achievable payoff \nagainst best play\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 8",
        "page": "7"
    },
    "8_8": {
        "content": "Minimax \nalgorithm\n\nIdea: \n\u27a2 Proceed all the way down   \n\nto the leaves of the tree \n\n\u27a2 then minimax values are \nbacked up through tree \n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 8",
        "page": "8"
    },
    "8_9": {
        "content": "Properties of Minimax\n\nComplete?\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 8",
        "page": "9"
    },
    "8_10": {
        "content": "Properties of Minimax\n\nComplete?\nYes (if tree is finite)\n\nTime complexity?\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 8",
        "page": "10"
    },
    "8_11": {
        "content": "Properties of Minimax\n\nComplete?\nYes (if tree is finite)\n\nTime complexity?\nO(bm)\n\nSpace complexity?\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 8",
        "page": "11"
    },
    "8_12": {
        "content": "Properties of Minimax\n\nComplete?\nYes (if tree is finite)\n\nTime complexity?\nO(bm)\n\nSpace complexity?\nO(bm)\n\nOptimal?\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 8",
        "page": "12"
    },
    "8_13": {
        "content": "Properties of Minimax\n\nComplete?\nYes (if tree is finite)\n\nTime complexity?\nO(bm)\n\nSpace complexity?\nO(bm)\n\nOptimal?\nYes (against an optimal opponent)\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 8",
        "page": "13"
    },
    "8_14": {
        "content": "Complexity\n\nFor chess, b \u2248 35, m \u2248100 (average \u2248 40) for \"reasonable\" games\n\n\u27a2 exact solution completely infeasible!\n\n\u27a2 would like to eliminate (large) parts of game tree\n\n3540=5.791\uf0d71061\n\n35100=2.552 \uf0d710154\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 8",
        "page": "14"
    },
    "8_15": {
        "content": "\u03b1-\u03b2 pruning\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 8",
        "page": "15"
    },
    "8_16": {
        "content": "\u03b1-\u03b2 pruning example\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 8",
        "page": "16"
    },
    "8_17": {
        "content": "\u03b1-\u03b2 pruning example\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 8",
        "page": "17"
    },
    "8_18": {
        "content": "\u03b1-\u03b2 pruning example\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 8",
        "page": "18"
    },
    "8_19": {
        "content": "\u03b1-\u03b2 pruning example\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 8",
        "page": "19"
    },
    "8_20": {
        "content": "\u03b1-\u03b2 pruning example\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 8",
        "page": "20"
    },
    "8_21": {
        "content": "\u03b1-\u03b2 pruning example\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 8",
        "page": "21"
    },
    "8_22": {
        "content": "\u03b1-\u03b2 pruning \nexample\n\nAre minimax value of root and, hence, minimax decision \nindependent of pruned leaves?\n\nLet pruned leaves have values u and v, \n\nMINIMAX(root) = max(min(3,12,8), min(2,u,v), min(14,5,2))\n\n= max(3, min(2,u,v), 2) \n\n= max(3, z, 2) \n\nwhere z \u2264 2\n\n= 3\n\nYes!\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 8",
        "page": "22"
    },
    "8_23": {
        "content": "Why is it called \n\u03b1-\u03b2?\n\n\u03b1 is the value of the best (i.e., highest-value) \nchoice found so far at any choice point \nalong the path for MAX\n\nIf v is worse than \u03b1, MAX will avoid it\n\u2192 prune that branch\n\n\u03b2 is defined symmetrically for MIN\n\nINF2D: REASONING AND AGENTS\n\n24\n\n",
        "lecture": " 8",
        "page": "23"
    },
    "8_24": {
        "content": "The \u03b1-\u03b2\nalgorithm\n\n\u03b1 is value of the best i.e. \nhighest-value choice found \nso far at any choice point \nalong the path for MAX\n\n\u03b2 is value of the best i.e. \nlowest-value choice found \nso far at any choice point \nalong the path for MIN\n\nINF2D: REASONING AND AGENTS\n\n25\n\n",
        "lecture": " 8",
        "page": "24"
    },
    "8_25": {
        "content": "Complexity of \u03b1-\u03b2\n\nPruning does not affect final result (as we saw for example)\n\nGood move ordering improves effectiveness of pruning\n\nWith \u201cperfect ordering\u201d, time complexity = O(bm/2)\n\u27a2 branching factor goes from \ud835\udc4f to  \ud835\udc4f\n\n\u27a2 doubles solvable depth of search compared to minimax\n\nA simple example of the value of reasoning about which \ncomputations are relevant (a form of meta-reasoning)\n\nINF2D: REASONING AND AGENTS\n\n26\n\n",
        "lecture": " 8",
        "page": "25"
    },
    "8_26": {
        "content": "Resource limits\n\nINF2D: REASONING AND AGENTS\n\n27\n\n",
        "lecture": " 8",
        "page": "26"
    },
    "8_27": {
        "content": "Resource limits\n\nSuppose we have 100 secs and can explore 104 nodes/sec\n\u27a2 106 nodes per move\n\u27a2 bm = 106\n\u27a2 For b = 35 \u2192 354 = 1.5\uf0d7106 \u2192 so m \u2248 4\n\n4-ply lookahead is a hopeless chess player!\n\u25e6 4-ply \u2248 human novice\n\u25e6 8-ply \u2248 typical PC, human master\n\u25e6 12-ply \u2248 Deep Blue, Kasparov\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 8",
        "page": "27"
    },
    "8_28": {
        "content": "Standard approach\n\nCutoff test\ne.g., depth limit (perhaps add quiescence search, which tries to search \n\ninteresting positions to a greater depth than quiet ones)\n\nEvaluation function\n= estimated desirability of position\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 8",
        "page": "28"
    },
    "8_29": {
        "content": "Standard \napproach\n\nMinimaxCutoff is identical to \nMinimaxValue except:\n\n1. TERMINAL-TEST is replaced by \n\nCUTOFF\n\n2. UTILITY is replaced by EVAL\n\nINF2D: REASONING AND AGENTS\n\n30\n\n",
        "lecture": " 8",
        "page": "29"
    },
    "8_30": {
        "content": "Evaluation functions\n\nOften a linear weighted sum of features\n\nEVAL(s) = w1 f1(s) + w2 f2(s) + \u2026 + wn fn(s)\n\nwhere each wi is a weight and each fi is a feature of state s\n\nChess example\n\u25e6 queen = 1, king = 2, etc.\n\u25e6 fi = number of pieces of type i on board \n\u25e6 wi = value of the piece of type i\n\nINF2D: REASONING AND AGENTS\n\n31\n\n",
        "lecture": " 8",
        "page": "30"
    },
    "8_31": {
        "content": "Deterministic games \nin practice\n\nINF2D: REASONING AND AGENTS\n\n32\n\n",
        "lecture": " 8",
        "page": "31"
    },
    "8_32": {
        "content": "https://www.ibm.com/ibm/history/ibm100/us/en/icons/ibm700series/impacts/\n\nCheckers\n\nChinook ended 40-year-reign of human world \nchampion Marion Tinsley in 1994. Used a \nprecomputed endgame database defining perfect \nplay for all positions involving 8 or fewer pieces on \nthe board, a total of 444 billion positions.\n\nhttp://jonathanschaeffer.blogspot.com/2012/08/chinook-twenty-years-later.html\n\nINF2D: REASONING AND AGENTS\n\n33\n\n",
        "lecture": " 8",
        "page": "32"
    },
    "8_33": {
        "content": "Othello\n\nHuman champions refuse to compete against computers.\n\nLogistello, written by Michael Buro, defeated the human \nworld champion Takeshi Murakami six games to none in \n1997. \n\nThe best Othello programs are now much stronger than any \nhuman player.\n\nINF2D: REASONING AND AGENTS\n\n34\n\nhttps://skatgame.net/mburo/log.html\n\n",
        "lecture": " 8",
        "page": "33"
    },
    "8_34": {
        "content": "Chess\n\nDeep Blue defeated human world champion Garry \nKasparov in a six-game match in 1997. Deep Blue \nsearches 200 million positions per second, uses \nvery sophisticated evaluation, and undisclosed \nmethods for extending some lines of search up to \n40-ply.\n\nINF2D: REASONING AND AGENTS\n\n35\n\nhttps://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)\n\n",
        "lecture": " 8",
        "page": "34"
    },
    "8_35": {
        "content": "Modern Chess\n\nStockfish\n\nAlphaZero\n\n(successor of AlphaGo Zero)\n\nLeela Zero\n\n\u2022 Uses and advanced \n\nversion of \u03b1-\u03b2 pruning \namong other algorithms.\n\u2022 Recently added a simple \n\nneural network in its \nevaluation. \n\u2022 Improved by 100+ ELO \n\npoints since.\n\n\u2022 Analyses 108 positions per \n\nsecond (half when using \nthe neural network).\n\n\u2022 Based on Monte Carlo tree \n\n\u2022 Released 2017 with ideas \n\nsearch, deep neural \nnetworks and self-play.\n\nfrom AlphaGo Zero\u2019s \npaper.\n\n\u2022 Analyses 80,000 positions \n\n\u2022 Believed to have \n\nper second.\n\n\u2022 Defeated Stockfish with \n\n28W-72D-0L in 2016.\n\nsurpassed AlphaZero.\n\n\u2022 Neck to neck with modern \n\nStockfish, losing narrowly \nto it in the last 3 TCEC \nsuperfinals.\n\nINF2D: REASONING AND AGENTS\n\n36\n\n",
        "lecture": " 8",
        "page": "35"
    },
    "8_36": {
        "content": "Go\n\nHuman champions used to refuse to compete against \ncomputers, because they were are too bad. \n\nIn Go, b > 300, so most programs use pattern knowledge \nbases to suggest plausible moves.\n\nIn 2015 AlphaGo became the first computer program to \nbeat a human professional Go player (Fan Hui) without \nhandicap.\n\nIn 2016 AlphaGo beat world\u2019s #2 Lee Sedol 4-1.\n\nEvolved into AlphaGo Zero (without human datasets), \nthen AlphaZero, and more recently MuZero (model-free) . \n\nINF2D: REASONING AND AGENTS\n\nhttps://en.wikipedia.org/wiki/Lee_Sedol\n\n37\n\n",
        "lecture": " 8",
        "page": "36"
    },
    "8_37": {
        "content": "Summary\n\nGames are fun to work on!\n\nThey illustrate several important points about AI.\n\nPerfection is unattainable \u2192 must approximate!\n\nGood idea to think about what to think about.\n\nModern AI demonstrating superhuman performance.\n\nINF2D: REASONING AND AGENTS\n\n38\n\n",
        "lecture": " 8",
        "page": "37"
    },
    "9_1": {
        "content": "Knowledge bases\n\nInference engine\n\nKnowledge base\n\nDomain-independent algorithms\n\nDomain-specific content\n\nKnowledge base (KB) = set of sentences in a formal language\n\nDeclarative approach to building an agent (or other system):\n\u25e6 Tell it what it needs to know\n\nThen it can Ask itself what to do - answers should follow from the KB\n\nAgents can be viewed at the knowledge level\ni.e. what they know, regardless of how implemented\n\nOr at the implementation level\n\u25e6 i.e. data structures in KB and algorithms that manipulate them\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 9",
        "page": "1"
    },
    "9_2": {
        "content": "A simple \nknowledge-\nbased \nagent\n\nThe agent must be able to:\n\u25e6 represent states, actions, etc.\n\n\u25e6 incorporate new percepts\n\n\u25e6 update internal representations of the world\n\n\u25e6 deduce hidden properties of the world\n\n\u25e6 deduce appropriate actions\n\npersistent:\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 9",
        "page": "2"
    },
    "9_3": {
        "content": "Wumpus World\n\nhttps://en.wikipedia.org/wiki/Hunt_the_Wumpus\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 9",
        "page": "3"
    },
    "9_4": {
        "content": "Wumpus World\n\nEnvironment\n\n\u25e6 Squares adjacent to wumpus are smelly\n\u25e6 Squares adjacent to pits are breezy\n\u25e6 Glitter iff gold is in the same square\n\u25e6 Shooting kills wumpus if you are facing it\n\u25e6 Shooting uses up the only arrow\n\u25e6 Grabbing picks up gold if in same square\n\u25e6 Releasing drops the gold in same square\n\nPerformance measure\n\n\u25e6 gold +1000, death -1000\n\u25e6 -1 per step, -10 for using the arrow\n\nSensors: Stench, Breeze, Glitter, Bump, Scream\n\nActuators: Left turn, Right turn, Forward, Grab, Release, Shoot\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 9",
        "page": "4"
    },
    "9_5": {
        "content": "Wumpus World Environment \nCharacterization\n\nObservable\n\nDeterministic\n\nEpisodic\n\nStatic\n\nDiscrete\n\nSingle-agent\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 9",
        "page": "5"
    },
    "9_6": {
        "content": "Wumpus World Environment \nCharacterization\n\nObservable\n\n\u2022 No \u2013 only local perception\n\nDeterministic\n\nEpisodic\n\nStatic\n\nDiscrete\n\nSingle-agent\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 9",
        "page": "6"
    },
    "9_7": {
        "content": "Wumpus World Environment \nCharacterization\n\nObservable\n\n\u2022 No \u2013 only local perception\n\nDeterministic\n\n\u2022 Yes \u2013 outcomes exactly specified\n\nEpisodic\n\nStatic\n\nDiscrete\n\nSingle-agent\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 9",
        "page": "7"
    },
    "9_8": {
        "content": "Wumpus World Environment \nCharacterization\n\nObservable\n\n\u2022 No \u2013 only local perception\n\nDeterministic\n\n\u2022 Yes \u2013 outcomes exactly specified\n\n\u2022 No \u2013 sequential at the level of actions\n\nEpisodic\n\nStatic\n\nDiscrete\n\nSingle-agent\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 9",
        "page": "8"
    },
    "9_9": {
        "content": "Wumpus World Environment \nCharacterization\n\nObservable\n\n\u2022 No \u2013 only local perception\n\nDeterministic\n\n\u2022 Yes \u2013 outcomes exactly specified\n\n\u2022 No \u2013 sequential at the level of actions\n\n\u2022 Yes \u2013 Wumpus and Pits do not move\n\nEpisodic\n\nStatic\n\nDiscrete\n\nSingle-agent\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 9",
        "page": "9"
    },
    "9_10": {
        "content": "Wumpus World Environment \nCharacterization\n\nObservable\n\n\u2022 No \u2013 only local perception\n\nDeterministic\n\n\u2022 Yes \u2013 outcomes exactly specified\n\nEpisodic\n\nStatic\n\n\u2022 No \u2013 sequential at the level of actions\n\n\u2022 Yes \u2013 Wumpus and Pits do not move\n\nDiscrete\n\n\u2022 Yes\n\nSingle-agent\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 9",
        "page": "10"
    },
    "9_11": {
        "content": "Wumpus World Environment \nCharacterization\n\nObservable\n\n\u2022 No \u2013 only local perception\n\nDeterministic\n\n\u2022 Yes \u2013 outcomes exactly specified\n\nEpisodic\n\nStatic\n\n\u2022 No \u2013 sequential at the level of actions\n\n\u2022 Yes \u2013 Wumpus and Pits do not move\n\nDiscrete\n\n\u2022 Yes\n\nSingle-agent\n\n\u2022 Yes \u2013 Wumpus is essentially a natural feature\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 9",
        "page": "11"
    },
    "9_12": {
        "content": "Exploring a wumpus world\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 9",
        "page": "12"
    },
    "9_13": {
        "content": "Exploring a wumpus world\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 9",
        "page": "13"
    },
    "9_14": {
        "content": "Exploring a wumpus world\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 9",
        "page": "14"
    },
    "9_15": {
        "content": "Exploring a wumpus world\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 9",
        "page": "15"
    },
    "9_16": {
        "content": "Exploring a wumpus world\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 9",
        "page": "16"
    },
    "9_17": {
        "content": "Exploring a wumpus world\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 9",
        "page": "17"
    },
    "9_18": {
        "content": "Exploring a wumpus world\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 9",
        "page": "18"
    },
    "9_19": {
        "content": "Exploring a wumpus world\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 9",
        "page": "19"
    },
    "9_20": {
        "content": "Exploring a wumpus world\n\nLogical agents apply \n\ninference to a \nknowledge base\nto derive new \n\ninformation and make \n\ndecisions.\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 9",
        "page": "20"
    },
    "9_21": {
        "content": "Logic\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 9",
        "page": "21"
    },
    "9_22": {
        "content": "Logic in general\n\nLogics are formal languages for representing information such that conclusions can be drawn\n\nSyntax defines the sentences in the language\n\nSemantics defines the meaning of sentences; define truth of a sentence in a world\n\nE.g., the language of arithmetic\n\nx+2 \u2265 y is a sentence\n\nx2+y > {} is not a sentence\n\nx+2 \u2265 y is true iff the number x+2 is no less than the number y\n\nx+2 \u2265 y is true in a world where x = 7, y = 1\nx+2 \u2265 y is false in a world where x = 0, y = 6\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 9",
        "page": "22"
    },
    "9_23": {
        "content": "Entailment\n\nEntailment means that one thing follows from another:\n\nKB \u22a8 \u03b1\n\nKnowledge base KB entails sentence \u03b1 if and only if \u03b1 is true in all \nworlds where KB is true\n\n\u25e6 e.g. x+y = 4 entails  4 = x+y\n\u25e6 e.g. the KB containing \u201cCeltic won\u201d and \u201cHearts won\u201d entails \u201cEither Celtic won or \n\nHearts won\u201d\n\n\u25e6 Entailment is a relationship between sentences (syntax) that is based on semantics\n\nINF2D: REASONING AND AGENTS\n\n24\n\n",
        "lecture": " 9",
        "page": "23"
    },
    "9_24": {
        "content": "Models\n\nLogicians typically think in terms of models, which are \nformally structured worlds with respect to which truth\ncan be evaluated\n\nWe say m is a model of a sentence \u03b1 if \u03b1 is true in m.\n\nM(\u03b1) is the set of all models of \u03b1.\n\nKB \u22a8 \u03b1 iff M(KB) \uf0cd M(\u03b1)\n\nThe stronger an assertion, the fewer models it has.\n\nINF2D: REASONING AND AGENTS\n\n25\n\n",
        "lecture": " 9",
        "page": "24"
    },
    "9_25": {
        "content": "Entailment in the \nwumpus world\n\nSituation after detecting nothing in [1,1], moving right, \nbreeze in [2,1]\n\nConsider possible models for KB assuming only pits\n\n3 Boolean choices \u2192 8 possible models\n\nWhat are these 8 models?\n\nINF2D: REASONING AND AGENTS\n\n26\n\n",
        "lecture": " 9",
        "page": "25"
    },
    "9_26": {
        "content": "Wumpus models\n\nINF2D: REASONING AND AGENTS\n\n27\n\n",
        "lecture": " 9",
        "page": "26"
    },
    "9_27": {
        "content": "Wumpus models\n\nKB = wumpus-world rules + observations\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 9",
        "page": "27"
    },
    "9_28": {
        "content": "Wumpus models\n\nKB = wumpus-world rules + observations\n\n\u03b11 = \"[1,2] has no pit\u201c\n\nKB \u22a8 \u03b11, proved by model checking\n\u25e6 In every model in which KB is true, \u03b11is also \n\ntrue\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 9",
        "page": "28"
    },
    "9_29": {
        "content": "Wumpus models\n\nKB = wumpus-world rules + observations\n\nINF2D: REASONING AND AGENTS\n\n30\n\n",
        "lecture": " 9",
        "page": "29"
    },
    "9_30": {
        "content": "Wumpus models\n\nKB = wumpus-world rules + observations\n\n\u03b12 = \"[2,2] has no pit\u201c\n\nKB \u22ad \u03b12\n\u25e6 In some models in which KB is true, \u03b12 is false\n\nINF2D: REASONING AND AGENTS\n\n31\n\n",
        "lecture": " 9",
        "page": "30"
    },
    "9_31": {
        "content": "Inference\n\nKB \u22a2i \u03b1 = sentence \u03b1 can be derived from KB by inference procedure i\n\nSoundness\n\u25e6 i is sound if whenever KB \u22a2i \u03b1, it is also true that KB \u22a8 \u03b1\n\nCompleteness\n\u25e6 i is complete if whenever KB \u22a8 \u03b1, it is also true that KB \u22a2i \u03b1\n\nPreview: we will define first-order logic:\n\u25e6 expressive enough to say almost anything of interest, \n\u25e6 sound and complete inference procedure exists.\n\nINF2D: REASONING AND AGENTS\n\nBut first\u2026\n\n32\n\n",
        "lecture": " 9",
        "page": "31"
    },
    "9_32": {
        "content": "Propositional logic\n\nP\n\n\u2228\n\n\u00ac\n\n\u21d2\n\n\u2227\n\n\u21d4\n\nINF2D: REASONING AND AGENTS\n\n33\n\n",
        "lecture": " 9",
        "page": "32"
    },
    "9_33": {
        "content": "Propositional logic: Syntax\n\nPropositional logic is the simplest logic \u2013 illustrates basic ideas\n\n\u25e6 The proposition symbols P1, P2 etc are sentences\n\u25e6 If S is a sentence, \u00acS is a sentence \n\n\u25e6 If S1 and S2 are sentences, S1 \u2227 S2 is a sentence \n\u25e6 If S1 and S2 are sentences, S1 \u2228 S2 is a sentence \n\u25e6 If S1 and S2 are sentences, S1 \u21d2 S2 is a sentence \n\u25e6 If S1 and S2 are sentences, S1 \u21d4 S2 is a sentence \n\n[negation]\n\n[conjunction]\n\n[disjunction]\n\n[implication]\n\n[biconditional]\n\nINF2D: REASONING AND AGENTS\n\n34\n\n",
        "lecture": " 9",
        "page": "33"
    },
    "9_34": {
        "content": "Propositional logic: Semantics\n\nEach model specifies true/false for each proposition symbol\n\ne.g.  P1,2\n\nP2,2\nfalse true\n\nP3,1\nfalse\n\nWith these symbols, 8 possible models\n\u25e6 can be enumerated automatically!\n\nINF2D: REASONING AND AGENTS\n\n35\n\n",
        "lecture": " 9",
        "page": "34"
    },
    "9_35": {
        "content": "Propositional logic: Semantics\n\nRules for evaluating truth with respect to a model m:\n\n\u00acS\n\nis true iff\n\nS is false  \n\nS1 \u2227 S2\nS1 \u2228 S2\nS1 \u21d2 S2\n\nis true iff\n\nis true iff\n\nis true iff\n\ni.e. \n\nis false iff\n\nS1 \u21d4 S2 is true iff\n\nS1 is true and S2 is true\nS1 is true or S2 is true\nS1 is false or S2 is true\nS1 is true and S2 is false\nS1 \u21d2 S2 is true and S2 \u21d2 S1 is true\n\nSimple recursive process evaluates an arbitrary sentence:\n\n\u00acP1,2 \u2227 (P2,2 \u2228 P3,1) = true \u2227 (true \u2228 false) =  true \u2227 true = true\n\nINF2D: REASONING AND AGENTS\n\n36\n\n",
        "lecture": " 9",
        "page": "35"
    },
    "9_36": {
        "content": "Truth tables for connectives\n\nINF2D: REASONING AND AGENTS\n\n37\n\n",
        "lecture": " 9",
        "page": "36"
    },
    "9_37": {
        "content": "Wumpus world \nsentences\n\nLet Pi,j be true if there is a pit in [i, j].\n\nLet Bi,j be true if there is a breeze in [i, j].\n\n\u00acP1,1\n\n\u00acB1,1\n\nB2,1\n\n\u201cPits cause breezes in adjacent squares\u201d\nB1,1 \u21d4 (P1,2 \u2228 P2,1)\nB2,1 \u21d4 (P1,1 \u2228 P2,2 \u2228 P3,1)\n\n\u03b11 = \"[1,2] has no pit\u201c\n\nINF2D: REASONING AND AGENTS\n\n38\n\n",
        "lecture": " 9",
        "page": "37"
    },
    "9_38": {
        "content": "Truth tables for inference\n\nINF2D: REASONING AND AGENTS\n\n39\n\n",
        "lecture": " 9",
        "page": "38"
    },
    "9_39": {
        "content": "Inference by \nenumeration\n\nDepth-first enumeration of all models is \nsound and complete\n\nPL-TRUE?\n\n\u25e6 returns true if a sentence holds within a model\n\nFor n symbols\n\u25e6 Time complexity is O(2n)\n\n\u25e6 Space complexity is O(n)\n\nINF2D: REASONING AND AGENTS\n\n40\n\n",
        "lecture": " 9",
        "page": "39"
    },
    "9_40": {
        "content": "Logical \nequivalence\n\nTwo sentences are logically \nequivalent iff true in the same \nmodels: \n\n\u03b1 \u2261 \u03b2 iff \u03b1 \u22a8 \u03b2 and \u03b2 \u22a8 \u03b1\n\nINF2D: REASONING AND AGENTS\n\n41\n\n",
        "lecture": " 9",
        "page": "40"
    },
    "9_41": {
        "content": "Validity and \nsatisfiability\n\nA sentence is valid if it is true in all models\n\n\u2022 true,  A \u2228 \u00acA,  A \u21d2 A,  (A \u2227 (A \u21d2 B)) \u21d2 B\n\nValidity is connected to inference via the Deduction \nTheorem\n\n\u2022 KB \u22a8 \u03b1 if and only if (KB \u21d2 \u03b1) is valid\n\nA sentence is satisfiable if it is true in some model\n\n\u2022 e.g., A \u2228 B,   C\n\nA sentence is unsatisfiable if it is true in no models\n\n\u2022 e.g., A \u2227 \u00acA\n\nSatisfiability is connected to inference via the \nfollowing:\n\n\u2022 KB \u22a8 \u03b1 if and only if (KB \u2227 \u00ac\u03b1) is unsatisfiable\n\u2022 prove \u03b1 by reductio ad absurdum\n\nINF2D: REASONING AND AGENTS\n\n42\n\n",
        "lecture": " 9",
        "page": "41"
    },
    "9_42": {
        "content": "Proof methods\n\nAPPLICATION OF INFERENCE RULES\n\nMODEL CHECKING\n\n\u2022 Legitimate (sound) generation of new \n\n\u2022 truth table enumeration\n\nsentences from old\n\n\u2022 Proof = a sequence of inference rule \n\napplications\n\u2022 Can use inference rules as operators \n\nin a standard search algorithm!\n\n\u2022 Typically require transformation of \n\nsentences into a normal form\n\n\u2022 Example: resolution\n\n\u2022 (always exponential in n)\n\n\u2022 improved backtracking \n\n\u2022 e.g. DPLL\n\n\u2022 heuristic search in model space\n\n\u2022 (sound but incomplete)\n\u2022 e.g. min-conflicts-like hill-climbing \n\nalgorithms\n\nINF2D: REASONING AND AGENTS\n\n43\n\n",
        "lecture": " 9",
        "page": "42"
    },
    "9_43": {
        "content": "Summary\n\nLogical agents apply inference to a knowledge base to derive new information and make \ndecisions\n\nBasic concepts of logic:\n\u25e6 syntax: formal structure of sentences\n\u25e6 semantics: truth of sentences wrt models\n\u25e6 entailment: necessary truth of one sentence given another\n\u25e6 inference: deriving sentences from other sentences\n\u25e6 soundness: derivations produce only entailed sentences\n\u25e6 completeness: derivations can produce all entailed sentences\n\nWumpus world requires the ability to represent partial and negated information, reason by \ncases, etc.\n\nDoes propositional logic have enough expressive power?\n\nINF2D: REASONING AND AGENTS\n\n44\n\n",
        "lecture": " 9",
        "page": "43"
    },
    "10_1": {
        "content": "Outline\n\nTwo families of efficient algorithms for propositional inference:\n\nComplete backtracking search algorithms\n\n\u2022 DPLL algorithm (Davis, Putnam, Logemann, Loveland)\n\nIncomplete local search algorithms\n\n\u2022 WalkSAT algorithm\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 10",
        "page": "1"
    },
    "10_2": {
        "content": "Clausal Form (CNF)\n\nDPLL and WalkSAT manipulate formulae in conjunctive normal form (CNF). \n\nSentence\n\n\u2022 Formula whose satisfiability is to be determined\n\u2022 Conjunction of clauses\n\nClause\n\nLiteral\n\n\u2022 Disjunction of literals\n\n\u2022 Proposition symbol or negated proposition symbol\n\ne.g.  \ud835\udc34, \u00ac\ud835\udc35 , \ud835\udc35, \u00ac\ud835\udc36 represents  \ud835\udc34 \u0680 \u00ac\ud835\udc35 \u22c0 \ud835\udc35 \u0680 \u00ac\ud835\udc36\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 10",
        "page": "2"
    },
    "10_3": {
        "content": "Conversion to CNF\n\n\ud835\udc351,1 \u21d4 \ud835\udc431,2 \u0680 \ud835\udc432,1\n\nEliminate \u21d4 : replace \u03b1 \u21d4 \ud835\udefd with (\u03b1 \u21d2 \ud835\udefd)\u22c0(\ud835\udefd \u21d2 \ud835\udefc)\n\n\u2022 \ud835\udc351,1 \u21d2 \ud835\udc431,2 \u0680 \ud835\udc432,1 \u22c0 \ud835\udc431,2 \u0680 \ud835\udc432,1 \u21d2 \ud835\udc351,1\n\nEliminate \u21d2 : replace \u03b1 \u21d2 \ud835\udefd with \u00ac\u03b1 \u0680 \ud835\udefd\n\n\u2022 \u00ac\ud835\udc351,1 \u0680 \ud835\udc431,2 \u0680 \ud835\udc432,1 \u22c0 \u00ac \ud835\udc431,2 \u0680 \ud835\udc432,1 \u0680 \ud835\udc351,1\n\nMove \u00ac inwards : use de Morgan\u2019s rules and double negation \u00ac\u00ac\ud835\udefc = \ud835\udefc\n\n\u2022 \u00ac\ud835\udc351,1 \u0680 \ud835\udc431,2 \u0680 \ud835\udc432,1 \u22c0 \u00ac\ud835\udc431,2 \u22c0 \u00ac\ud835\udc432,1 \u0680 \ud835\udc351,1\n\nCreate clauses: apply distributivity law (\u0680 over \u22c0) and flatten\n\n\u2022 \u00ac\ud835\udc351,1 \u0680 \ud835\udc431,2 \u0680 \ud835\udc432,1 \u22c0 \u00ac\ud835\udc431,2\u0680\ud835\udc351,1 \u22c0(\u00ac\ud835\udc432,1\u0680\ud835\udc351,1)\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 10",
        "page": "3"
    },
    "10_4": {
        "content": "DPLL\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 10",
        "page": "4"
    },
    "10_5": {
        "content": "The DPLL algorithm\n\nDetermine if an input propositional logic sentence (in CNF) is satisfiable.\n\nImprovements over truth table enumeration:\n\u25e6 Early termination\n\n\u25e6 Pure symbol heuristic\n\n\u25e6 Unit clause heuristic\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 10",
        "page": "5"
    },
    "10_6": {
        "content": "Early termination\n\nA clause is true if one of its literals is true,\n\u25e6 e.g. if A is true then (A \uf0da \uf0d8B) is true. \n\nA sentence is false if any of its clauses is false,\n\u25e6 e.g. if A is false and B is true then \n\n\u25e6\n\n(A \uf0da \uf0d8B) is false, so any sentence containing it is false. \n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 10",
        "page": "6"
    },
    "10_7": {
        "content": "Pure symbol heuristic\n\nPure symbol: always appears with the same \u201csign\u201d or polarity in all clauses. \n\u25e6 e.g., In the three clauses (A \uf0da \uf0d8B), (\uf0d8B \uf0da \uf0d8C), (C \uf0da A): \n\n\u25e6 A and B are pure, C is impure. \n\nMake literal containing a pure symbol true.\n\u25e6 e.g. Let A and \uf0d8B both be true.\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 10",
        "page": "7"
    },
    "10_8": {
        "content": "Unit clause heuristic\n\nUnit clause: only one literal in the clause\n\u25e6 e.g. (A)\n\nThe only literal in a unit clause must be true.\n\u25e6 e.g. A must be true. \n\nAlso includes clauses where all but one literal is false, \n\u25e6 e.g. (A,B,C) where B and C are false since it is equivalent to (A, false, false) i.e. (A).\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 10",
        "page": "8"
    },
    "10_9": {
        "content": "The DPLL \nalgorithm\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 10",
        "page": "9"
    },
    "10_10": {
        "content": "Tautology Deletion (Optional)\n\nTautology: both a proposition and its negation in a clause.\n\u25e6 e.g. (A, B, \u00acA)\n\nClause bound to be true.\n\u25e6 e.g. whether A is true or false.\n\n\u25e6 Therefore, can be deleted.\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 10",
        "page": "10"
    },
    "10_11": {
        "content": "Mid-Lecture Exercise\n\nApply DPLL heuristics to the following sentence:\n\n(S2,1),  (\u00acS1,1),  (\u00acS1,2),\n\n(\u00acS2,1, W2,2),  (\u00acS1,1, W2,2),  (\u00acS1,2, W2,2), \n\n(\u00acW2,2, S2,1, S1,1, S1,2)\n\nUse case splits if model not found by the heuristics.\n\nSymbols: S1,1 , S1,2 , S2,1, W2,2\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 10",
        "page": "11"
    },
    "10_12": {
        "content": "Solution\n\nPure symbol heuristic:\n\n(S2,1)\n\n(\u00acS1,1)\n\n(\u00acS1,2)\n\n(\u00acS2,1, W2,2)\n\n(\u00acS1,1, W2,2)\n\n(\u00acS1,2, W2,2) \n\n(\u00acW2,2, S2,1, S1,1, S1,2)\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 10",
        "page": "12"
    },
    "10_13": {
        "content": "Solution\n\nPure symbol heuristic:\n\n\u25e6 No literal is pure.\n\nUnit clause heuristic: \n\n(S2,1)\n\n(\u00acS1,1)\n\n(\u00acS1,2)\n\n(\u00acS2,1, W2,2)\n\n(\u00acS1,1, W2,2)\n\n(\u00acS1,2, W2,2) \n\n(\u00acW2,2, S2,1, S1,1, S1,2)\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 10",
        "page": "13"
    },
    "10_14": {
        "content": "Solution\n\nPure symbol heuristic:\n\n\u25e6 No literal is pure.\n\nUnit clause heuristic:\n\n\u25e6 S2,1 is true\n\nT\n\n(\u00acS1,1)\n\n(\u00acS1,2)\n\n(F, W2,2)\n\n(\u00acS1,1, W2,2)\n\n(\u00acS1,2, W2,2) \n\n(\u00acW2,2, T, S1,1, S1,2)\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 10",
        "page": "14"
    },
    "10_15": {
        "content": "Solution\n\nPure symbol heuristic:\n\n\u25e6 No literal is pure.\n\nUnit clause heuristic:\n\n\u25e6 S2,1 is true\n\nEarly termination heuristic:\n\n\u25e6 (\u00acW2,2, S2,1, S1,1, S1,2) is true\n\nT\n\n(\u00acS1,1)\n\n(\u00acS1,2)\n\n(F, W2,2)\n\n(\u00acS1,1, W2,2)\n\n(\u00acS1,2, W2,2) \n\nT\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 10",
        "page": "15"
    },
    "10_16": {
        "content": "Solution\n\nPure symbol heuristic:\n\n\u25e6 No literal is pure.\n\nUnit clause heuristic:\n\n\u25e6 S2,1 is true\n\u25e6 S1,1 is false\n\nEarly termination heuristic:\n\n\u25e6 (\u00acW2,2, S2,1, S1,1, S1,2) is true\n\nT\n\nT\n\n(\u00acS1,2)\n\n(F, W2,2)\n\n(T, W2,2)\n\n(\u00acS1,2, W2,2) \n\nT\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 10",
        "page": "16"
    },
    "10_17": {
        "content": "Solution\n\nPure symbol heuristic:\n\n\u25e6 No literal is pure.\n\nUnit clause heuristic:\n\n\u25e6 S2,1 is true\n\u25e6 S1,1 is false\n\u25e6 S1,2 is false\n\nEarly termination heuristic:\n\n\u25e6 (\u00acW2,2, S2,1, S1,1, S1,2) is true\n\u25e6 (\u00acS1,1, W2,2) is true\n\nT\n\nT\n\nT\n\n(F, W2,2)\n\nT\n\n(T, W2,2) \n\nT\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 10",
        "page": "17"
    },
    "10_18": {
        "content": "Solution\n\nPure symbol heuristic:\n\u25e6 No literal is pure.\n\nUnit clause heuristic:\n\u25e6 S2,1 is true\n\u25e6 S1,1 is false\n\u25e6 S1,2 is false\n\nEarly termination heuristic:\n\u25e6 (\u00acW2,2, S2,1, S1,1, S1,2) is true\n\u25e6 (\u00acS1,1, W2,2) is true\n\u25e6 (\u00acS2,1, W2,2) is true\n\nT\n\nT\n\nT\n\n(F, W2,2)\n\nT\n\nT\n\nT\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 10",
        "page": "18"
    },
    "10_19": {
        "content": "Solution\n\nPure symbol heuristic:\n\u25e6 No literal is pure.\n\nUnit clause heuristic:\n\u25e6 S2,1 is true\n\u25e6 S1,1 is false\n\u25e6 S1,2 is false\n\nEarly termination heuristic:\n\u25e6 (\u00acW2,2, S2,1, S1,1, S1,2) is true\n\u25e6 (\u00acS1,1, W2,2) is true\n\u25e6 (\u00acS2,1, W2,2) is true\n\nUnit clause heuristic:\n\u25e6 W2,2 is true\n\nINF2D: REASONING AND AGENTS\n\nT\n\nT\n\nT\n\nT\n\nT\n\nT\n\nT\n\n20\n\n",
        "lecture": " 10",
        "page": "19"
    },
    "10_20": {
        "content": "WalkSAT\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 10",
        "page": "20"
    },
    "10_21": {
        "content": "The WalkSAT algorithm\n\nIncomplete, local search algorithm\n\nEvaluation function: \n\u25e6 The min-conflict heuristic of minimizing the number of unsatisfied clauses\n\nAlgorithm checks for satisfiability by randomly flipping the values of variables\n\nBalance between greediness and randomness\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 10",
        "page": "21"
    },
    "10_22": {
        "content": "The WalkSAT algorithm\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 10",
        "page": "22"
    },
    "10_23": {
        "content": "Hard satisfiability problems\n\nConsider random 3-CNF sentences. \n\u25e6 Example:\n\n(\uf0d8D \uf0da \uf0d8B \uf0da C) \uf0d9 (B \uf0da \uf0d8A \uf0da \uf0d8C) \uf0d9 (\uf0d8C \uf0da \uf0d8B \uf0da E) \uf0d9 (E \uf0da \uf0d8D \uf0da B) \uf0d9 (B \uf0da E \uf0da \uf0d8C)\n\n\u25e6\n\n\u25e6\n\nm = number of clauses \n\nn = number of symbols\n\nHard problems seem to cluster near m/n = 4.3 (critical point)\n\nINF2D: REASONING AND AGENTS\n\n24\n\n",
        "lecture": " 10",
        "page": "23"
    },
    "10_24": {
        "content": "Hard \nsatisfiability \nproblems\n\nINF2D: REASONING AND AGENTS\n\n25\n\n",
        "lecture": " 10",
        "page": "24"
    },
    "10_25": {
        "content": "Hard \nsatisfiability \nproblems\n\nMedian runtime for 100 \nsatisfiable random 3-CNF \nsentences, n = 50\n\nINF2D: REASONING AND AGENTS\n\n26\n\n",
        "lecture": " 10",
        "page": "25"
    },
    "10_26": {
        "content": "Inference in the Wumpus World\n\nINF2D: REASONING AND AGENTS\n\n27\n\n",
        "lecture": " 10",
        "page": "26"
    },
    "10_27": {
        "content": "Inference-based agents in the \nwumpus world\n\nA wumpus-world agent using propositional logic:\n\u25e6 \uf0d8P1,1\n\u25e6 \uf0d8W1,1\n\u25e6 Bx,y \uf0db (Px,y+1 \uf0da Px,y-1 \uf0da Px+1,y \uf0da Px-1,y) \n\u25e6 Sx,y \uf0db (Wx,y+1 \uf0da Wx,y-1 \uf0da Wx+1,y \uf0da Wx-1,y)\n\u25e6 W1,1 \uf0da W1,2 \uf0da \u2026 \uf0da W4,4\n\u25e6 \uf0d8W1,1 \uf0da \uf0d8W1,2\n\u25e6 \uf0d8W1,1 \uf0da \uf0d8W1,3\n\u25e6 \u2026\n\n\uf0de 64 distinct proposition symbols, 155 sentences\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 10",
        "page": "27"
    },
    "10_28": {
        "content": "The \nWumpus \nAgent\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 10",
        "page": "28"
    },
    "10_29": {
        "content": "The \nWumpus \nAgent\n\nINF2D: REASONING AND AGENTS\n\n30\n\n",
        "lecture": " 10",
        "page": "29"
    },
    "10_30": {
        "content": "We need more!\n\nEffect axioms\n\n0 \u2227 \ud835\udc39\ud835\udc4e\ud835\udc50\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc38\ud835\udc4e\ud835\udc60\ud835\udc610 \u2227 \ud835\udc39\ud835\udc5c\ud835\udc5f\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc510 \u21d2 \ud835\udc3f2,1\n\ud835\udc3f1,1\n\n1\n1 \u2227 \u00ac\ud835\udc3f1,1\n\nWe need extra axioms about the world.\n\nFrame problem! \u2013 representational & inferential\n\nFrame axioms:\n\n\ud835\udc39\ud835\udc5c\ud835\udc5f\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc61 \u21d2 \ud835\udc3b\ud835\udc4e\ud835\udc63\ud835\udc52\ud835\udc34\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc61 \u21d4 \ud835\udc3b\ud835\udc4e\ud835\udc63\ud835\udc52\ud835\udc34\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc61+1\n\n\ud835\udc39\ud835\udc5c\ud835\udc5f\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51\ud835\udc61 \u21d2 \ud835\udc4a\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc62\ud835\udc60\ud835\udc34\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc61 \u21d4 \ud835\udc4a\ud835\udc62\ud835\udc5a\ud835\udc5d\ud835\udc62\ud835\udc60\ud835\udc34\ud835\udc59\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc61+1\n\nSuccessor-state axioms:\n\n\ud835\udc3b\ud835\udc4e\ud835\udc63\ud835\udc52\ud835\udc34\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc61+1 \u21d4 (\ud835\udc3b\ud835\udc4e\ud835\udc63\ud835\udc52\ud835\udc34\ud835\udc5f\ud835\udc5f\ud835\udc5c\ud835\udc64\ud835\udc61 \u2227 \u00ac\ud835\udc46\u210e\ud835\udc5c\ud835\udc5c\ud835\udc61\ud835\udc61)\n\nINF2D: REASONING AND AGENTS\n\n31\n\n",
        "lecture": " 10",
        "page": "30"
    },
    "10_31": {
        "content": "Expressiveness limitation of \npropositional logic\n\nKB contains \"physics\" sentences for every single square.\n\nFor every time t and every location [x,y],\n\nLt\n\nx,y \uf0d9 FacingRightt \uf0d9 Forwardt \uf0de Lt+1\n\nx+1,y\n\nRapid proliferation of clauses!\n\nINF2D: REASONING AND AGENTS\n\n32\n\n",
        "lecture": " 10",
        "page": "31"
    },
    "10_32": {
        "content": "Why?\n\nFundamentals behind SAT/SMT solvers.\n\nHighly specialised and optimised tools.\n\u25e6 Capable of solving problems with thousands of propositions and millions of constraints, \n\ndespite NP-completeness and exponential algorithms!\n\nClose relation to CSPs and optimization problems.\n\nVery large array of applications, e.g.:\n\u25e6 Circuit routing and testing, automatic test generation, formal verification, planning &\n\nscheduling, configuration/customisation, etc.\n\nINF2D: REASONING AND AGENTS\n\n33\n\n",
        "lecture": " 10",
        "page": "32"
    },
    "11_1": {
        "content": "Pros and cons of propositional logic\n\nDeclarative\n\nAllows partial/disjunctive/negated \ninformation\n\n\u25e6 (unlike most data structures and \n\ndatabases!)\n\nCompositional\n\nThe meaning of B1,1 \uf0d9 P1,2 is derived from \nthat of B1,1 and of P1,2\n\nMeaning is context-independent\n\n\u25e6 (unlike natural language, where meaning \n\ndepends on context)\n\nVery limited expressive power\n\n\u25e6 (unlike natural language)\n\n\u25e6 for example, we cannot say \"pits cause \nbreezes in adjacent squares\u201c, except by \nwriting one sentence for each square\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 11",
        "page": "1"
    },
    "11_2": {
        "content": "First-order logic (FOL)\n\nPropositional logic assumes the world contains atomic facts.\n\u25e6 Non-structured propositional symbols, usually finitely many.\n\nFOL assumes the world contains:\n\nObjects\n\nRelations\n\nFunctions\n\n\u2022 people, houses, numbers, colours, football games, \n\nwars, \u2026\n\n\u2022 red, round, prime, brother of, bigger than, part of, \n\ncomes between, \u2026\n\n\u2022 father of, best friend, one more than, plus, \u2026\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 11",
        "page": "2"
    },
    "11_3": {
        "content": "Syntax of FOL: Basic elements\n\nConstants\n\n\u2022 KingJohn, 2, UoE,... \n\nPredicates\n\n\u2022 Brother, >,...\n\nFunctions\n\n\u2022 Sqrt, LeftLegOf,...\n\nVariables\n\n\u2022 x, y, a, b,...\n\nConnectives\n\n\u2022 \uf0d8, \uf0de, \uf0d9, \uf0da, \uf0db\n\nEquality\n\n\u2022 =\n\nQuantifiers\n\n\u2022 \uf022, \uf024\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 11",
        "page": "3"
    },
    "11_4": {
        "content": "Syntax of FOL: Basic elements\n\nArity!\n\nConstants\n\n\u2022 KingJohn/0, 2 /0, UoE /0, ... \n\nPredicates\n\n\u2022 Brother/2, >/2, ...\n\nFunctions\n\n\u2022 Sqrt/1, LeftLegOf/1, +/2, \u2026\n\nVariables\n\n\u2022 x, y, a, b, ...\n\nConnectives\n\n\u2022 \uf0d8, \uf0de, \uf0d9, \uf0da, \uf0db\n\nEquality\n\n\u2022 =\n\nQuantifiers\n\n\u2022 \uf022, \uf024\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 11",
        "page": "4"
    },
    "11_5": {
        "content": "Atomic formulae\n\nAtomic formula = predicate (term1,...,termn) \n\nor term1 = term2\n\n= function (term1,...,termn) \n\nor constant or variable\n\nTerm \n\nExamples:\n\n\u25e6 Brother(KingJohn,RichardTheLionheart)\n\n\u25e6 >( Length( LeftLegOf( Richard )), Length( LeftLegOf( KingJohn )))\n\npredicate\n\nfunctions\n\nconstants\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 11",
        "page": "5"
    },
    "11_6": {
        "content": "Complex formulae\n\nComplex formulae are made from atomic formulae using connectives\n\n\uf0d8P\n\nP \uf0d9Q\n\nP \uf0da Q\n\nP \uf0de Q\n\nP \uf0db Q\n\nExamples:\n\nSibling(KingJohn,Richard) \uf0de Sibling(Richard,KingJohn) \n\n>(1,2) \uf0da \u2264 (1,2)\n\n>(1,2) \uf0d9 \uf0d8 >(1,2)\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 11",
        "page": "6"
    },
    "11_7": {
        "content": "Semantics of first-order logic\n\nFormulae are mapped to an interpretation.\n\n\u25e6 An interpretation is called a model of a set of formulae when all the formulae are true in the interpretation.\n\nAn interpretation contains objects (domain elements) and relations between them. \nMapping specifies referents for :\n\nconstant symbols \uf061 objects\n\npredicate symbols \uf061 relation\n\nfunction symbols \uf061 functions\n\nAn atomic formula predicate(term1,...,termn) is true\n\niff the objects referred to by term1,...,termn\n\nare in the relation referred to by predicate.\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 11",
        "page": "7"
    },
    "11_8": {
        "content": "Interpretations \nfor FOL: \nExample\n\nBrother(KingJohn,RichardTheLionheart)\n\n>( Length( LeftLegOf( Richard )), Length( LeftLegOf( KingJohn )))\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 11",
        "page": "8"
    },
    "11_9": {
        "content": "Universal quantification\n\n\uf022<variables>. <formula>\n\n\u25e6 But will often write \uf022 x,y. P for \uf022x. \uf022y. P\n\n\u25e6 Example: Everyone at UoE is smart: \uf022x. At(x, UoE) \uf0de Smart(x)\n\n\uf022x. P is true in an interpretation m iff P is true with x being each possible \nobject in the interpretation.\n\nRoughly speaking, equivalent to the conjunction of instantiations of P\n\nAt(KingJohn, UoE) \uf0de Smart(KingJohn) \n\n\uf0d9 At(Richard, UoE) \uf0de Smart(Richard) \n\n\uf0d9 At(UoE, UoE) \uf0de Smart(UoE) \n\n\uf0d9 ...\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 11",
        "page": "9"
    },
    "11_10": {
        "content": "Existential quantification\n\n\uf024<variables>. <formula>\n\n\u25e6 But will often write \uf024 x,y. P for \uf024 x. \uf024 y. P\n\n\u25e6 Example: Someone at UoE is smart: \uf024x. At(x, UoE) \uf0d9 Smart(x)\n\n\uf024x. P is true in an interpretation m iff P is true with x being some possible \nobject in the interpretation.\n\nRoughly speaking, equivalent to the disjunction of instantiations of P\n\nAt(KingJohn, UoE) \uf0d9 Smart(KingJohn) \n\n\uf0da At(Richard, UoE) \uf0d9 Smart(Richard) \n\n\uf0da At(UoE, UoE) \uf0d9 Smart(UoE) \n\n\uf0da ...\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 11",
        "page": "10"
    },
    "11_11": {
        "content": "Rule of thumb\n\n\uf0de\uf022\n\n\uf024\n\n\uf0d9\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 11",
        "page": "11"
    },
    "11_12": {
        "content": "Common mistakes\n\n\uf022x. King(x) \uf0de Person(x)\n\n\uf022x. King(x) \uf0d9 Person(x)\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 11",
        "page": "12"
    },
    "11_13": {
        "content": "Common mistakes\n\n\uf024x. Crown(x) \uf0de OnHead(x, John)\n\n\uf024x. Crown(x) \uf0d9 OnHead(x, John)\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 11",
        "page": "13"
    },
    "11_14": {
        "content": "Properties of quantifiers\n\n\uf022x.\uf022y. is the same as \uf022y.\uf022x. \n\n\uf024x.\uf024y. is the same as \uf024y.\uf024x.\n\n\uf024x.\uf022y. is not the same as \uf022y.\uf024x.\n\u25e6 \uf024x. \uf022y. Loves(x, y) : \u201cThere is a person who loves everyone in the world\n\n\u25e6 \uf022y. \uf024x. Loves(x, y) : \u201cEveryone in the world is loved by at least one person\u201d\n\nQuantifier duality: each can be expressed using the other:\n\u25e6 \uf022x. Likes(x, IceCream)\n\n\uf0d8\uf024x. \uf0d8Likes(x, IceCream)\n\n\u2261\n\n\u25e6 \uf024x. Likes(x, Broccoli) \n\n\u2261\n\n\uf0d8\uf022x. \uf0d8Likes(x, Broccoli)\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 11",
        "page": "14"
    },
    "11_15": {
        "content": "Equality\n\nterm1 = term2 is true under a given interpretation if and only if\nterm1 and term2 refer to the same object.\n\nExample. Definition of Sibling in terms of Parent:\n\n\uf022x, y. Sibling(x, y) \uf0db (\uf0d8(x = y) \uf0d9\n\n\uf024m, f. \uf0d8 (m = f) \uf0d9\n\nParent(m, x) \uf0d9 Parent(f, x) \uf0d9 Parent(m, y) \uf0d9 Parent(f, y))\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 11",
        "page": "15"
    },
    "11_16": {
        "content": "Example: The \nkinship \ndomain\n\nBrothers are siblings.\n\n\u2022 \uf022x, y. Brother(x, y) \uf0de Sibling(x, y)\n\nOne's mother is one's female parent.\n\n\u2022 \uf022m, c. Mother(c) = m \uf0db (Female(m) \uf0d9 Parent(m, c))\n\n\u201cSibling\u201d is symmetric.\n\n\u2022 \uf022x, y. Sibling(x, y) \uf0db Sibling(y, x)\n\n\u201cParent\u201d and \u201cChild\u201d are inverse relations.\n\n\u2022 \uf022x, y. Parent(x, y) \uf0db Child(y, x)\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 11",
        "page": "16"
    },
    "11_17": {
        "content": "Example: The \nSet domain\n\n\uf022s. Set(s) \uf0db (s = {}) \uf0da (\uf024x,s2. Set(s2) \uf0d9 s = {x|s2})\n\n\uf0d8\uf024x,s. {x|s} = {}\n\n\uf022x,s. x \uf0ce s \uf0db s = {x|s}\n\n\uf022x,s. x \uf0ce s \uf0db [ \uf024y,s2. (s = {y|s2} \uf0d9 (x = y \uf0da x \uf0ce s2))]\n\n\uf022s1,s2. s1 \uf0cd s2 \uf0db (\uf022x. x \uf0ce s1 \uf0de x \uf0ce s2)\n\n\uf022s1,s2. (s1 = s2) \uf0db (s1 \uf0cd s2 \uf0d9 s2 \uf0cd s1)\n\n\uf022x,s1,s2. x \uf0ce (s1 \uf0c7 s2) \uf0db (x \uf0ce s1 \uf0d9 x \uf0ce s2)\n\n\uf022x,s1,s2. x \uf0ce (s1 \uf0c8 s2) \uf0db (x \uf0ce s1 \uf0da x \uf0ce s2)\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 11",
        "page": "17"
    },
    "11_18": {
        "content": "Interacting with FOL KBs\n\nSuppose a wumpus-world agent using a FOL KB perceives a smell and a breeze (but \nno glitter) at t=5:\n\nTell(KB, Percept( [Smell, Breeze, None], 5))\n\nAsk(KB, \uf024a. BestAction(a, 5))\n\n\u25e6 i.e., does the KB entail some best action at t=5?\n\n\u25e6 Answer: Yes, {a/Shoot}  \n\n\u2190 substitution (binding list) \n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 11",
        "page": "18"
    },
    "11_19": {
        "content": "Substitution\n\nGiven a sentence S and a substitution \u03c3,\n\n\u25e6 S\u03c3 denotes the result of \u201cplugging\u201d \u03c3 into S; e.g.,\n\nS = Smarter(x, y)\n\n\u03c3 = {x/Obama, y/Palin}\n\nS\u03c3 = Smarter(Obama, Palin)\n\nAsk(KB, S) returns some/all \u03c3 such that KB \u22a8 S\u03c3\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 11",
        "page": "19"
    },
    "11_20": {
        "content": "Knowledge base for the wumpus world\n\nPerception\n\n\uf022 t,s,b. Percept( [s, b, Glitter], t) \uf0de Glitter(t)\n\nReflex\n\n\uf022 t. Glitter(t) \uf0de BestAction(Grab, t)\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 11",
        "page": "20"
    },
    "11_21": {
        "content": "Deducing hidden properties\n\n\uf022x, y, a, b. Adjacent([x, y], [a, b]) \uf0db [a, b] \uf0ce { [x+1, y], [x-1, y], [x, y+1], [x, y-1] }\n\n\uf022s, t. At(Agent, s, t) \uf0d9 Breeze(t) \uf0de Breezy(s)\n\nSquares are breezy near a pit:\n\n\u25e6 Diagnostic rule: infer cause from effect\n\n\uf022s. Breezy(s) \uf0de \uf024r. Adjacent(r, s) \uf0d9 Pit(r)\n\n\u25e6 Causal rule: infer effect from cause\n\n\uf022r. Pit(r) \uf0de (\uf022s. Adjacent(r, s) \uf0de Breezy(s) )\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 11",
        "page": "21"
    },
    "11_22": {
        "content": "Why?\n\nUniversal ontology language.\n\u25e6 Onto-logy: from the Greek \u03cc\u03bd (= being, that which is) + \u03bb\u03cc\u03b3\u03bf\u03c2 (= discourse, speaking) \n\n\u25e6 e.g. databases, semantic web, knowledge graphs\n\nAt the core of:\n\u25e6 programming language semantics and type theory.\n\n\u25e6 formal verification and advanced (> propositional) automated reasoning.\n\n\u25e6 theorem proving, including in mathematics, physics, cryptography, and beyond.\n\n\u25e6 logic programming and its derivations, expert systems, rule-based systems.\n\nRenewed interest in the context of explainable AI (XAI) and the \u201cthird-wave of AI\u201d.\n\nINF2D: REASONING AND AGENTS\n\n23\n\nPhil Wadler \u201cWhat does logic have to do with Java?\u201d 2009\n\n",
        "lecture": " 11",
        "page": "22"
    },
    "12_1": {
        "content": "Universal instantiation (UI)\n\nEvery instantiation of a universally quantified formula \uf061 is entailed by it:\n\n\u2200\ud835\udc63. \ud835\udc4e\n\ud835\udc4e{ \u03a4\ud835\udc63 \ud835\udc54}\n\nContains no \n\nvariables!\n\nfor any variable v and ground term g\n\nExample: \uf022x. King(x) \uf0d9 Greedy(x) \uf0de Evil(x)  yields:\n\n\u25e6 King(John) \uf0d9 Greedy(John) \uf0de Evil(John)\n\n\u25e6 King(Richard) \uf0d9 Greedy(Richard) \uf0de Evil(Richard)\n\n\u25e6 King(Father(John)) \uf0d9 Greedy(Father(John)) \uf0de Evil(Father(John))\n\netc...\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 12",
        "page": "1"
    },
    "12_2": {
        "content": "Existential instantiation (EI)\n\nFor any formula \u03b1, variable v, and some constant symbol k that does not\nappear elsewhere in the knowledge base:\n\u2203\ud835\udc63. \ud835\udc4e\n\ud835\udc4e{ \u03a4\ud835\udc63 \ud835\udc58}\n\nExample. \uf024x. Crown(x) \uf0d9 OnHead(x,John)  yields:\n\n\u25e6 Crown(C1) \uf0d9 OnHead(C1,John)\n\nprovided C1 is a new constant symbol, called a Skolem constant\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 12",
        "page": "2"
    },
    "12_3": {
        "content": "Reduction to propositional inference\n\nSuppose the KB contains just the following:\n\n\uf022x. King(x) \uf0d9Greedy(x) \uf0deEvil(x)\n\nKing(John)\n\nGreedy(John)\n\nBrother(Richard, John)\n\nInstantiating the universal sentence in all possible ways, we have:\n\n\u25e6 King(John) \uf0d9Greedy(John) \uf0deEvil(John)\n\n\u25e6 King(Richard) \uf0d9Greedy(Richard) \uf0deEvil(Richard)\n\n\u25e6 King(John)\n\n\u25e6 Greedy(John)\n\n\u25e6 Brother(Richard, John) \n\nUniversal sentence can then be discarded!\n\nThe new KB is propositionalized: proposition symbols are\n\nKing(John), Greedy(John), Evil(John), King(Richard), etc.\n\nNot in KB as a \n\nfact!\n\n4\n\nINF2D: REASONING AND AGENTS\n\n",
        "lecture": " 12",
        "page": "3"
    },
    "12_4": {
        "content": "Reduction contd.\n\nEvery FOL KB can be propositionalized so as to preserve entailment\n\u25e6 A ground sentence is entailed by new KB iff entailed by original KB\n\nIdea: propositionalize KB and query, apply DPLL (or some other \ncomplete propositional method), return result\n\nProblem: with function symbols, there are infinitely many ground terms,\n\u25e6 e.g., Father(Father(Father(John)))\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 12",
        "page": "4"
    },
    "12_5": {
        "content": "Reduction contd.\n\nTheorem: Herbrand (1930)\n\n\u2022 If a sentence \u03b1 is entailed by a FOL KB, it is entailed by a finite subset of the \n\npropositionalized KB\n\nIdea: For n = 0 to \u221e do\n\n\u25e6 create a propositional KB by instantiating with depth-n terms\n\u25e6 see if \u03b1 is entailed by this KB\n\nProblem: works if \u03b1 is entailed, loops forever if \u03b1 is not entailed\n\nTheorem: Turing (1936), Church (1936). \n\n\u2022 Entailment for FOL is semi-decidable (i.e. algorithms exist that say yes to every \n\nentailed sentence, but no algorithm exists that also says no to every non-\nentailed sentence.)\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 12",
        "page": "5"
    },
    "12_6": {
        "content": "Problems with propositionalization\n\nPropositionalization seems to generate lots of irrelevant sentences.\n\nExample:\n\n\uf022x. King(x) \uf0d9 Greedy(x) \uf0de Evil(x) \n\nKing(John)\n\n\uf022y. Greedy(y)\n\nBrother(Richard,John)\n\n\u25e6 It seems obvious that Evil(John), but propositionalization produces lots of facts such as \n\nGreedy(Richard) that are irrelevant.\n\nWith p k-ary predicates and n constants, there are p\u00b7nk instantiations.\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 12",
        "page": "6"
    },
    "12_7": {
        "content": "Unification\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 12",
        "page": "7"
    },
    "12_8": {
        "content": "Unification\n\nWe can get the inference immediately if we can find a substitution \ud835\udf03 such that such \nthat King(x) and Greedy(x) match King(John) and Greedy(y).\n\n\ud835\udf03 = {x/John, y/John}\n\nMore generally:\n\n\ud835\udc48\ud835\udc5b\ud835\udc56\ud835\udc53\ud835\udc66 \ud835\udefc, \ud835\udefd = \ud835\udf03 \u21d4 \ud835\udefc\ud835\udf03 = \ud835\udefd\ud835\udf03\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 12",
        "page": "8"
    },
    "12_9": {
        "content": "Unification examples\n\n\ud835\udefc\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\n\ud835\udefd\n\nKnows(John, Jane)\n\nKnows(y, OJ)\n\nKnows(y, Mother(y))\n\nKnows(x, Richard)\n\n\ud835\udf03\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 12",
        "page": "9"
    },
    "12_10": {
        "content": "Unification examples\n\n\ud835\udefc\n\n\ud835\udefd\n\n\ud835\udf03\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, Jane)\n\n{x/Jane}\n\nKnows(y, OJ)\n\nKnows(y, Mother(y))\n\nKnows(x, Richard)\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 12",
        "page": "10"
    },
    "12_11": {
        "content": "Unification examples\n\n\ud835\udefc\n\n\ud835\udefd\n\n\ud835\udf03\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, Jane)\n\n{x/Jane}\n\nKnows(y, OJ)\n\nKnows(y, Mother(y))\n\nKnows(x, Richard)\n\n{x/OJ, y/John}\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 12",
        "page": "11"
    },
    "12_12": {
        "content": "Unification examples\n\n\ud835\udefc\n\n\ud835\udefd\n\n\ud835\udf03\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, Jane)\n\n{x/Jane}\n\nKnows(y, OJ)\n\n{x/OJ, y/John}\n\nKnows(y, Mother(y))\n\n{y/John, x/Mother(John)}\n\nKnows(x, Richard)\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 12",
        "page": "12"
    },
    "12_13": {
        "content": "Unification examples\n\n\ud835\udefc\n\n\ud835\udefd\n\n\ud835\udf03\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, Jane)\n\n{x/Jane}\n\nKnows(y, OJ)\n\n{x/OJ, y/John}\n\nKnows(y, Mother(y))\n\n{y/John, x/Mother(John)}\n\nKnows(x, Richard)\n\nFail!\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 12",
        "page": "13"
    },
    "12_14": {
        "content": "Unification examples\n\n\ud835\udefc\n\n\ud835\udefd\n\n\ud835\udf03\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, x)\n\nKnows(John, Jane)\n\n{x/Jane}\n\nKnows(y, OJ)\n\n{x/OJ, y/John}\n\nKnows(y, Mother(y))\n\n{y/John, x/Mother(John)}\n\nKnows(x, Richard)\n\nFail!\n\nStandardizing variables apart eliminates overlap of variables\ne.g. change Knows(x, Richard) to Knows(z17, Richard) and then we succeed the last case with \n\ud835\udf03 = {z17/John, x/Richard}\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 12",
        "page": "14"
    },
    "12_15": {
        "content": "MGU\n\nUnifying Knows(John, x) and Knows(y, z)\n\n\ud835\udf03 = {y/John, x/z}    or    \ud835\udf03 = {y/John, x/John, z/John}\n\nThe first unifier is more general than the second.\n\nFOL: There is a single most general unifier (MGU) that is unique up to renaming of \nvariables.\n\nCan be viewed as an equation solving problem.\n\u25e6 i.e. solve Knows(John, x) \u225f Knows(y, z)\n\nMGU = {y/John, x/z}\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 12",
        "page": "15"
    },
    "12_16": {
        "content": "MGU Examples\n\nLoves(John, x) \u225f Loves(y, Mother(y))\n\nLoves(John, Mother(y)) \u225f Loves(y, y)\n\nMGU\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 12",
        "page": "16"
    },
    "12_17": {
        "content": "MGU Examples\n\nLoves(John, x) \u225f Loves(y, Mother(y))\n\n{x/Mother(John), y/John}\n\nLoves(John, Mother(y)) \u225f Loves(y, y)\n\nMGU\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 12",
        "page": "17"
    },
    "12_18": {
        "content": "MGU Examples\n\nLoves(John, x) \u225f Loves(y, Mother(y))\n\n{x/Mother(John), y/John}\n\nLoves(John, Mother(y)) \u225f Loves(y, y)\n\nFail!\n\nMGU\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 12",
        "page": "18"
    },
    "12_19": {
        "content": "Finding the MGU\n\nCan be broken-down into a series of steps\n\u25e6 Decomposition\n\u25e6 Conflict\n\u25e6 Eliminate\n\u25e6 Delete\n\u25e6 Switch\n\u25e6 Coalesce\n\u25e6 Occurs Check\n\nOther presentations of algorithm are possible (see R&N)\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 12",
        "page": "19"
    },
    "12_20": {
        "content": "Given\n\nf(s1, \u2026, sn) \u225f f(t1, \u2026, tn)\n\nReplace with\n\ns1 \u225f t1, \u2026, sn \u225f tn\n\nExample\n\nGiven\n\nKnows(John, x) \u225f Knows(y, z)\n\nReplace with\n\nJohn \u225f y,   x \u225f z\n\nDecomposition\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 12",
        "page": "20"
    },
    "12_21": {
        "content": "Given\n\nf(s1, \u2026, sn) \u225f g(t1, \u2026, tn) where f\u2260g\n\nConflict\n\nFail!\n\nExample\n\nGiven\n\nKnows(John, x) \u225f Greedy(y)\n\nfail\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 12",
        "page": "21"
    },
    "12_22": {
        "content": "Given\n\nP,   x \u225f t where x occurs in P but not in t, and t is not a variable\n\nReplace with\n\nP{x/t} and x \u225f t\n\nExample\n\nGiven\n\nKnows(John, x) \u225f Knows(y, z),   z \u225f Richard\n\nKnows(John, x) \u225f Knows(y, Richard),   z \u225f Richard\n\nReplace with\n\nEliminate\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 12",
        "page": "22"
    },
    "12_23": {
        "content": "Given\n\nP,  s \u225f s\n\nReplace with\n\nP\n\nExample\n\nGiven\n\nDelete\n\nz \u225f Richard,   Greedy(John) \u225f Greedy(John)\n\nReplace with\n\nz \u225f Richard\n\nINF2D: REASONING AND AGENTS\n\n24\n\n",
        "lecture": " 12",
        "page": "23"
    },
    "12_24": {
        "content": "Given\n\nP,  s \u225f x where x is a variable and s is not\n\nReplace with\n\nP and x \u225f s\n\nExample\n\nGiven\n\nKnows(John, x) \u225f Knows(y, z),   Richard \u225f z\n\nReplace with\n\nKnows(John, x) \u225f Knows(y, z),   z \u225f Richard\n\nSwitch\n\nINF2D: REASONING AND AGENTS\n\n25\n\n",
        "lecture": " 12",
        "page": "24"
    },
    "12_25": {
        "content": "Given\n\nP,  x \u225f y where x, y variables occurring in P\n\nReplace with\n\nP{x/y} and x \u225f y\n\nExample\n\nGiven\n\nKnows(John, x) \u225f Knows(y, z),   y \u225f z\n\nReplace with\n\nKnows(John, x) \u225f Knows(z, z),   y \u225f z\n\nCoalesce\n\nINF2D: REASONING AND AGENTS\n\n26\n\n",
        "lecture": " 12",
        "page": "25"
    },
    "12_26": {
        "content": "Given\n\nx \u225f s where x occurs in s and s not a variable\n\nFail!\n\nExample\n\nGiven\n\nP(x),   x \u225f Father(x)\n\nOccurs Check\n\nFail (else Eliminate will loop)\n\nP(Father(Father(Father(\u2026))))\n\nINF2D: REASONING AND AGENTS\n\n27\n\n",
        "lecture": " 12",
        "page": "26"
    },
    "12_27": {
        "content": "Example\n\nLoves(John, x) \u225f Loves(y, Mother(y))\n\nDecompose\n\nJohn \u225f y,  x \u225f Mother(y)\n\nSwitch\n\ny \u225f John,  x \u225f Mother(y)\n\nEliminate\n\ny \u225f John,  x \u225f Mother(John)\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 12",
        "page": "27"
    },
    "12_28": {
        "content": "Generalised Modus \nPonens\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 12",
        "page": "28"
    },
    "12_29": {
        "content": "Modus Ponendo Ponens\n\nLatin for \u201cmethod of putting by placing\u201d \u2013 \u201cway that affirms by affirming\u201d\n\n\ud835\udc43\n\n\ud835\udc43 \u27f9 \ud835\udc44\n\ud835\udc44\n\n\ud835\udc43,\n\n\ud835\udc43 \u27f9 \ud835\udc44 \u22a2 \ud835\udc44\n\nINF2D: REASONING AND AGENTS\n\n30\n\n",
        "lecture": " 12",
        "page": "29"
    },
    "12_30": {
        "content": "Generalized Modus Ponens (GMP)\n\n\u2032 , \ud835\udc5d2\n\ud835\udc5d1\n\n\u2032 ,\u2026,\ud835\udc5d\ud835\udc5b\n\u2032\n\n(\ud835\udc5d1 \u067f \ud835\udc5d2 \u067f\u2026 \u067f \ud835\udc5d\ud835\udc5b\u27f9\ud835\udc5e)\n\n\ud835\udc5e\ud835\udf03\n\nwhere \ud835\udc5d\ud835\udc56\n\n\u2032\ud835\udf03 \u2261 \ud835\udc5d\ud835\udc56\ud835\udf03\n\nExample: \uf022x. King(x) \uf0d9 Greedy(x) \uf0de Evil(x) \n\n\u2032 is King(John)  \n\ud835\udc5d1\n\u2032\n\ud835\udc5d2\n\nis Greedy(y)  \n\n\ud835\udf03 is {x/John, y/John} \n\n\ud835\udc5e\ud835\udf03 is Evil(John)\n\n\ud835\udc5d1 is King(x) \n\n\ud835\udc5d2 is Greedy(x) \n\n\ud835\udc5e is Evil(x) \n\nGMP used with KB of definite clauses (exactly one positive literal)\n\nAll variables assumed universally quantified\n\nINF2D: REASONING AND AGENTS\n\n31\n\n",
        "lecture": " 12",
        "page": "30"
    },
    "12_31": {
        "content": "Soundness of GMP\n\nNeed to show that \n\u2032 , \ud835\udc5d2\n\ud835\udc5d1\n\n\u2032 , \u2026 , \ud835\udc5d\ud835\udc5b\n\n\u2032 , (\ud835\udc5d1\u2227 \ud835\udc5d2 \u2227 \u2026 \u2227 \ud835\udc5d\ud835\udc5b \u21d2 \ud835\udc5e) \u22a8 \ud835\udc5e\ud835\udf03\n\nprovided that \ud835\udc5d\ud835\udc56\n\n\u2032\ud835\udf03 = \ud835\udc5d\ud835\udc56\ud835\udf03 for all i\n\nLemma: For any sentence p, we have \ud835\udc5d \u22a8 \ud835\udc5d\ud835\udf03 by UI\n1. (\ud835\udc5d1\u2227 \ud835\udc5d2 \u2227 \u2026 \u2227 \ud835\udc5d\ud835\udc5b \u21d2 \ud835\udc5e) \u22a8 (\ud835\udc5d1\u2227 \ud835\udc5d2 \u2227 \u2026 \u2227 \ud835\udc5d\ud835\udc5b \u21d2 \ud835\udc5e)\ud835\udf03 = (\ud835\udc5d1\ud835\udf03 \u2227 \ud835\udc5d2\ud835\udf03 \u2227 \u2026 \u2227 \ud835\udc5d\ud835\udc5b\ud835\udf03 \u21d2 \ud835\udc5e\ud835\udf03)\n2. \ud835\udc5d1\n\n\u2032 \u2227 \u2026 \u2227 \ud835\udc5d\ud835\udc5b\n\n\u2032 \u2227 \u2026 \u2227 \ud835\udc5d\ud835\udc5b\n\n\u2032 \u22a8 (\ud835\udc5d1\n\n\u2032 \u2227 \ud835\udc5d2\n\n\u2032 , \ud835\udc5d2\n\n\u2032 , \u2026 , \ud835\udc5d\ud835\udc5b\n\n\u2032 \u22a8 \ud835\udc5d1\n\n\u2032 \u2227 \ud835\udc5d2\n\n\u2032 ) \ud835\udf03\n\n\u2032 \ud835\udf03 = \ud835\udc5d1\ud835\udf03 \u2227 \ud835\udc5d2\ud835\udf03 \u2227 \u2026 \u2227 \ud835\udc5d\ud835\udc5b\n\n= \ud835\udc5d1\n\n\u2032 \ud835\udf03 \u2227 \ud835\udc5d2\n\n\u2032 \ud835\udf03 \u2227 \u2026 \u2227 \ud835\udc5d\ud835\udc5b\n\n3. From 1 and 2, \ud835\udc5e\ud835\udf03 follows by ordinary Modus Ponens\n\nINF2D: REASONING AND AGENTS\n\n32\n\n",
        "lecture": " 12",
        "page": "31"
    },
    "12_32": {
        "content": "New \nExample \nKB\n\nINF2D: REASONING AND AGENTS\n\n33\n\n",
        "lecture": " 12",
        "page": "32"
    },
    "12_33": {
        "content": "Example Knowledge Base\n\nIt is known in The Hundred-Acre \nWood that if someone who is very \nfond of food gives a treat to one of \ntheir friends, they are really generous.\n\nEeyore, the sad donkey, has some \nhunny that he has received for his \nbirthday from Winnie-the-Pooh, who, \nas we know, is very fond of food.\n\nProve that Winnie-the-Pooh is \ngenerous.\n\nINF2D: REASONING AND AGENTS\n\n34\n\n",
        "lecture": " 12",
        "page": "33"
    },
    "12_34": {
        "content": "Formalisation\n\nif someone who is very fond of food gives a treat to one of their friends, they \nare really generous\n\n\u2022 \ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2\n\n\ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\nEeyore (\u2026) has some hunny\n\n\u2022 \u2203\ud835\udc65. \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 or after EI: \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b1 \u2227\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3b1\n\nthat he has received for his birthday from Winnie-the-Pooh\n\n\u2022 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\nHunny is a treat.\n\n\u2022 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\nResidents of the the Hundred-Acre Wood are friends.\n\n\u2022 \ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc51\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc34\ud835\udc50\ud835\udc5f\ud835\udc52\ud835\udc4a\ud835\udc5c\ud835\udc5c\ud835\udc51 \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\nEeyore is a resident of the the Hundred-Acre Wood.\n\n\u2022 \ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc51\ud835\udc5f\ud835\udc52\ud835\udc51\ud835\udc34\ud835\udc50\ud835\udc5f\ud835\udc52\ud835\udc4a\ud835\udc5c\ud835\udc5c\ud835\udc51)\n\nPooh is very fond of food.\n\n\u2022 \ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n35\n\n",
        "lecture": " 12",
        "page": "34"
    },
    "12_35": {
        "content": "Why?\n\nSetting the scene for inference & resolution. \n\nLinked to logic programming.\n\nMeta-theory. \n\nINF2D: REASONING AND AGENTS\n\n36\n\n\u2026but more in the next lecture!\n\n",
        "lecture": " 12",
        "page": "35"
    },
    "13_1": {
        "content": "Forward chaining\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 13",
        "page": "1"
    },
    "13_2": {
        "content": "\u2018Winnie-the-Pooh\u2019 Knowledge Base\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 13",
        "page": "2"
    },
    "13_3": {
        "content": "Forward chaining \nproof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 13",
        "page": "3"
    },
    "13_4": {
        "content": "Forward chaining \nproof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 13",
        "page": "4"
    },
    "13_5": {
        "content": "Forward chaining \nproof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 13",
        "page": "5"
    },
    "13_6": {
        "content": "Forward chaining \nproof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 13",
        "page": "6"
    },
    "13_7": {
        "content": "Forward \nchaining \nalgorithm\n\nPattern-matching\n\nFacts irrelevant to the goal can be generated\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 13",
        "page": "7"
    },
    "13_8": {
        "content": "Properties of forward chaining\n\nSound and complete for first-order definite clauses\n\u25e6 Definite clause = exactly one positive literal.\n\nDatalog = first-order definite clauses + no functions\n\u25e6 FC terminates for Datalog in finite number of iterations\n\nMay not terminate in general if \u03b1 is not entailed\n\nThis is unavoidable: entailment with definite clauses is semi-decidable\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 13",
        "page": "8"
    },
    "13_9": {
        "content": "Efficiency of forward chaining\n\nIncremental forward chaining: no need to match a rule on iteration k if a \npremise wasn't added on iteration k-1\n\uf0de match each rule whose premise contains a newly added positive literal\n\nMatching itself can be expensive:\n\nDatabase indexing allows O(1) retrieval of known facts\n\n\u25e6 e.g. query Hunny(x) retrieves Hunny(J)\n\nForward chaining is widely used in deductive databases\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 13",
        "page": "9"
    },
    "13_10": {
        "content": "Efficiency of forward chaining II\n\n\u25e6 Finding all possible unifiers can be very expensive\n\nExample:\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\u25e6 Can find each object owned by Eeyore in constant time and then check if it is a jar of hunny.\n\n\u25e6 But what if Eeyore owns many objects but very few jars?\n\n\u25e6 Conjunct Ordering: Better (cost-wise) to find all jars first and then check whether they are \n\nowned by Eeyore. \n\n\u25e6 Optimal ordering is NP-hard. Heuristics available: e.g. MRV from CSP if each conjunct is \n\nviewed as a constraint on its variables. \n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 13",
        "page": "10"
    },
    "13_11": {
        "content": "Hard matching example\n\nDiff(WA, NT) \uf0d9 Diff(WA, SA) \uf0d9 Diff(NT, Q) \uf0d9\nDiff(NT, SA) \uf0d9 Diff(Q, NSW) \uf0d9 Diff(Q, SA) \uf0d9\nDiff(NSW, V) \uf0d9 Diff(NSW, SA) \uf0d9 Diff(V, SA) \n\n\uf0de Colourable\n\nDiff(Red, Blue)  Diff (Red, Green) \nDiff(Green, Red)  Diff(Green, Blue) \nDiff(Blue, Red)  Diff(Blue, Green)\n\nEvery finite domain CSP can be expressed as a single definite clause + ground facts\n\nColourable is inferred iff the CSP has a solution\n\nCSPs include 3SAT as a special case, hence matching is NP-hard\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 13",
        "page": "11"
    },
    "13_12": {
        "content": "Backward chaining\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 13",
        "page": "12"
    },
    "13_13": {
        "content": "Backward \nchaining proof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 13",
        "page": "13"
    },
    "13_14": {
        "content": "Backward \nchaining proof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 13",
        "page": "14"
    },
    "13_15": {
        "content": "Backward \nchaining proof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 13",
        "page": "15"
    },
    "13_16": {
        "content": "Backward \nchaining proof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 13",
        "page": "16"
    },
    "13_17": {
        "content": "Backward \nchaining proof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 13",
        "page": "17"
    },
    "13_18": {
        "content": "Backward \nchaining proof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n19\n\n",
        "lecture": " 13",
        "page": "18"
    },
    "13_19": {
        "content": "Backward \nchaining proof\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227\n\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n20\n\n",
        "lecture": " 13",
        "page": "19"
    },
    "13_20": {
        "content": "Fetch rules \nthat might \n\nunify\n\nBackward \nchaining \nalgorithm\n\nA function that \nreturns multiple \ntimes, each time \n\ngiving one possible \n\nresult\n\nINF2D: REASONING AND AGENTS\n\n21\n\n",
        "lecture": " 13",
        "page": "20"
    },
    "13_21": {
        "content": "Properties of backward chaining\n\nDepth-first recursive proof search: space is linear in size of proof\n\nIncomplete due to infinite loops\n\u25e6 partial fix by checking current goal against every goal on stack\n\nInefficient due to repeated subgoals (both success and failure)\n\u25e6 fix using caching of previous results (extra space)\n\nWidely used for logic programming.\n\nINF2D: REASONING AND AGENTS\n\n22\n\n",
        "lecture": " 13",
        "page": "21"
    },
    "13_22": {
        "content": "Resolution\n\nINF2D: REASONING AND AGENTS\n\n23\n\n",
        "lecture": " 13",
        "page": "22"
    },
    "13_23": {
        "content": "Ground Binary Resolution\n\n\ud835\udc36 \u2228 \ud835\udc43\n\n\ud835\udc37 \u2228 \u00ac\ud835\udc43\n\n\ud835\udc36 \u2228 \ud835\udc37\n\nSoundness:\n\nC \uf0da P\n\niff \u00ac C \uf0de P\n\nD \uf0da \u00acP\n\niff P \uf0de D\n\n\u25e6 Therefore, \u00ac C \uf0de D\n\n\u25e6 Which is equivalent to C \uf0da D\n\nNote:  if both C and D are empty then resolution deduces the empty clause, i.e. false.\n\nINF2D: REASONING AND AGENTS\n\n24\n\n",
        "lecture": " 13",
        "page": "23"
    },
    "13_24": {
        "content": "Non-Ground Binary Resolution\n\n\ud835\udc36\u2228\ud835\udc43\n\n\ud835\udc37\u2228\u00ac\ud835\udc43\u2032\n\n(\ud835\udc36\u2228\ud835\udc37)\ud835\udf03\n\nwhere \ud835\udf03 is the mgu of P and P\u2019\n\n\u27a2 The two clauses are assumed to be standardized apart so that they \nshare no variables.\n\nSoundness: apply \uf071 to premises then appeal to ground binary \nresolution.\n\n\ud835\udc36\ud835\udf03 \u2228 \ud835\udc43\ud835\udf03\n\n\ud835\udc37\ud835\udf03 \u2228 \u00ac\ud835\udc43\ud835\udf03\n\n\ud835\udc36\ud835\udf03 \u2228 \ud835\udc37\ud835\udf03\n\nINF2D: REASONING AND AGENTS\n\n25\n\n",
        "lecture": " 13",
        "page": "24"
    },
    "13_25": {
        "content": "Example\n\n\u00ac\ud835\udc3b\ud835\udc4e\ud835\udc60\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2228 \ud835\udc3b\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc66 \ud835\udc65\n\n\ud835\udc3b\ud835\udc4e\ud835\udc60\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\n\ud835\udc3b\ud835\udc4e\ud835\udc5d\ud835\udc5d\ud835\udc66(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nwith \ud835\udf03 = { \u03a4\ud835\udc65 \ud835\udc43\ud835\udc5c\ud835\udc5c\u210e}\n\nINF2D: REASONING AND AGENTS\n\n26\n\n",
        "lecture": " 13",
        "page": "25"
    },
    "13_26": {
        "content": "Factoring\n\n\ud835\udc36\u2228\ud835\udc431\u2228\u22ef\u2228\ud835\udc43\ud835\udc5a\n\n(\ud835\udc36\u2228\ud835\udc431)\ud835\udf03\n\nwhere \ud835\udf03 is the mgu of the Pi\n\nSoundness: by universal instantiation and deletion of duplicates.\n\nINF2D: REASONING AND AGENTS\n\n27\n\n",
        "lecture": " 13",
        "page": "26"
    },
    "13_27": {
        "content": "Full Resolution\n\n\ud835\udc36\u2228\ud835\udc431\u2228\u22ef\u2228\ud835\udc43\ud835\udc5a\n\n\ud835\udc37\u2228\u00ac\ud835\udc431\n\n\u2032\u2228\u22ef\u2228\u00ac\ud835\udc43\ud835\udc5b\n\u2032\n\n(\ud835\udc36\u2228\ud835\udc37)\ud835\udf03\n\nwhere \u03b8 is mgu of all Pi and Pi\n\n\u2019\n\nSoundness: by combination of factoring and binary resolution.\n\nTo prove \u03b1: apply resolution steps to CNF(KB \uf0d9 \uf0d8\u03b1);\n\u25e6 complete for FOL, if full resolution or binary resolution + factoring is used\n\nINF2D: REASONING AND AGENTS\n\n28\n\n",
        "lecture": " 13",
        "page": "27"
    },
    "13_28": {
        "content": "Conversion to CNF (1/2)\n\n\u2200\ud835\udc65. \u2200\ud835\udc66. \ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc66 \u21d2 \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66 \u21d2 (\u2203\ud835\udc66. \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc66, \ud835\udc65 )\n\nEliminate \u21d4, \u21d2 : replace \u03b1 \u21d4 \ud835\udefd with (\u03b1 \u21d2 \ud835\udefd)\u22c0(\ud835\udefd \u21d2 \ud835\udefc) and \u03b1 \u21d2 \ud835\udefd with \u00ac\u03b1 \u0680 \ud835\udefd\n\n\u2022 \u2200\ud835\udc65. \u00ac \u2200\ud835\udc66. \u00ac\ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc66 \u2228 \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66 \u2228 (\u2203\ud835\udc66. \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc66, \ud835\udc65 )\n\nMove \u00ac inwards : use de Morgan\u2019s rules, \u00ac\u00ac\ud835\udefc = \ud835\udefc, \u00ac\u2200\ud835\udc65. \ud835\udc43 \u2261 \u2203\ud835\udc65. \u00ac\ud835\udc43, \u00ac\u2203\ud835\udc65. \ud835\udc43 \u2261 \u2200\ud835\udc65. \u00ac\ud835\udc43\n\n\u2022 \u2200\ud835\udc65. \u2203\ud835\udc66. \u00ac(\u00ac\ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc66 \u2228 \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66 ) \u2228 (\u2203\ud835\udc66. \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc66, \ud835\udc65 )\n\u2022 \u2200\ud835\udc65. \u2203\ud835\udc66. \u00ac\u00ac\ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc66 \u2227 \u00ac\ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66 \u2228 (\u2203\ud835\udc66. \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc66, \ud835\udc65 )\n\u2022 \u2200\ud835\udc65. \u2203\ud835\udc66. \ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc66 \u2227 \u00ac\ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66 \u2228 (\u2203\ud835\udc66. \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc66, \ud835\udc65 )\n\nStandardize variables apart: each quantifier should use a different one\n\n\u2022 \u2200\ud835\udc65. \u2203\ud835\udc66. \ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc66 \u2227 \u00ac\ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66 \u2228 (\u2203\ud835\udc67. \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc67, \ud835\udc65 )\n\nINF2D: REASONING AND AGENTS\n\n29\n\n",
        "lecture": " 13",
        "page": "28"
    },
    "13_29": {
        "content": "Conversion to CNF (2/2)\n\n\u2200\ud835\udc65. \u2203\ud835\udc66. \ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc66 \u2227 \u00ac\ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66 \u2228 (\u2203\ud835\udc67. \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc67, \ud835\udc65 )\n\nSkolemize: a more general form of existential instantiation\n\n\u2022 Each existential variable is replaced by a Skolem function of the enclosing\n\nuniversally quantified variables.\n\n\u2022 \u2200\ud835\udc65. \ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc39(\ud835\udc65) \u2227 \u00ac\ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc39(\ud835\udc65) \u2228 \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc3a(\ud835\udc65), \ud835\udc65\n\nDrop universal quantifiers \u2200\n\n\u2022 \ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc39(\ud835\udc65) \u2227 \u00ac\ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc39(\ud835\udc65) \u2228 \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc3a(\ud835\udc65), \ud835\udc65\n\nCreate clauses: apply distributivity law (\u0680 over \u22c0) and flatten\n\n\u2022 \ud835\udc34\ud835\udc5b\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc59 \ud835\udc39 \ud835\udc65 \u2228 \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc3a \ud835\udc65 , \ud835\udc65 \u2227 (\u00ac\ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc39 \ud835\udc65 \u2228 \ud835\udc3f\ud835\udc5c\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc3a \ud835\udc65 , \ud835\udc65 )\n\nINF2D: REASONING AND AGENTS\n\n30\n\n",
        "lecture": " 13",
        "page": "29"
    },
    "13_30": {
        "content": "Resolution \nalgorithm\n\nINF2D: REASONING AND AGENTS\n\n31\n\n",
        "lecture": " 13",
        "page": "30"
    },
    "13_31": {
        "content": "\u2018Winnie-the-Pooh\u2019 Knowledge Base\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2227 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2227 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2227 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u21d2 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d \u2227 \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2227 \ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u21d2 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u21d2 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u21d2 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n32\n\n",
        "lecture": " 13",
        "page": "31"
    },
    "13_32": {
        "content": "\u2018Winnie-the-Pooh\u2019 Knowledge Base\n\n\u00ac\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc65 \u2228 \u00ac\ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61 \ud835\udc66 \u2228 \u00ac\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51 \ud835\udc67 \u2228 \u00ac\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60 \ud835\udc65, \ud835\udc66, \ud835\udc67 \u2228 \ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d\n\n\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc3d\n\n\u00ac\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2228 \u00ac\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60 \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65 \u2228 \ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\u00ac\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66 \ud835\udc65 \u2228 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\u00ac\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61 \ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a \u2228 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n33\n\n",
        "lecture": " 13",
        "page": "32"
    },
    "13_33": {
        "content": "Resolution proof\n\n\u00ac\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc65) \u2228 \u00ac\ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc66) \u2228\n\u00ac\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc67) \u2228 \u00ac\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc65, \ud835\udc66, \ud835\udc67) \u2228\n\ud835\udc3a\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc5f\ud835\udc5c\ud835\udc62\ud835\udc60(\ud835\udc65)\n\n\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3d) \ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66(\ud835\udc3d)\n\n\u00ac\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66(\ud835\udc65) \u2228 \u00ac\ud835\udc42\ud835\udc64\ud835\udc5b\ud835\udc60(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc65) \u2228\n\ud835\udc3a\ud835\udc56\ud835\udc63\ud835\udc52\ud835\udc60(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e, \ud835\udc65, \ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52)\n\n\u00ac\ud835\udc3b\ud835\udc62\ud835\udc5b\ud835\udc5b\ud835\udc66(\ud835\udc65) \u2228 \ud835\udc47\ud835\udc5f\ud835\udc52\ud835\udc4e\ud835\udc61(\ud835\udc65)\n\n\u00ac\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc65, \ud835\udc3b\ud835\udc34\ud835\udc4a) \u2228 \ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc65)\n\n\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc51\ud835\udc52\ud835\udc5b\ud835\udc61(\ud835\udc38\ud835\udc52\ud835\udc66\ud835\udc5c\ud835\udc5f\ud835\udc52, \ud835\udc3b\ud835\udc34\ud835\udc4a)\n\n\ud835\udc49\ud835\udc52\ud835\udc5f\ud835\udc66\ud835\udc39\ud835\udc5c\ud835\udc5b\ud835\udc51\ud835\udc42\ud835\udc53\ud835\udc39\ud835\udc5c\ud835\udc5c\ud835\udc51(\ud835\udc43\ud835\udc5c\ud835\udc5c\u210e)\n\nINF2D: REASONING AND AGENTS\n\n34\n\n",
        "lecture": " 13",
        "page": "33"
    },
    "13_34": {
        "content": "Why?\n\nFundamentals of reasoning in FOL.\n\nAutomated logic-based reasoning.\n\nProof search.\n\nApplications discussed in Lecture 11.\n\nINF2D: REASONING AND AGENTS\n\n35\n\n",
        "lecture": " 13",
        "page": "34"
    },
    "15_1": {
        "content": "Intelligent Agents and their \nEnvironments\n\nSimple reflex agents\n\nModel-based reflex agents\n\nGoal-based agents\n\nUtility-based agents\n\nLearning agents\n\nProperties of environments\n\u25e6 Partially vs. fully observable\n\n\u25e6 Deterministic vs. stochastic\n\n\u25e6 Episodic vs. sequential\n\n\u25e6 Static vs. dynamic\n\n\u25e6 Discrete vs. continuous\n\n\u25e6 Single vs. multi-agent\n\nINF2D: REASONING AND AGENTS\n\n2\n\n",
        "lecture": " 15",
        "page": "1"
    },
    "15_2": {
        "content": "Problem Solving by Searching\n\nProblem formulation usually requires abstracting away real-world details to define a \nstate space that can feasibly be explored.\n\nVariety of uninformed search strategies:\n\n\u25e6 breadth-first, depth-first, iterative deepening\n\nIterative deepening search uses only linear space and not much more time than other \nuninformed algorithms.\n\nINF2D: REASONING AND AGENTS\n\n3\n\n",
        "lecture": " 15",
        "page": "2"
    },
    "15_3": {
        "content": "Informed Search and Exploration for \nAgents\n\nSmart search based on heuristic scores\n\u25e6 Best-first search\n\n\u25e6 Greedy best-first search\n\n\u25e6 A* search\n\n\u25e6 Admissible heuristics and optimality.\n\nINF2D: REASONING AND AGENTS\n\n4\n\n",
        "lecture": " 15",
        "page": "3"
    },
    "15_4": {
        "content": "Example\n\nINF2D: REASONING AND AGENTS\n\n5\n\n",
        "lecture": " 15",
        "page": "4"
    },
    "15_5": {
        "content": "Example\n\nA\nB C D\n\n4 + 4 = 8\n\n5 + 2 = 7\n\n4 + 2 = 6\n\nINF2D: REASONING AND AGENTS\n\n6\n\n",
        "lecture": " 15",
        "page": "5"
    },
    "15_6": {
        "content": "Example\n\nA\nB C D\nI J\n\n4 + 4 = 8\n\n5 + 2 = 7\n\n4 + 2 = 6\n\n4 + 4 = 8\n\n4 + 2 + 2 = 8\n\nINF2D: REASONING AND AGENTS\n\n7\n\n",
        "lecture": " 15",
        "page": "6"
    },
    "15_7": {
        "content": "Example\n\nA\nB C D\nI J\nG H\n\n5 + 2 = 7\n\n4 + 2 = 6\n\n4 + 4 = 8\n\n5 + 2 + 2 = 9\n\n4 + 2 + 2 = 8\n\n5 + 2 + 0 = 7\n\n4 + 4 = 8\n\nWe\u2019re done as we\u2019ve expanded a node containing a goal state\n\nINF2D: REASONING AND AGENTS\n\n8\n\n",
        "lecture": " 15",
        "page": "7"
    },
    "15_8": {
        "content": "Smart Searching Using Constraints\n\nCSPs are a special kind of problem:\n\u25e6 states defined by values of a fixed set of variables\n\n\u25e6 goal test defined by constraints on variable values\n\nBacktracking = depth-first search with one variable assigned per node.\n\nVariable ordering and value selection heuristics help significantly.\n\nForward checking prevents assignments that guarantee later failure.\n\nConstraint propagation (e.g. arc consistency) does additional work to constrain values \nand detect inconsistencies.\n\nINF2D: REASONING AND AGENTS\n\n9\n\n",
        "lecture": " 15",
        "page": "8"
    },
    "15_9": {
        "content": "Minimax values\n\nAdversarial \nSearch\n\nMinimax assumes that \nboth players play \noptimally\n\u25e6 Informally, each agent is \n\nmaking its decision for the \nnext move based on the \nassumption that the other \nagent is playing as well as \nit can. \n\n\u25e6 If not things can go wrong!\n\nINF2D: REASONING AND AGENTS\n\n10\n\n",
        "lecture": " 15",
        "page": "9"
    },
    "15_10": {
        "content": "Adversarial Search (Contd)\n\n\u03b1-\u03b2 Pruning and its properties\n\nReasoning about relevant computations only enables search space to be pruned.\n\nHow to deal with deep trees: need for evaluation functions.\n\nINF2D: REASONING AND AGENTS\n\n11\n\n",
        "lecture": " 15",
        "page": "10"
    },
    "15_11": {
        "content": "Logical Agents: Knowledge Bases \nand the Wumpus World\n\nLogical agents apply inference to a knowledge base to derive new information and make decisions.\n\nBasic concepts of logic:\n\n\u25e6 syntax: formal structure of sentences\n\n\u25e6 semantics: truth of sentences with respect to models\n\n\u25e6 entailment: necessary truth of one sentence given another\n\n\u25e6 inference: deriving sentences from other sentences\n\n\u25e6 soundness: derivations produce only entailed sentences\n\n\u25e6 completeness: derivations can produce all entailed sentences\n\nWumpus world requires the ability to represent partial and negated information, reason by cases, etc.\n\nPropositional logic solves many problems, but lacks expressive power.\n\nINF2D: REASONING AND AGENTS\n\n12\n\n",
        "lecture": " 15",
        "page": "11"
    },
    "15_12": {
        "content": "Effective Propositional Inference\n\nLogical agents apply inference to a knowledge base to derive new information and make \ndecisions.\n\nTwo algorithms: DPLL & WalkSAT\n\u25e6 DPLL is a decision procedure, i.e. it will return true (yes) or false (no) for a set of propositional \n\nclauses (cf. complete algorithm)\n\nTautology refers to a valid statement i.e. one that is true under all possible interpretations \n\u25e6 e.g. P \u2228 \u00acP or P \u21d2 P\n\nA sentence is satisfiable if there exists an interpretation that makes it true \n\u25e6 e.g. P \uf0d9 Q is true with P true and Q true. It is false if any of P or Q is false.\n\nHard satisfiability problems.\n\nINF2D: REASONING AND AGENTS\n\n13\n\n",
        "lecture": " 15",
        "page": "12"
    },
    "15_13": {
        "content": "First-Order Logic\n\nFirst-order logic:\n\u25e6 objects and relations are semantic primitives\n\n\u25e6 syntax: constants, functions, predicates, equality, quantifiers.\n\n\u25e6 Informally: \n\n\u25e6 Predicates (applied to terms) have a truth value (i.e. true or false) \n\n\u25e6 e.g. < (less than) is a predicate so, x < 3 + 5 is either true or false\n\n\u25e6 Functions just construct new terms out of other terms \n\n\u25e6 So, a function such as + (addition) can construct a new term 3 + 5. This does not have a truth value. \n\nIncreased expressive power: sufficient to define Wumpus world.\n\nINF2D: REASONING AND AGENTS\n\n14\n\n",
        "lecture": " 15",
        "page": "13"
    },
    "15_14": {
        "content": "Unification and \nGeneralised \nModus Ponens\n\nRules for quantifiers\n\n\u25e6 Need to be able to state rules such \n\nas UI, EI etc.\n\nReducing FOL to PL\n\nUnification as equation solving\n\n\u25e6 How to apply rules to 2 \n\nexpressions produce unifier\n\n\u25e6 All steps should be clearly labelled\n\nGeneralised modus ponens (GMP)\n\nINF2D: REASONING AND AGENTS\n\n15\n\n",
        "lecture": " 15",
        "page": "14"
    },
    "15_15": {
        "content": "Inference\n\nForward chaining\n\u25e6 Informally: Unify all the assumptions in an implication rule with facts in KB to discharge them. If \n\nsuccessful, add instantiated conclusion to KB. This is similar to a discovery process.\n\nBackward chaining\n\u25e6 Informally: Unify conclusion of an implication rule with some fact in KB. If successful, add \n\ninstantiated assumptions to KB as new goals. This is similar to a decomposition into sub-problems.\n\nResolution\n\u25e6 Formal statements for various types of resolution.\n\n\u25e6 Refutation method i.e. works by looking for a contradiction i.e. tries to derive falsity (empty clause).\n\n\u25e6 Need to negate the goal before converting it to clausal form.\n\nINF2D: REASONING AND AGENTS\n\n16\n\n",
        "lecture": " 15",
        "page": "15"
    },
    "15_16": {
        "content": "Example\n\nP(G(y)) \uf0da \u00acQ(G(y))\n\n\u00acP(F(x)) \uf0da Q(x) \n\n{x/G(y)}\n\nIdentify unifiable, \ncomplementary \nliterals\n\nP(G(y)) \uf0da \u00acP(F(G(y))) \n\nResulting unifier\n\nSubstitute to get resolvant\n\nINF2D: REASONING AND AGENTS\n\n17\n\n",
        "lecture": " 15",
        "page": "16"
    },
    "15_17": {
        "content": "Situation Calculus\n\nPlanning\n\nSituations\n\u25e6 These are not states of the world, despite what their names might suggest. A situation is a \n\n(history of a) sequence of actions. \n\n\u25e6 Reiter:\n\n\u25e6 The central ontological ingredient of the sitcalc is the situation. Even at this late stage in AI, many people still don't \n\nunderstand what a situation is, so here's the secret: A situation is a finite sequence of actions. Period. It's not a state, it's \nnot a snapshot, it's a history. \n\n\u25e6 E.g. Result(Move(A, B), Result(Move(B,C),S0))   \n\nFrame problem\n\u25e6 How to formulate the various properties logically etc.\n\nINF2D: REASONING AND AGENTS\n\n18\n\n",
        "lecture": " 15",
        "page": "17"
    },
    "16a_1": {
        "content": "What is planning?\nWhy planning?\n\nWhere are we?\n\nThe first two blocks of the course dealt with . . .\n\nBasic notions of agency\nIntelligent problem-solving\nHeuristic search, constraints\nLogic & logical reasoning\nReasoning about actions and time\n\nIn the remainder of the course we will talk about . . .\n\nPlanning\nUncertainty\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 10\n\n",
        "lecture": " 16a: Introduction to Planning",
        "page": "1"
    },
    "16a_2": {
        "content": "What is planning?\nWhy planning?\n\nWhere are we?\n\nThe first two blocks of the course dealt with . . .\n\nBasic notions of agency\nIntelligent problem-solving\nHeuristic search, constraints\nLogic & logical reasoning\nReasoning about actions and time\n\nIn the remainder of the course we will talk about . . .\n\nPlanning\nUncertainty\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 10\n\n",
        "lecture": " 16a: Introduction to Planning",
        "page": "2"
    },
    "16a_3": {
        "content": "What is planning?\nWhy planning?\n\nWhat is planning?\n\nPlanning is the task of coming up with a sequence of actions\nthat will achieve a goal\nWe are only considering classical planning in which\nenvironments are\n\nfully observable (accessible),\ndeterministic,\nfinite,\nstatic (up to agents\u2019 actions),\ndiscrete (in actions, states, objects and events).\n\n(Lifting some of these assumptions will be the subject of the\n\u201cuncertainty\u201d part of the course)\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 10\n\n",
        "lecture": " 16a: Introduction to Planning",
        "page": "3"
    },
    "16a_4": {
        "content": "What is planning?\nWhy planning?\n\nProblems with Search\nProblems with Logic\n\nWhy planning?\n\nSo far we have dealt with two types of agents:\n\n1 Search-based problem-solving agents\n2 Logical planning agents\n\nDo these techniques work for solving planning problems?\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 10\n\n",
        "lecture": " 16a: Introduction to Planning",
        "page": "4"
    },
    "16a_5": {
        "content": "What is planning?\nWhy planning?\n\nProblems with Search\nProblems with Logic\n\nWhy planning?\n\nConsider a search-based problem-solving agent in a robot\nshopping world\nTask: Go to the supermarket and get milk, bananas and a\ncordless drill\nWhat would a search-based agent do?\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 10\n\n. . .Buy Tuna FishBuy ArugulaBuy MilkGo To ClassBuy a DogTalk to ParrotSit Some MoreRead A Book...Go To SupermarketGo To SleepRead A BookGo To SchoolGo To Pet StoreEtc. Etc. ...Sit in ChairStartFinish",
        "lecture": " 16a: Introduction to Planning",
        "page": "5"
    },
    "16a_6": {
        "content": "What is planning?\nWhy planning?\n\nProblems with Search\nProblems with Logic\n\nProblems with search\n\nNo goal-directedness.\nNo problem decomposition into sub-goals that build on each\nother\n\nMay undo past achievements\nMay go to the store 3 times!\n\nSimple goal test doesn\u2019t allow for the identification of\nmilestones\nHow do we find a good heuristic function?\nHow do we model the way humans perceive complex goals and\nthe quality of a plan?\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 10\n\n",
        "lecture": " 16a: Introduction to Planning",
        "page": "6"
    },
    "16a_7": {
        "content": "What is planning?\nWhy planning?\n\nProblems with Search\nProblems with Logic\n\nHow about logic & deductive inference?\n\nGenerally a good idea, allows for \u201copening up\u201d representations\nof states, actions, goals and plans\nIf Goal = Have(Bananas)\u2227 Have(Milk) this allows\nachievement of sub-goals (if independent)\nCurrent state can be described by properties in a compact way\n(e.g. Have(Drill) stands for hundreds of states)\nAllows for compact description of actions, for example\n\nObject(x) \u21d2 Can(a, Grab(x))\n\nAllows for representing a plan hierarchically,\ne.g. GoTo(Supermarket) = Leave(House)\u2227\nReachLocationOf (Supermarket)\u2227 Enter (Supermarket) then\ndecompose further into sub-plans\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 10\n\n",
        "lecture": " 16a: Introduction to Planning",
        "page": "7"
    },
    "16a_8": {
        "content": "What is planning?\nWhy planning?\n\nProblems with Search\nProblems with Logic\n\nHow about logic & deductive inference?\n\nProblems:\n\n1\n\nIn its general form either awkward (propositional logic) or\ntractability problems (first-order logic)\nIf p is a sequence that achieves the goal,\nthen so is [a, a\u22121|p]!\n(Logically independent) subgoals may need to be undone to\nachieve other goals.\nGoal: on(A, B)\u2227 on(B, C )\n\n2\n\n3\n\nInitial State\n\nGoal\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 10\n\n",
        "lecture": " 16a: Introduction to Planning",
        "page": "8"
    },
    "16a_9": {
        "content": "What is planning?\nWhy planning?\n\nProblems with Search\nProblems with Logic\n\nWhat next?\n\nSolutions: We need\n\n1 To reduce complexity to allow scaling up.\n2 To allow reasoning to be guided by plan \u2018quality\u2019/efficiency.\n\nDo 1. next, and 2. after that.\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 10\n\n",
        "lecture": " 16a: Introduction to Planning",
        "page": "9"
    },
    "16a_10": {
        "content": "What is planning?\nWhy planning?\n\nProblems with Search\nProblems with Logic\n\nSummary\n\nPlanning is the task of identifying a sequence of actions to\nachieve your goal.\nBoth search and logic are important tools, but need adapting\nto tackle:\n\nefficient search and inference\ndiscriminating the variable quality of valid plans\n\nNext time: Planning Domain Definition Language (PDDL)\nFormal representation of states, goals and plans.\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 10\n\n",
        "lecture": " 16a: Introduction to Planning",
        "page": "10"
    },
    "16b_1": {
        "content": "Representing planning problems\n\nWhere are we?\n\nLast time. . .\n\nWhy we need a formal representation of planning problems:\n\nstates, goals, actions\n\nthat supports efficient algorithms for finding a valid plan\n\nNow: Representing planning problems\n\nPlanning Domain Definition Language (PDDL)\n\nLater: The planning algorithms\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 9\n\n",
        "lecture": " 16b: Representing Planning Problems with PDDL",
        "page": "1"
    },
    "16b_2": {
        "content": "Representing planning problems\n\nPDDL\n\nRepresenting planning problems\n\nNeed a language expressive enough to cover interesting\nproblems, restrictive enough to allow efficient algorithms.\nPlanning Domain Definition Language or PDDL\nPDDL will allow you to express:\n\nstates\n\n1\n2 actions: a description of transitions between states\n3 and goals: a (partial) description of a state.\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 9\n\n",
        "lecture": " 16b: Representing Planning Problems with PDDL",
        "page": "2"
    },
    "16b_3": {
        "content": "Representing planning problems\n\nPDDL\n\nRepresenting States and Goals in PDDL\n\nStates represented as conjunctions of propositional or\nfunction-free first order positive literals:\n\nHappy \u2227 Sunshine,\nAt(Plane1, Melbourne)\u2227 At(Plane2, Sydney )\n\nSo these aren\u2019t states:\n\nAt(x, y ) (no variables allowed),\nLove(Father(Fred),Fred) (no function symbols allowed)\n\u00acHappy (no negation allowed).\n\nClosed-world assumption!\nA goal is a partial description of a state, and you can use\nnegation, variables etc. to express that description.\n\u00acHappy, At(x,SFO), Love(Father(Fred),Fred) . . .\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 9\n\n",
        "lecture": " 16b: Representing Planning Problems with PDDL",
        "page": "3"
    },
    "16b_4": {
        "content": "Representing planning problems\n\nPDDL\n\nActions in PDDL\n\nAction(Fly (p, from, to),\n\nPrecond:At(p, from)\u2227 Plane(p)\u2227 Airport(from)\u2227 Airport(to)\nE\ufb00ect:\u00acAt(p, from)\u2227 At(p, to))\n\nActually action schemata, as they may contain variables\nAction name and parameter list serves to identify the action\nPrecondition: defines states in which action is executable:\n\nConjunction of positive and negative literals, where all\nvariables must occur in action name.\n\nE\ufb00ect: defines how literals in the input state get changed\n(anything not mentioned stays the same).\n\nConjunction of positive and negative literals, with all its\nvariables also in the preconditions.\nOften positive and negative e\ufb00ects are divided into add list\nand delete list\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 9\n\n",
        "lecture": " 16b: Representing Planning Problems with PDDL",
        "page": "4"
    },
    "16b_5": {
        "content": "Representing planning problems\n\nPDDL\n\nThe semantics of PDDL: States and their Descriptions\n\ns |= At(P1,SFO) i\ufb00 At(P1,SFO) \u2208 s\ns |= \u00acAt(P1,SFO) i\ufb00 At(P1,SFO) (cid:54)\u2208 s\ns |= \u03c6 (x) i\ufb00 there is a ground term d such that s |= \u03c6 [x/d].\ns |= \u03c6 \u2227 \u03c8 i\ufb00 s |= \u03c6 and s |= \u03c8\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 9\n\n",
        "lecture": " 16b: Representing Planning Problems with PDDL",
        "page": "5"
    },
    "16b_6": {
        "content": "Representing planning problems\n\nPDDL\n\nThe Semantics of PDDL: Applicable Actions\n\nAny action is applicable in any state that satisfies the\nprecondition with an appropriate substitution for parameters.\nExample: State\n\nAt(P1, Melbourne)\u2227 At(P2, Sydney )\u2227 Plane(P1)\u2227 Plane(P2)\n\u2227 Airport(Sydney )\u2227 Airport(Melbourne)\u2227 Airport(Heathrow )\n\nsatisfies\n\nAt(p, from)\u2227 Plane(p)\u2227 Airport(from)\u2227 Airport(to)\n\nwith substitution (among others)\n\n{p/P2, from/Sydney , to/Heathrow}\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 9\n\n",
        "lecture": " 16b: Representing Planning Problems with PDDL",
        "page": "6"
    },
    "16b_7": {
        "content": "Representing planning problems\n\nPDDL\n\nThe semantics of PDDL: The Result of an Action\n\nResult of executing action a in state s is state s(cid:48) with any\npositive literal P in a\u2019s E\ufb00ects added to the state and every\nnegative literal \u00acP removed from it (under the given\nsubstitution) .\nIn our example s(cid:48) would be\n\nAt(P1, Melbourne)\u2227 At(P2, Heathrow )\u2227 Plane(P1)\u2227 Plane(P2)\n\u2227 Airport(Sydney )\u2227 Airport(Melbourne)\u2227 Airport(Heathrow )\n\n\u201cPDDL assumption\u201d: every literal not mentioned in the e\ufb00ect\nremains unchanged (cf. frame problem)\nSolution = action sequence that leads from the initial state to\na state that satisfies the goal.\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 9\n\n",
        "lecture": " 16b: Representing Planning Problems with PDDL",
        "page": "7"
    },
    "16b_8": {
        "content": "Representing planning problems\n\nPDDL\n\nSummary\n\nIntroduced PDDL:\n\nstates, goals, actions\n\nFragment of first order logic\nRestrictive enough to design efficient algorithms for solving\nplanning problems\n\nGiven the current state and goal, find a sequence of actions to\nget from the former to the latter.\n\nNext Time\n\nSome simple examples of planning problems\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 9\n\n",
        "lecture": " 16b: Representing Planning Problems with PDDL",
        "page": "8"
    },
    "16c_1": {
        "content": "Blocks world example\nSummary\n\nWhere are we?\n\nLast time . . .\n\nPlanning Domain Definition Language (PDDL)\n\nNow:\n\nSome examples\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 6\n\n",
        "lecture": " 16c: Examples in PDDL",
        "page": "1"
    },
    "16c_2": {
        "content": "Blocks world example\nSummary\n\nBlocks world example\n\nGiven: A set of cube-shaped blocks sitting on a table\nCan be stacked, but only one on top of the other\nRobot arm can move around blocks (one at a time)\nGoal: to stack blocks in a certain way\nFormalisation in PDDL:\n\nOn(b, x) to denote that block b is on x (block/table)\nMove(b, x, y ) to indicate action of moving b from x to y\nPrecondition for this action requires Clear (z): nothing stacked\non z.\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 6\n\n",
        "lecture": " 16c: Examples in PDDL",
        "page": "2"
    },
    "16c_3": {
        "content": "Blocks world example\nSummary\n\nBlocks world example\n\nAction schema:\n\nAction(Move(b, x, y ),\n\nPrecond:On(b, x)\u2227 Clear (b)\u2227 Clear (y )\nE\ufb00ect:On(b, y )\u2227 Clear (x)\u2227\u00acOn(b, x)\u2227\u00acClear (y ))\n\nProblem: when x = Table or y = Table we infer that the table\nis clear when we have moved a block from it (not true) and\nrequire that table is clear to move something on it (not true)\nSolution: introduce another action\n\nAction(MoveToTable(b, x),\n\nPrecond:On(b, x)\u2227 Clear (b)\nE\ufb00ect:On(b, Table)\u2227 Clear (x)\u2227\u00acOn(b, x))\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 6\n\n",
        "lecture": " 16c: Examples in PDDL",
        "page": "3"
    },
    "16c_4": {
        "content": "Blocks world example\nSummary\n\nDoes this Work?\n\nInterpret Clear (b) as \u201cthere is space on b to hold a block\u201d\n(thus Clear (Table) is always true)\nBut without further modification, planner can still use\nMove(b, x, Table):\n\nNeedlessly increases search space\n(not a big problem here, but can be)\n\nSo part of solution is to also add Block(b)\u2227 Block(y ) to\nprecondition of Move\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 6\n\n",
        "lecture": " 16c: Examples in PDDL",
        "page": "4"
    },
    "16c_5": {
        "content": "Blocks world example\nSummary\n\nSummary\n\nWe have now defined a language for expressing planning\nproblems\nBlocks world example as a famous application domain\nDiscussed how to address some specific problems in\nrepresenting states and actions\nNext time: Algorithms for planning!\nState-Space Search and Partial-Order Planning\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 6\n\n",
        "lecture": " 16c: Examples in PDDL",
        "page": "5"
    },
    "17a_1": {
        "content": "Planning with state-space search\n\nWhere are we?\n\nSo far . . .\n\nwe defined the planning problem\ndiscussed problem with using search and logic in planning\nintroduced representation languages for planning\nlooked at blocks world example\n\nOver next few slots . . .\n\nState-space search and partial-order planning\nNow: state-space search\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "1"
    },
    "17a_2": {
        "content": "Planning with state-space search\n\nForward state-space search\nBackward state-space search\nHeuristics for state-space search\n\nPlanning with state-space search\n\nMost straightforward way to think of planning process:\nsearch the space of states using action schemata\nSince actions are defined both in terms of preconditions and\ne\ufb00ects we can search in both directions\nTwo methods:\n\n1\n\nforward state-space search: Start in initial state; consider\naction sequences until goal state is reached.\n\n2 backward state-space search: Start from goal state; consider\n\naction sequences until initial state is reached\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "2"
    },
    "17a_3": {
        "content": "Planning with state-space search\n\nForward state-space search\nBackward state-space search\nHeuristics for state-space search\n\nPlanning with state-space search\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "3"
    },
    "17a_4": {
        "content": "Planning with state-space search\n\nForward state-space search\nBackward state-space search\nHeuristics for state-space search\n\nForward state-space search\n\nAlso called progression planning\nFormulation of planning problem:\n\nInitial state of search is initial state of planning problem\n(=set of positive literals)\nApplicable actions are those whose preconditions are satisfied\nSingle successor function works for all planning problems\n(consequence of action representation)\nGoal test = checking whether state satisfies goal of planning\nproblem\nStep cost usually 1, but di\ufb00erent costs can be allowed\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "4"
    },
    "17a_5": {
        "content": "Planning with state-space search\n\nForward state-space search\nBackward state-space search\nHeuristics for state-space search\n\nForward state-space search\n\nSearch space is finite in the absence of function symbols\nAny complete graph search algorithm (like A\u2217) will be a\ncomplete graph planning algorithm\nForward search does not solve problem of irrelevant actions (all\nactions considered from each state)\nEfficiency depends largely on quality of heuristics\nExample:\n\nAir cargo problem, 10 airports with 5 planes each, 20 pieces of\ncargo\nTask: move all 20 pieces of cargo at airport A to airport B\nEach of 50 planes can \ufb02y to 9 airports, each of 200 packages\ncan be unloaded or loaded (individually)\nSo approximately 10K executable actions in each state\n(50\u00d7 9\u00d7 200)\nLots of irrelevant actions get considered, although solution is\ntrivial!\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "5"
    },
    "17a_6": {
        "content": "Planning with state-space search\n\nForward state-space search\nBackward state-space search\nHeuristics for state-space search\n\nBackward state-space search\n\nIn normal search, backward approach hard because goal\ndescribed by a set of constraints (rather than being listed\nexplicitly)\nProblem of how to generate predecessors, but planning\nrepresentations allow us to consider only relevant actions\nExclusion of irrelevant actions decreases branching factor\nIn example, only about 20 actions working backward from goal\nRegression planning = computing the states from which\napplying a given action leads to the goal\nMust ensure that actions are consistent, i.e. they don\u2019t undo\nany desired literals\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "6"
    },
    "17a_7": {
        "content": "Planning with state-space search\n\nForward state-space search\nBackward state-space search\nHeuristics for state-space search\n\nAir cargo domain example\n\nGoal can be described as\n\nAt(C1, B)\u2227 At(C2, B)\u2227 . . . At(C20, B)\n\nTo achieve At(C1, B) there is only one action,\nUnload(C1, p, B) (p unspecified)\nCan do this action only if its preconditions are satisfied.\nSo the predecessor to the goal state must include\nIn(C1, p)\u2227 At(p, B), and should not include At(C1, B)\n(otherwise irrelevant action)\nFull predecessor:\n\nIn(C1, p)\u2227 At(p, B)\u2227 . . .\u2227 At(C20, B)\n\nLoad(C1, p) would be inconsistent (negates At(C1, B))\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "7"
    },
    "17a_8": {
        "content": "Planning with state-space search\n\nForward state-space search\nBackward state-space search\nHeuristics for state-space search\n\nBackward state-space search\n\nGeneral process of constructing predecessors for backward\nsearch given goal description G, relevant and consistent action\nA:\n\nAny positive e\ufb00ects of A that appear in G are deleted\nEach precondition of A is added unless it already appears\n\nAny standard search algorithm can be used, terminates when\npredecessor description is satisfied by initial (planing) state\nFirst-order case may require additional substitutions which\nmust be applied to actions leading from state to goal\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "8"
    },
    "17a_9": {
        "content": "Planning with state-space search\n\nForward state-space search\nBackward state-space search\nHeuristics for state-space search\n\nHeuristics for state-space search\n\nTwo possibilities:\n\n1 Divide and Conquer (subgoal decomposition)\n2 Derive a Relaxed Problem\n\nSubgoal decomposition is . . .\n\noptimistic (admissible) if negative interactions exist\n(e.g. subplan deletes goal achieved by other subplan)\npessimistic (inadmissible) if positive interactions exist\n(e.g. subplans contain redundant actions)\n\nRelaxations:\n\ndrop all preconditions (all actions always applicable, combined\nwith subgoal independence makes prediction even easier)\nremove all negative e\ufb00ects (and count minimum number of\nactions so that union satisfies goals)\nempty delete lists approach (involves running a simple planning\nproblem to compute heuristic value)\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "9"
    },
    "17a_10": {
        "content": "Planning with state-space search\n\nForward state-space search\nBackward state-space search\nHeuristics for state-space search\n\nSummary\n\nState-space search approaches (forward/backward)\nHeuristics for state-space search planning\n\nNext time. . .\n\nPartial-order planning\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 11\n\n",
        "lecture": " 17a: Forward State-Space Search in Planning",
        "page": "10"
    },
    "17b_1": {
        "content": "Partial-order planning\nSummary\n\nWhere are we?\n\nLast time. . .\n\nPlanning using state-space search (forward/backward)\n\nNow:\n\nPartial-order Planning\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "1"
    },
    "17b_2": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nPartial-order planning\n\nState-space search planning algorithms consider totally\nordered sequences of actions\nBetter not to commit ourselves to complete chronological\nordering of tasks (least commitment strategy)\nBasic idea:\n\n1 Add actions to a plan without specifying which comes first\n\nunless necessary\n\n2 Combine \u2018independent\u2019 subsequences afterwards\n\nPartial-order solution will correspond to one or several\nlinearisations of partial-order plan\nSearch in plan space rather than state spaces (because your\nsearch is over ordering constraints on actions, as well as\ntransitions among states).\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "2"
    },
    "17b_3": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nExample: Put your socks and shoes on\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "3"
    },
    "17b_4": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nPartial-order planning (POP) as a search problem\n\nDefine POP as search problem over plans consisting of:\n\nActions; initial plan contains dummy actions Start (no\npreconditions, e\ufb00ect=initial state) and Finish (no e\ufb00ects,\nprecondition=goal literals)\nOrdering constraints on actions A \u227a B (A must occur before\nB); contradictory constraints prohibited\np\u2192 B express A achieves p for\nCausal links between actions A\nB (p precondition of B, e\ufb00ect of A, must remain true between\nA and B); inserting action C with e\ufb00ect \u00acp (A \u227a C and\nC \u227a B) would lead to con\ufb02ict\nOpen preconditions: set of conditions not yet achieved by\nthe plan (planners try to make open precondition set empty\nwithout introducing contradictions)\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "4"
    },
    "17b_5": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nThe POP algorithm\n\nFinal plan for socks and shoes example (without trivial\nordering constraints):\nActions: {RightSock, RightShoe, LeftSock, LeftShoe, Start, Finish}\nOrderings: {RightSock \u227a RightShoe, LeftSock \u227a LeftShoe}\nLinks:\n\nRightSockOn\u2192 RightShoe,\n\n{RightSock\nLeftSock LeftSockOn\u2192 LeftShoe,\nRightShoeOn\u2192 Finish,\nRightShoe\nLeftShoe LeftShoeOn\u2192 Finish}\n\nOpen preconditions: {}\nConsistent plan = plan without cycles in orderings and\ncon\ufb02icts with links\nSolution = consistent plan without open preconditions\nEvery linearisation of a partial-order solution is a total-order\nsolution (implications for execution!)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "5"
    },
    "17b_6": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nThe POP algorithm\n\nInitial plan:\nActions: {Start, Finish}, Orderings: {Start \u227a Finish},\nLinks: {}, Open preconditions: Preconditions of Finish\nPick p from open preconditions on some action B, generate a\nconsistent successor plan for every A that achieves p\nEnsuring consistency:\n\n1 Add A\n\np\u2192 B and A \u227a B to plan. If A new, add A and Start \u227a A\n\nand A \u227a Finish to plan\n\n2 Resolve con\ufb02icts between the new link and all actions and\n\nbetween A (if new) and all links as follows:\nIf con\ufb02ict between A\n\np\u2192 B and C, add B \u227a C or C \u227a A\nGoal test: check whether there are open preconditions\n(only consistent plans are generated)\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "6"
    },
    "17b_7": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nPartial-order planning example (1)\n\nInit(At(Flat, Axle)\u2227 At(Spare, Trunk)). Goal(At(Spare, Axle)).\nAction(Remove(Spare, Trunk),\n\nPrecond:At(Spare, Trunk)\nE\ufb00ect:\u00acAt(Spare, Trunk)\u2227 At(Spare, Ground))\n\nAction(Remove(Flat, Axle),\n\nPrecond:At(Flat, Axle)\nE\ufb00ect:\u00acAt(Flat, Axle)\u2227 At(Flat, Ground))\n\nAction(PutOn(Spare, Axle),\n\nPrecond:At(Spare, Ground)\u2227\u00acAt(Flat, Axle)\nE\ufb00ect:\u00acAt(Spare, Ground)\u2227 At(Spare, Axle))\n\nAction(LeaveOvernight,\n\nPrecond:\n\nE\ufb00ect:\u00acAt(Spare, Ground)\u2227\u00acAt(Spare, Axle)\u2227\u00acAt(Spare, Trunk)\n\n\u2227\u00acAt(Flat, Ground)\u2227\u00acAt(Flat, Axle))\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "7"
    },
    "17b_8": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nPartial-order planning example (2)\n\nPick (only) open precondition At(Spare, Axle) of Finish\nOnly applicable action = PutOn(Spare, Axle)\nPick At(Spare, Ground) from PutOn(Spare, Axle)\nOnly applicable action = Remove(Spare, Trunk)\nSituation after two steps:\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "8"
    },
    "17b_9": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nPartial-order planning example (3)\n\nPick \u00acAt(Flat, Axle) precondition of PutOn(Spare, Axle)\nChoose LeaveOvernight, e\ufb00ect \u00acAt(Spare, Ground)\nCon\ufb02ict with link\nRemove(Spare, Trunk)\nResolve by adding LeaveOvernight \u227a Remove(Spare, Trunk)\nWhy is this the only solution?\n\nPutOn(Spare, Axle)\n\nAt(Spare,Ground)\n\n\u2192\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "9"
    },
    "17b_10": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nPartial-order planning example (4)\n\nRemaining open precondition At(Spare, Trunk), but con\ufb02ict\nbetween Start and \u00acAt(Spare, Trunk) e\ufb00ect of\nLeaveOvernight\nNo ordering before Start possible or after\nRemove(Spare, Trunk) possible\nNo successor state, backtrack to previous state and remove\nLeaveOvernight, resulting in this situation:\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "10"
    },
    "17b_11": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nPartial-order planning example (5)\n\nNow choose Remove(Flat, Axle) instead of LeaveOvernight\nNext, choose At(Spark, Trunk) precondition of\nRemove(Spare, Trunk)\nChoose Start to achieve this\nPick At(Flat, Axle) precondition of Remove(Flat, Axle),\nchoose Start to achieve it\nFinal, complete, consistent plan:\n\nAlex Lascarides\n\nInformatics 2D\n\n12 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "11"
    },
    "17b_12": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nDealing with unbound variables\n\nIn first-order case, unbound variables may occur during\nplanning process\nExample:\n\nAction(Move(b, x, y ),\n\nPrecond:On(b, x)\u2227 Clear (b)\u2227 Clear (y )\nE\ufb00ect:On(b, y )\u2227 Clear (x)\u2227\u00acOn(b, x)\u2227\u00acClear (y ))\n\nachieves On(A, B) under substitution {b/A, y /B}\nApplying this substitution yields\n\nAction(Move(A, x, B),\n\nPrecond:On(A, x)\u2227 Clear (A)\u2227 Clear (B)\nE\ufb00ect:On(A, B)\u2227 Clear (x)\u2227\u00acOn(A, x)\u2227\u00acClear (B))\n\nand x is still unbound (another side of the least commitment\napproach)\n\nAlex Lascarides\n\nInformatics 2D\n\n13 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "12"
    },
    "17b_13": {
        "content": "Partial-order planning\nSummary\n\nThe POP algorithm\nExample\nDealing with unbound variables\n\nDealing with unbound variables\n\nOn(A,B)\u2192 Finish would be added\n\nAlso has an e\ufb00ect on links, e.g. in example above\nMove(A, x, B)\nIf another action has e\ufb00ect \u00acOn(A, z) then this is only a\ncon\ufb02ict if z = B\nSolution: insert inequality constraints (in example: z (cid:54)= B)\nand check these constraints whenever applying substitutions\nRemark on heuristics: Even harder than in total-order planning,\ne.g. adapt most-constrained-variable approach from CSPs\n\nAlex Lascarides\n\nInformatics 2D\n\n14 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "13"
    },
    "17b_14": {
        "content": "Partial-order planning\nSummary\n\nSummary\n\nPartial-order planning\nThe POP algorithms\nPOP as search in planning space\nPOP example\nPOP with unbound variables\nNext time: Planning and Acting in the Real World\n\nAlex Lascarides\n\nInformatics 2D\n\n15 / 15\n\n",
        "lecture": " 17b: Partial Order Planning",
        "page": "14"
    },
    "18a_1": {
        "content": "Introduction\nPartially Observable Environments\n\nWhere are we?\n\nSo far . . .\n\nDiscussed PDDL\nAlgorithms for finding valid plans in PDDL\n\nState-space search\nPartial-order planning\n\nBut so far, confined to classical planning\n\nToday . . .\n\nPlanning and acting in the real world I\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 9\n\n",
        "lecture": " 18a: Planning and acting in the real world:",
        "page": "1"
    },
    "18a_2": {
        "content": "Introduction\nPartially Observable Environments\n\nPlanning/acting in Nondeterministic Domains\n\nSo far only looked at classical planning,\ni.e. environments are fully observable, static, deterministic\nAlso assumed that action descriptions are correct and complete\nUnrealistic in many real-world applications:\n\nDon\u2019t know everything; may even hold incorrect information\nActions can go wrong\n\nDistinction: bounded vs. unbounded indeterminacy: can\npossible preconditions and e\ufb00ects be listed at all?\nUnbounded indeterminacy related to qualification problem\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 9\n\n",
        "lecture": " 18a: Planning and acting in the real world:",
        "page": "2"
    },
    "18a_3": {
        "content": "Introduction\nPartially Observable Environments\n\nMethods for handling indeterminacy\n\nSensorless/conformant planning: achieve goal in all\npossible circumstances, relies on coercion\nContingency planning: for partially observable and\nnon-deterministic environments; includes sensing actions and\ndescribes di\ufb00erent paths for di\ufb00erent circumstances\nOnline planning and replanning: check whether plan\nrequires revision during execution and replan accordingly\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 9\n\n",
        "lecture": " 18a: Planning and acting in the real world:",
        "page": "3"
    },
    "18a_4": {
        "content": "Introduction\nPartially Observable Environments\n\nExample Problem: Paint table and chair same colour\n\nInitial State: We have two cans of paint and table and chair, but\n\ncolours of paint and of furniture is unknown:\n\nObject(Table)\u2227 Object(Chair)\u2227 Can(C1)\u2227 Can(C2)\u2227 InView(Table)\n\nGoal State: Chair and table same colour:\n\nColor(Chair, c)\u2227 Color(Table, c)\n\nActions: To look at something; to open a can; to paint.\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 9\n\n",
        "lecture": " 18a: Planning and acting in the real world:",
        "page": "4"
    },
    "18a_5": {
        "content": "Introduction\nPartially Observable Environments\n\nFormal Representation of the Three Actions\n\nNow we allow variables in preconditions that aren\u2019t part of the\nactions\u2019s variable list!\n\nAction(RemoveLid(can),\n\nPrecond:Can(can)\nE\ufb00ect:Open(can))\n\nAction(Paint(x,can),\n\nPrecond:Object(x)\u2227 Can(can)\u2227 Color(can, c)\u2227 Open(can)\nE\ufb00ect:Color(x, c))\n\nAction(LookAt(x),\n\nPrecond:InView(y )\u2227 (x (cid:54)= y )\nE\ufb00ect:InView(x)\u2227\u00acInView(y ))\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 9\n\n",
        "lecture": " 18a: Planning and acting in the real world:",
        "page": "5"
    },
    "18a_6": {
        "content": "Introduction\nPartially Observable Environments\n\nSensing with Percepts\n\nA percept schema models the agent\u2019s sensors.\nIt tells the agent what it knows, given certain conditions about\nthe state it\u2019s in.\n\nPercept(Color(x, c),\n\nPrecond:Object(x)\u2227 InView(x))\n\nPercept(Color(can, c),\n\nPrecond:Can(can)\u2227 Open(can)\u2227 InView(can))\n\nA fully observable environment has a percept axiom for each\n\ufb02uent with no preconditions!\nA sensorless planner has no percept schemata at all!\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 9\n\n",
        "lecture": " 18a: Planning and acting in the real world:",
        "page": "6"
    },
    "18a_7": {
        "content": "Introduction\nPartially Observable Environments\n\nPlanning\n\nOne could coerce the table and chair to be the same colour by\npainting them both\u2014a sensorless planner would have to do\nthis!\nBut a contingent planner can do better than this:\n1 Look at the table and chair to sense their colours.\n2\n\nIf they\u2019re the same colour, you\u2019re done.\nIf not, look at the paint cans.\nIf one of the can\u2019s is the same colour as one of the pieces of\nfurniture, then apply that paint to the other piece of furniture.\n\n3\n\n4\n\n5 Otherwise, paint both pieces with one of the cans.\n\nNext time: we\u2019ll look at these types of planning in more\ndetail. . .\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 9\n\n",
        "lecture": " 18a: Planning and acting in the real world:",
        "page": "7"
    },
    "18a_8": {
        "content": "Introduction\nPartially Observable Environments\n\nSummary\n\nPlanning must deal with:\n\nActions with non-determinate outcomes\nStates that are partially observable\n\nSome new techniques:\n\nPercepts\nPlanning strategies:\nconformant, contingent, online monitoring and re-planning\n\nNext time:\n\nSensorless and contingent planning in more detail\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 9\n\n",
        "lecture": " 18a: Planning and acting in the real world:",
        "page": "8"
    },
    "18b_1": {
        "content": "Beliefs\nSensorless Planning\n\nWhere are we?\n\nLast time. . .\n\nWe have impractical assumptions about planning\n\nactions have deterministic outcomes\nstates are fully observable\n\nthat we now need to drop.\nGeneral discussion of challenges and planning strategies to deal\nwith more realistic planning problems.\n\nNow: More formal detail.\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 9\n\n",
        "lecture": " 18b: Planning and acting in the real world:",
        "page": "1"
    },
    "18b_2": {
        "content": "Beliefs\nSensorless Planning\n\nWhat\u2019s needed?\n\nWhen sensors aren\u2019t powerful enough\n\nDon\u2019t know the value of all relevant \ufb02uents\nSo you must plan using your beliefs, not the representation of\nthe actual state.\nHow do we represent beliefs?\n\nWhen actions can have more than one outcome\n\nNeed to represent conditional e\ufb00ects in action schemata.\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 9\n\n",
        "lecture": " 18b: Planning and acting in the real world:",
        "page": "2"
    },
    "18b_3": {
        "content": "Beliefs\nSensorless Planning\n\nWhat\u2019s needed?\n\nWhen sensors aren\u2019t powerful enough\n\nDon\u2019t know the value of all relevant \ufb02uents\nSo you must plan using your beliefs, not the representation of\nthe actual state.\nHow do we represent beliefs?\n\nWhen actions can have more than one outcome\n\nNeed to represent conditional e\ufb00ects in action schemata.\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 9\n\n",
        "lecture": " 18b: Planning and acting in the real world:",
        "page": "3"
    },
    "18b_4": {
        "content": "Beliefs\nSensorless Planning\n\nHow to represent belief states\n\n1. Sets of state representations, e.g.\n\n{(AtL\u2227 CleanR \u2227 CleanL), (AtL\u2227 CleanL)}\n\n(2n states!)\n\n2. Logical sentences can capture a belief state, e.g. AtL\u2227 CleanL\n\nshows ignorance about CleanR by not mentioning it!\nThis often o\ufb00ers a more compact representation, but\nMany equivalent sentences; need canonical representation to\navoid general theorem proving; E.g:\n\nAll representations are ordered conjunctions of literals (under\nopen-world assumption)\nBut this doesn\u2019t capture everything (e.g. AtL\u2228 CleanR)\n\n3. Knowledge propositions, e.g. K (AtR)\u2227 K (CleanR)\n\n(closed-world assumption)\nWill use second method, but clearly loss of expressiveness\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 9\n\n",
        "lecture": " 18b: Planning and acting in the real world:",
        "page": "4"
    },
    "18b_5": {
        "content": "Beliefs\nSensorless Planning\n\nBeliefs and Sensorless Planning\n\nWhen you have no sensors, you need:\n\nto represent and track your (changing) beliefs as you perform\nactions . . .\n. . . and so cope with sensorless planning\n\nExample\nTable and chair, two cans of paint\nyou know these objects exist, but you can\u2019t see them\nYou can open cans, and paint furniture\nGoal: table and chair to be same colour\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 9\n\n",
        "lecture": " 18b: Planning and acting in the real world:",
        "page": "5"
    },
    "18b_6": {
        "content": "Beliefs\nSensorless Planning\n\nSensorless Planning Example: The Belief States\n\nThere are no InView \ufb02uents, because there are no sensors!\nThere are unchanging facts:\nObject(Table)\u2227 Object(Chair)\u2227 Can(C1)\u2227 Can(C2)\nAnd we know that the objects and cans have colours:\n\u2200x\u2203cColor(x, c)\nAfter skolemisation this gives an initial belief state:\n\nb0 = Color(x, C (x))\n\nA belief state corresponds exactly to the set of possible worlds\nthat satisfy the formula\u2014open world assumption.\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 9\n\n",
        "lecture": " 18b: Planning and acting in the real world:",
        "page": "6"
    },
    "18b_7": {
        "content": "Beliefs\nSensorless Planning\n\n[RemoveLid(C1),Paint(Chair, C1),Paint(Table, C1)]\n\nThe Plan\n\nRules:\n\nYou can only apply actions whose preconditions are satisfied by\nyour current belief state b.\nThe update of a belief state b given an action a is the set of\nall states that result (in the physical transition model) from\ndoing a in each possible state s that satisfies belief state b:\n\nb(cid:48) = Result(b, a) = {s(cid:48) : s(cid:48) = ResultP (s, a)\u2227 s \u2208 b}\n\nOr, when a belief b is expressed as a formula:\n\nIf action adds l, l becomes a conjunct of the formula b(cid:48) (and\nthe conjunct \u00acl removed, if necessary); so b(cid:48) |= l\nIf action deletes l, \u00acl becomes a conjunct of b(cid:48) (and l\nremoved).\nIf action says nothing about l, it retains its b-value.\n\n1\n\n2\n\n3\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 9\n\n",
        "lecture": " 18b: Planning and acting in the real world:",
        "page": "7"
    },
    "18b_8": {
        "content": "Beliefs\nSensorless Planning\n\nShowing the Plan Works\n\nb0 = Color(x, C (x))\nb1 = Result(b0,RemoveLid(C1))\n= Color(x, C (x))\u2227 Open(C1)\nb2 = Result(b1,Paint(Chair, C1))\n\n(binding {x/C1, c/C (C1)} satisfies Precond)\n\n= Color(x, C (x))\u2227 Open(C1)\u2227 Color(Chair, C (C1))\nb3 = Result(b2,Paint(Table, C1))\n= Color(x, C (x))\u2227 Open(C1)\u2227\n\nColor(Chair, C (C1))\u2227 Color(Table, C (C1))\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 9\n\n",
        "lecture": " 18b: Planning and acting in the real world:",
        "page": "8"
    },
    "18b_9": {
        "content": "Beliefs\nSensorless Planning\n\nSummary\n\nWhen your sensors aren\u2019t powerful enough to fully observe the\ncurrent state, you need to reason about your beliefs\nVarious ways of representing beliefs\nExamined how you can keep track of beliefs as you act, and so\ncope with sensorless planning.\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 9\n\n",
        "lecture": " 18b: Planning and acting in the real world:",
        "page": "9"
    },
    "18c_1": {
        "content": "Contingent Planning\nSummary\n\nWhere are we?\n\nLast time. . .\n\nRepresenting beliefs (to handle partially observable states)\nSensorless planning\n\nToday. . .\n\nRepresenting actions with nondeterminate outcomes\nContingent planning\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "1"
    },
    "18c_2": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nConditional E\ufb00ects\n\nSo far, we have only considered actions that have the same\ne\ufb00ects on all states where the preconditions are satisfied.\nThis means that any initial belief state that is a conjunction is\nupdated by the actions to a belief state that is also a\nconjunction.\nBut some actions are best expressed with conditional e\ufb00ects.\nThis is especially true if the e\ufb00ects are non-deterministic, but\nin a bounded way.\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "2"
    },
    "18c_3": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nExtending action representations\n\nDisjunctive e\ufb00ects: Action(Left,Precond:AtR,E\ufb00ect:AtL\u2228 AtR)\nConditional e\ufb00ects:\nAction(Vacuum,\n\nPrecond:\nE\ufb00ect:(when AtL : CleanL)\u2227 (when AtR : CleanR))\n\nCombination:\n\nAction(Left,\n\nPrecond:AtR\nE\ufb00ect:AtL\u2228 (AtL\u2227 (when CleanL : \u00acCleanL)))\n\nConditional steps: if AtL\u2227 CleanL then Right else Vacuum\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "3"
    },
    "18c_4": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nThe earlier painting furniture example (Lecture 18a)\n\nPlanning Problem\nTable and chair, two cans of paint,\ncan open can, paint furniture with paint inside\nGoal: table and chair the same colour\n\nContingent Plan\n\n1 Look at the table and chair to sense their colours.\n2\n\nIf they\u2019re the same colour, you\u2019re done.\nIf not, look at the paint cans.\nIf one of the can\u2019s is the same colour as one of the pieces of\nfurniture, then apply that paint to the other piece of furniture.\n\n3\n\n4\n\n5 Otherwise, paint both pieces with one of the cans.\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "4"
    },
    "18c_5": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nThe Three Actions (from Lecture 18a)\n\nAction(RemoveLid(can),\n\nPrecond:Can(can)\nE\ufb00ect:Open(can))\n\nAction(Paint(x,can),\n\nPrecond:Object(x)\u2227 Can(can)\u2227 Color(can, c)\u2227 Open(can)\nE\ufb00ect:Color(x, c))\n\nAction(LookAt(x),\n\nPrecond:InView(y )\u2227 (x (cid:54)= y )\nE\ufb00ect:InView(x)\u2227\u00acInView(y ))\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "5"
    },
    "18c_6": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nPercepts (from Lecture 18a)\n\nPercept(Color(x, c),\n\nPrecond:Object(x)\u2227 InView(x))\n\nPercept(Color(can, c),\n\nPrecond:Can(can)\u2227 Open(can)\u2227 InView(can))\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "6"
    },
    "18c_7": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nFormal Representation of the Contingent Plan\n\n[LookAt(Table),LookAt(Chair)\n\nif Color(Table, c)\u2227 Color(Chair, c) then NoOp\n\nelse [RemoveLid(C1),LookAt(C1),RemoveLid(C2),LookAt(C2),\nif Color(Chair, c)\u2227 Color(can, c) then Paint(Table, can)\nelse if Color(Table, c)\u2227 Color(can, c) then Paint(Chair, can)\nelse [Paint(Chair, C1),Paint(Table, C1)]]]\n\nVariables (e.g., c) are existentially quantified.\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "7"
    },
    "18c_8": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nGames against nature\n\nConditional plans should succeed regardless of circumstances\nNesting conditional steps results in trees\nSimilar to adversarial search, games against nature\nGame tree has state nodes and chance nodes where nature\ndetermines the outcome\nDefinition of solution: A subtree with\n\na goal node at every leaf\nspecifies one action at each state node\nincludes every outcome at chance node\n\nAND-OR graphs can be used in similar way to the minimax\nalgorithm (basic idea: find a plan for every possible result of a\nselected action)\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "8"
    },
    "18c_9": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nExample: \u201cdouble Murphy\u201d vacuum cleaner\n\nThis wicked vacuum cleaner sometimes deposits dirt when\nmoving to a clean destination or when vacuuming in a clean\nsquare\nSolution: [Left,if CleanL;then [] else Vacuum]\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 15\n\nLeftVacuumRightVacuumLeftVacuumGOALGOALLOOPLOOP8 3 6 8 7 1 5 7 8 4 2 ",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "9"
    },
    "18c_10": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nAcyclic vs. cyclic solutions\n\nIf identical state is encountered (on same path), terminate\nwith failure (if there is an acyclic solution it can be reached\nfrom previous incarnation of state)\nHowever, sometimes all solutions are cyclic!\nE.g., \u201ctriple Murphy\u201d (also) sometimes fails to move.\nPlan [Left,if CleanL then [] else Vacuum] doesn\u2019t work\nanymore\nCyclic plan:\n[L : Left,if AtR then L elseif CleanL then [] else Vacuum]\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 15\n\n8 LeftVacuum6 3 7 GOAL",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "10"
    },
    "18c_11": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nNondeterminism and partially observable environments\n\n\u201calternate double Murphy\u201d:\n\nVacuum cleaner can sense cleanliness of square it\u2019s in, but not\nthe other square, and\ndirt can sometimes be left behind when leaving a clean square.\nPlan in fully observable world: \u201cKeep moving left and right,\nvacuuming up dirt whenever it appears, until both squares are\nclean and in the left square\u201d\nBut now goal test cannot be performed!\n\nAlex Lascarides\n\nInformatics 2D\n\n12 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "11"
    },
    "18c_12": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nHousework in partially observable worlds\n\nAlex Lascarides\n\nInformatics 2D\n\n13 / 15\n\nLeftCleanLCleanLVacuumVacuumRightCleanRCleanR8 4 A7 5 B3 1 C6 2 D\u00ac(cid:13)\u00ac(cid:13)",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "12"
    },
    "18c_13": {
        "content": "Contingent Planning\nSummary\n\nExtending Representations to handle nondeterministic outcomes\nSearch with Nondeterministic Actions\nAnd with Partially observable environments\n\nConditional planning, partial observability\n\nBasically, we can apply our AND-OR-search to belief states\n(rather than world states)\nFull observability is special case of partial observability with\nsingleton belief states\nIs it really that easy?\nNot quite, need to describe\n\nrepresentation of belief states\nhow sensing works\nrepresentation of action descriptions\n\nAlex Lascarides\n\nInformatics 2D\n\n14 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "13"
    },
    "18c_14": {
        "content": "Contingent Planning\nSummary\n\nSummary\n\nMethods for planning and acting in the real world\nDealing with indeterminacy\nContingent planning: use percepts and conditionals to cater\nfor all contingencies.\nFully observable environments: AND-OR graphs, games\nagainst nature\nPartially observable environments: belief states, action and\nsensing\nNext time: Monitoring, re-planning, and Hierarchical\nTask Networks\n\nAlex Lascarides\n\nInformatics 2D\n\n15 / 15\n\n",
        "lecture": " 18c: Planning and Acting in the Real World:",
        "page": "14"
    },
    "19a_1": {
        "content": "Execution monitoring and replanning\nSummary\n\nWhere are we?\n\nLast time . . .\n\nFully and partially observable environments\nActions with non-deterministic outcomes\nSensorless planning and contingent planning\n\nToday . . .\n\nMonitoring and replanning\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 6\n\n",
        "lecture": " 19a: Planning and acting in the real world:",
        "page": "1"
    },
    "19a_2": {
        "content": "Execution monitoring and replanning\nSummary\n\nExecution monitoring and replanning\n\nExecution monitoring = checking whether things are going\naccording to plan (necessitated by unbounded indeterminacy in\nrealistic environments)\n\nAction monitoring = checking whether next action is feasible\nPlan monitoring = checking whether remainder of plan is\nfeasible\n\nReplanning = ability to find new plan when things go wrong\n(usually repairing the old plan)\nTaken together these methods yield powerful planning abilities\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 6\n\n",
        "lecture": " 19a: Planning and acting in the real world:",
        "page": "2"
    },
    "19a_3": {
        "content": "Execution monitoring and replanning\nSummary\n\nAction monitoring and replanning\n\nWhile attempting to get from S to G, a problem is\nencountered in E, agent discovers actual state is O and plans\nto get to P and execute the rest of the original plan\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 6\n\n",
        "lecture": " 19a: Planning and acting in the real world:",
        "page": "3"
    },
    "19a_4": {
        "content": "Execution monitoring and replanning\nSummary\n\nPlan monitoring\n\nAction monitoring often results in suboptimal behaviour,\nexecutes everything until actual failure\nPlan monitoring checks preconditions for entire remaining\nplan\nCan also take advantage of serendipity (unexpected\ncircumstances might make remaining plan easier)\nIn partially observable environments things are more complex\n(sensing actions have to be planned for, they can fail in turn,\netc.)\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 6\n\n",
        "lecture": " 19a: Planning and acting in the real world:",
        "page": "4"
    },
    "19a_5": {
        "content": "Execution monitoring and replanning\nSummary\n\nSummary\n\nUnbounded non-determinacy requires:\n\nExecution monitoring: checking sucess of execution\nReplanning: repairing plans in case of failure\n\nNext time: Hierarchical Planning\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 6\n\n",
        "lecture": " 19a: Planning and acting in the real world:",
        "page": "5"
    },
    "19b_1": {
        "content": "Hierarchical Planning\nSummary\n\nWhere are we?\n\nActions are often non-deterministic\n\nBounded non-determinism: when clauses; contingent planning\nUnbounded non-determinism: execution monitoring and\nre-planning\n\nNow: Hierarchical Planning\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 8\n\n",
        "lecture": " 19b: Planning and acting in the real world",
        "page": "1"
    },
    "19b_2": {
        "content": "Hierarchical Planning\nSummary\n\nRepresenting action refinements\n\nHierarchical decomposition in planning\n\nHierarchical decomposition seems a natural idea to improve\nplanning capabilities.\nKey idea: at each level of the hierarchy, activity involves only\nsmall number of steps (i.e. small computational cost)\nHierarchical task network (HTN) planning: initial plan\nprovides only high-level description, refined by action\nrefinements\nRefinement process continued until plan consists only of\nprimitive actions\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\n",
        "lecture": " 19b: Planning and acting in the real world",
        "page": "2"
    },
    "19b_3": {
        "content": "Hierarchical Planning\nSummary\n\nRepresenting action refinements\n\nRepresenting action decompositions\n\nEach high level action (HLA) has (at least) one refinement\ninto a sequence of actions.\nThe actions in the sequence may be HLAs or primitive.\n\nSo HLAs form a hierarchy!\n\nIf they\u2019re all primitive, then that\u2019s an implementation of the\nHLA.\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 8\n\n",
        "lecture": " 19b: Planning and acting in the real world",
        "page": "3"
    },
    "19b_4": {
        "content": "Hierarchical Planning\nSummary\n\nRepresenting action refinements\n\nExample: Go to SF Airport\n\nRefinment(Go(Home,SFO),\n\nPrecond:At(Car,Home)\nSteps:[Drive(Home,SFOLongTermParking)\n\nShuttle(SFOLongTermParking,SFO)])\n\nRefinment(Go(Home,SFO),\nPrecond:Cash,At(Home)\nSteps:[Taxi(Home,SFO)])\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 8\n\n",
        "lecture": " 19b: Planning and acting in the real world",
        "page": "4"
    },
    "19b_5": {
        "content": "Hierarchical Planning\nSummary\n\nRepresenting action refinements\n\nRefinements can be Recursive\n\nRefinment(Navigate([a, b], [x, y ]),\n\nPrecond:a = x, b = y\nSteps:[])\n\nRefinment(Navigate([a, b], [x, y ]),\n\nPrecond:Connected([a, b], [a\u2212 1, b])\nSteps:[Left,Navigate([a\u2212 1, b], [x, y ])])\n\nRefinment(Navigate([a, b], [x, y ]),\n\nPrecond:Connected([a, b], [a + 1, b])\nSteps:[Right,Navigate([a + 1, b], [x, y ])])\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 8\n\n",
        "lecture": " 19b: Planning and acting in the real world",
        "page": "5"
    },
    "19b_6": {
        "content": "Hierarchical Planning\nSummary\n\nRepresenting action refinements\n\nHigh-Level Plans\n\nHigh-Level Plans (HLP) are a sequence of HLAs.\nAn implementation of a High Level Plan is the concatenation\nof an implementation of each of its HLAs.\nA HLP achieves the goal from an initial state if at least one of\nits implementations does this.\nNot all implementations of an HLP have to reach the goal\nstate!\nThe agent gets to decide which implementation of which HLAs\nto execute.\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 19b: Planning and acting in the real world",
        "page": "6"
    },
    "19b_7": {
        "content": "Hierarchical Planning\nSummary\n\nSummary\n\nNatural to think of actions at di\ufb00erent levels of granularity.\nCan achieve this through refinements of high-level actions\n(HLAs)\nDi\ufb00erent refinments of an HLA can have di\ufb00ering\npreconditions and e\ufb00ects.\nA refinement that consists entirely of primitive or executable\nactions is an implementation of the HLA.\nYou can choose which implementation of an HLA to execute.\nNext time: searching for valid HLP\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 8\n\n",
        "lecture": " 19b: Planning and acting in the real world",
        "page": "7"
    },
    "19c_1": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nWhere are we?\n\nHierarchical Plans\n\nYou can represent high level actions (HLAs) using alternative\nrefinements\nA valid high level plan (HLP) is a sequence of HLAs for which\nthere is an implementation that achieves the goal.\nNow: searching for a valid HLP\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "1"
    },
    "19c_2": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nSearching for Primitive Solutions\n\nThe HLA plan library is a hierarchy:\n\n(Ordered) Daughters to an HLA are the sequences of actions\nprovided by one of its refinements;\nBecause a given HLA can have more than one refinement,\nthere can be more than one node for a given HLA in the\nhierarchy.\n\nThis hierarchy is essentially a search space of action sequences\nthat conform to knowledge about how high-level actions can\nbe broken down.\nSo you can search this space for a plan!\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "2"
    },
    "19c_3": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nSearching for Primitive Solutions: Breadth First\n\nStart your plan P with the HLA [Act],\nTake the first HLA A in P (recall that P is an action\nsequence).\nDo a breadth-first search in your hierarchical plan library, to\nfind a refinement of A whose preconditions are satisfied by the\noutcome of the action in P that is prior to A.\nReplace A in P with this refinement.\nKeep going until your plan P has no HLAs and either:\n\n1 Your plan P\u2019s outcome is the goal, in which case return P; or\n2 Your plan P\u2019s outcome is not the goal, in which case\n\nbacktrack,\nand if nowhere to backtrack then return failure.\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "3"
    },
    "19c_4": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nProblems!\n\nLike forward search, you consider lots of irrelevant actions.\nThe algorithm essentially refines HLAs right down to primitive\nactions so as to determine if a plan will succeed.\nThis contradicts common sense!\nSometimes you know an HLA will work regardless of how it\u2019s\nbroken down!\nWe don\u2019t need to know which route to take to SFOParking to\nknow this plan works:\n\n[Drive(Home,SFOParking),Shuttle(SFOParking , SFO)]\n\nWe can capture this if we add to HLAs themselves a set of\npreconditions and e\ufb00ects.\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "4"
    },
    "19c_5": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nAdding Preconditions and E\ufb00ects to HLAs\n\nOne challenge in specifying preconditions and e\ufb00ects of an\nHLA is that the HLA may have more than one refinement,\neach one with slightly di\ufb00erent preconditions and e\ufb00ects!\n\nIf you refine Go(Home,SFO) with Taxi action: you need Cash.\nIf you refine it with Drive, you don\u2019t!\nThis di\ufb00erence may a\ufb00ect your choice on how to refine the\nHLA!\n\nRecall that an HLA achieves a goal if one of its refinements\ndoes this.\nAnd you can choose the refinement!\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "5"
    },
    "19c_6": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nGetting Formal\n\ns(cid:48) \u2208 Reach(s, h) i\ufb00 s(cid:48) is reachable from at least one of HLA h\u2019s\nrefinements, given (initial) state s.\n\n(cid:91)\n\nReach(s, [h1, h2]) =\n\nReach(s(cid:48), h2)\n\ns(cid:48)\u2208Reach(s,h1)\n\nHLP p achieves goal g given initial state s i\ufb00 \u2203s(cid:48) st\n\ns(cid:48) |= g and s(cid:48) \u2208 Reach(s, p)\n\nSo we should search HLPs to find a p with this relation to g,\nand then focus on refining it.\nBut a pre-requisite to this algorithm is to define Reach(s, h)\nfor each h and s.\nIn other words, we still need to determine how to represent\ne\ufb00ects (and preconditions) of HLAs. . .\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "6"
    },
    "19c_7": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nDefining Reach\n\nA primitive action makes a \ufb02uent true, false, or leaves it\nunchanged.\nBut with HLAs you sometimes get to choose, by choosing a\nparticular refinement!\nWe add new notation to re\ufb02ect this:\n\nyou can possibly add A (or leave A unchanged)\nyou can possibly delete A (or leave A unchanged)\nyou can possibly add A, or\npossibly delete A (or leave A unchanged)\n\n(cid:101)+A:\n(cid:101)\u2212A:\n(cid:101)+A:\n\nYou should now derive the correct preconditions and e\ufb00ects\nfrom its refinements!\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "7"
    },
    "19c_8": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nOur SFO Example\n\nRefinment(Go(Home,SFO),\n\nPrecond:At(Car,Home)\nSteps:[Drive(Home,SFOLongTermParking)\n\nShuttle(SFOLongTermParking,SFO)])\n\nRefinment(Go(Home,SFO),\nPrecond:Cash,At(Home)\nSteps:[Taxi(Home,SFO)])\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "8"
    },
    "19c_9": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nThe \u2018Primitive\u2019 Actions\n\nAction(Taxi(a, b),\n\nPrecond:Cash,At(Taxi, a)\nE\ufb00ect:\u00acCash,\u00acAt(Taxi, a),At(Taxi, b))\n\nAction(Drive(a, b),\n\nPrecond:At(Car, a)\nE\ufb00ect:\u00acAt(Car, a),At(Car, b))\n\nAction(Shuttle(a, b),\n\nPrecond:At(Shuttle, a)\nE\ufb00ect:\u00acAt(Shuttle, a),At(Shuttle, b))\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "9"
    },
    "19c_10": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nDeriving the Preconds and E\ufb00ects of the HLA\n\n\u00acCash is E\ufb00ect of one HLA refinement, but not the other.\n\nSo (cid:101)\u00acCash in HLA E\ufb00ect!\n\nNot so Simple!\n\nSimilar argument for At(Car,SFOParking)\nBut you can\u2019t choose the combination:\n\u00acCash\u2227 At(Car,SFOParking)\nSolution is to write approximate descriptions.\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "10"
    },
    "19c_11": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nApproximate Descriptions\n\nOptimistic Description: Reach+(s, h)\n\nTake union of all possible outcomes from all refinements.\n\nSo this includes (cid:101)\u00acCash and (cid:101)+At(Car,SFOParking).\n\nThis overgenerates reachable states.\n\nPessimistic Description: Reach\u2212(s, h)\n\nOnly states that satisfy e\ufb00ects from all refinements survive.\n\nSo this does not include (cid:101)\u00acCash or (cid:101)+At(Car,SFOParking).\n\nThis undergenerates reachable states.\n\nReach\u2212(s, h) \u2286 Reach(s, h) \u2286 Reach+(s, h)\n\nAlex Lascarides\n\nInformatics 2D\n\n12 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "11"
    },
    "19c_12": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nAlgorithm for Finding a Plan\n\nTwo Important Facts:\n\nIf \u2203s(cid:48) \u2208 Reach\u2212(s, h) st s(cid:48) |= g, you know h can succeed.\nIf \u00ac\u2203s(cid:48) \u2208 Reach+(s, h) st s(cid:48) |= g, you know h will fail!\n\n1\n\n2\n\nThe Algorithm:\n\nDo breadth first search as before.\nBut now you can stop searching and implement instead when\nyou reach an h where 1. is true.\nAnd you can drop h (and all its refinements) when 2. is true.\nIf 1. and 2. are both false for the current h, then you don\u2019t\nknow if h will succeed or fail, but you can find out by refining\nit.\n\nAlex Lascarides\n\nInformatics 2D\n\n13 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "12"
    },
    "19c_13": {
        "content": "Primitive Search\nMore Advanced Search\nSummary\n\nSummary\n\nHLAs and HLPs\nUsing refinements and preconditions and e\ufb00ects of primitive\nactions to approximate which states are reachable.\nSuch approximate descriptions of HLAs help to inform search\nand when to refine an HLP so as to reach a goal.\nNext time: Acting under Uncertainty\n\nAlex Lascarides\n\nInformatics 2D\n\n14 / 14\n\n",
        "lecture": " 19c: Planning and acting in the real world:",
        "page": "13"
    },
    "20a_1": {
        "content": "Acting under uncertainty\nSummary\n\nWhere are we?\n\nLast time . . .\n\nPrevious part of course discussed planning as an efficient way\nof determining actions that will achieve goals\nUsed more elaborate representations than in search, but\navoided full complexity of logical reasoning\nAllowed uncertainty to some extent (e.g. conditional planning,\nreplanning)\nHowever the approaches seen so far don\u2019t allow for a\nquantification of uncertainty\n\nToday . . .\n\nActing under uncertainty\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 7\n\n",
        "lecture": " 20a: Quantifying Uncertainty",
        "page": "1"
    },
    "20a_2": {
        "content": "Acting under uncertainty\nSummary\n\nHandling uncertain knowledge\n\nHandling uncertain knowledge\n\nSo far we have always assumed that propositions are assumed\nto be true, false, or unknown\nBut in reality, we have hunches rather than complete ignorance\nor absolute knowledge\nApproaches like conditional planning and replanning handle\nthings that might go wrong\nBut they don\u2019t tell us how likely it is that something might go\nwrong. . .\nAnd rational decisions (i.e. \u2018the right thing to do\u2019) depend\non the relative importance of various goals and the likelihood\nthat (and degree to which) they will be achieved\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 7\n\n",
        "lecture": " 20a: Quantifying Uncertainty",
        "page": "2"
    },
    "20a_3": {
        "content": "Acting under uncertainty\nSummary\n\nHandling uncertain knowledge\n\nHandling uncertain knowledge\n\nTo develop theories of uncertain reasoning we must look at the\nnature of uncertain knowledge\nExample: rules for dental diagnosis\n\nA rule like \u2200p Symptom(p, Toothache) \u21d2 Disease(p, Cavity ) is\nclearly wrong\nDisjunctive conclusions require long lists of potential diagnoses:\n\n\u2200p Symptom(p, Toothache) \u21d2\n\nDisease(p, Cavity )\u2228Disease(p, GumDisease)\u2228Disease(p, Abscess). . .\n\nCausal rules like\n\u2200p Disease(p, Cavity ) \u21d2 Symptom(p, Toothache) can also\ncause problems\nEven if we know all possible causes, what if the cavity and the\ntoothache are not connected?\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 7\n\n",
        "lecture": " 20a: Quantifying Uncertainty",
        "page": "3"
    },
    "20a_4": {
        "content": "Acting under uncertainty\nSummary\n\nHandling uncertain knowledge\n\nUncertain knowledge, logic, and probabilities\n\nClearly, using (classical) logic is not very useful to capture\nuncertainty, because of . . .\n\ncomplexity (can be impractical to include all antecedents and\nconsequents in rules, and/or too hard to use them)\ntheoretical ignorance (don\u2019t know a rule completely)\npractical ignorance (don\u2019t know the current state)\nHow likely an unknown factor is in\ufb02uences how we reason and\nact\n\nOne possible approach: express degrees of belief in\npropositions using probability theory\n\nProbability can summarise the uncertainty that comes\nfrom our \u2018laziness\u2019 and ignorance\n\nProbabilities between 0 and 1 express the degree to which we\nbelieve a proposition to be true\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 7\n\n",
        "lecture": " 20a: Quantifying Uncertainty",
        "page": "4"
    },
    "20a_5": {
        "content": "Acting under uncertainty\nSummary\n\nHandling uncertain knowledge\n\nDegrees of belief and probabilities\n\nIn probability theory, propositions themselves are actually true\nor false!\nDegrees of truth are the subject of other methods (like fuzzy\nlogic) not dealt with here\nDegrees of belief depend on evidence and should change with\nnew evidence\nDon\u2019t confuse this with change in the world that might make\nthe proposition itself true or false!\nBefore evidence is obtained we speak of prior/unconditional\nprobability, after evidence of posterior probability\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 7\n\n",
        "lecture": " 20a: Quantifying Uncertainty",
        "page": "5"
    },
    "20a_6": {
        "content": "Acting under uncertainty\nSummary\n\nSummary\n\nWe are rarely completely uncertain about things we don\u2019t\nknow.\nWe can use probabilities to express our confidence in whether\na proposition is true, or false.\nQuantifying uncertainty is critical for intelligent decision\nmaking, because it contributes to quantifying risk.\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 7\n\n",
        "lecture": " 20a: Quantifying Uncertainty",
        "page": "6"
    },
    "20b_1": {
        "content": "Uncertainty and rational decisions\nDecision Theory\nSummary\n\nWhere are we?\n\nLast time. . .\n\nUse probabilities to represent uncertainty in our beliefs about\n\ncurrent state, future outcomes of our actions\nignorance about cause and e\ufb00ect\n\nUseful for managing risk\nNow: uncertainty and rational decisions\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 8\n\n",
        "lecture": " 20b: Rational Decisions",
        "page": "1"
    },
    "20b_2": {
        "content": "Uncertainty and rational decisions\nDecision Theory\nSummary\n\nUncertainty and rational decisions\n\nLogical agent has a goal and executes any plan guaranteed to\nachieve it\nDi\ufb00erent with degrees of belief: If plan P has a 90% chance of\nsuccess, how about another P(cid:48) with a higher probability? Or\nhow about P(cid:48)(cid:48) with higher cost but same probability?\nAgent must have preferences over outcomes of plans\nUtility theory can be used to reason about those preferences\nBased on idea that every state has a degree of usefulness and\nagents prefer states with higher utility\nUtilities vary from one agent to another.\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\n",
        "lecture": " 20b: Rational Decisions",
        "page": "2"
    },
    "20b_3": {
        "content": "Uncertainty and rational decisions\nDecision Theory\nSummary\n\nDesign for a decision-theoretic agent\n\nDecision theory\n\nA general theory of rational decision making\nDecision theory = probability theory + utility theory\nFoundation of decision theory:\n\nAn agent is rational if and only if it chooses the action\nthat yields the highest expected utility, averaged over all\npossible outcomes of the action\n\nPrinciple of Maximum Expected Utility\nAlthough we follow it here, some points of criticism:\n\nKnowledge of preferences?\nConsistency of preferences?\nRisk-taking attitude?\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 8\n\n",
        "lecture": " 20b: Rational Decisions",
        "page": "3"
    },
    "20b_4": {
        "content": "Uncertainty and rational decisions\nDecision Theory\nSummary\n\nDesign for a decision-theoretic agent\n\nAre We Rational?\n\nA: 100% chance of \u00a33000\nB: 80% chance of \u00a34000\n\nC: 25% chance of \u00a33000\nD: 20% chance of \u00a34000\n\n84% of you chose lottery A over lottery B.\n68% of you chose lottery D over lottery C.\nSo lots of you chose A and D, which is irrational!\n\nIf U(3000) > 0.8\u2217 U(4000), then\n0.25\u2217 U(3000) > 0.2\u2217 U(4000)!!\n\nOur ability to MEU also a\ufb00ected by emotion, social\nrelationships, relationships among our choices. . .\nIn fact, we\u2019re predictably irrational.\nIf we were always rational, we wouldn\u2019t have self-help, life\ncoaches etc.\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 8\n\n",
        "lecture": " 20b: Rational Decisions",
        "page": "4"
    },
    "20b_5": {
        "content": "Uncertainty and rational decisions\nDecision Theory\nSummary\n\nDesign for a decision-theoretic agent\n\nUtility of money (empirical study)\n\nFor most people concave curve (a), showing that going into\ndebt is considered disastrous relative to small gains in\nmoney\u2014risk averse.\n\nBut if you\u2019re already $10M in debt, your utility curve is more\nlike (b)\u2014risk seeking when desperate!\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 8\n\n",
        "lecture": " 20b: Rational Decisions",
        "page": "5"
    },
    "20b_6": {
        "content": "Uncertainty and rational decisions\nDecision Theory\nSummary\n\nDesign for a decision-theoretic agent\n\nDesign for a decision-theoretic agent\n\nFor the time being, we will focus on probability and not utility.\nBut still useful to have an idea of general abstract design for a\ndecision-theoretic (utility-based) agent\nCharacterised by basic perception-action loop as follows:\n1 Update belief state based on previous action and percept\n2 Calculate outcome probabilities for actions given action\n\ndescriptions and belief states\n\n3 Select action with highest expected utility given probabilities of\n\noutcomes and utility information\n\nVery simple but broadly accepted as a general principle for\nbuilding agents able to cope with real-world environments\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 20b: Rational Decisions",
        "page": "6"
    },
    "20b_7": {
        "content": "Uncertainty and rational decisions\nDecision Theory\nSummary\n\nSummary\n\nProbabilities represent degrees of belief\nTogether with utility theory, we can model rational\ndecisions:\n\nChoose an action that maximises expected utility\n\nan optimal trade o\ufb00 between what you prefer and what you\nbelieve you can achieve\nHumans sometimes behave in a predictably irrational way, so\nrational agents will sometimes deviate from typical human\nbehaviour.\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 8\n\n",
        "lecture": " 20b: Rational Decisions",
        "page": "7"
    },
    "20c_1": {
        "content": "Basic probability notation\nThe axioms of probability\nSummary\n\nWhere are we?\n\nExplained why logic in itself is insufficient to model uncertainty\nDiscussed principles of decision making under uncertainty\n\nDecision theory, MEU principle\n\nProbability theory provides useful tools for quantifying degree\nof belief/uncertainty in propositions\nNow: basic probablity notation and axioms of probability\ntheory\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 9\n\n",
        "lecture": " 20c: Probability: Notation and Axioms",
        "page": "1"
    },
    "20c_2": {
        "content": "Basic probability notation\nThe axioms of probability\nSummary\n\nPropositions & atomic events\nConditional probability\n\nPropositions & atomic events\n\nDegrees of belief concern propositions\nBasic notion: random variable, a part of the world whose\nstatus is unknown, with a domain (e.g. Cavity with domain\n(cid:104)true, false(cid:105))\nCan be boolean, discrete or continuous\nCan compose complex propositions from statements about\nrandom variables (e.g. Cavity = true \u2227 Toothache = false)\nAtomic event = complete specification of the state of the\nworld\n\nAtomic events are mutually exclusive\nTheir set is exhaustive\nEvery event entails truth or falsehood of any proposition (like\nmodels in logic)\nEvery proposition logically equivalent to the disjunction of all\natomic events that entail it\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 9\n\n",
        "lecture": " 20c: Probability: Notation and Axioms",
        "page": "2"
    },
    "20c_3": {
        "content": "Basic probability notation\nThe axioms of probability\nSummary\n\nPropositions & atomic events\nConditional probability\n\nPropositions & atomic events\n\nUnconditional/prior probability = degree of belief in a\nproposition a in the absence of any other information\nCan be between 0 and 1, write as P(Cavity = true) = 0.1 or\nP(cavity ) = 0.1\nProbability distribution = the probabilities of all values of a\nrandom variable\nWrite P(Weather ) = (cid:104)0.7,0.2,0.1(cid:105) for\n\nP(Weather = sunny ) = 0.7\nP(Weather = rain) = 0.2\nP(Weather = cloudy ) = 0.1\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 9\n\n",
        "lecture": " 20c: Probability: Notation and Axioms",
        "page": "3"
    },
    "20c_4": {
        "content": "Basic probability notation\nThe axioms of probability\nSummary\n\nPropositions & atomic events\nConditional probability\n\nProbability distributions/conditional probabilities\n\nFor a mixture of several variables, we obtain a joint\nprobability distribution (JPD) \u2013 cross-product of individual\ndistributions\nA JPD (\u201cjoint\u201d) describes one\u2019s uncertainty about the world as\nit specifies the probability of every atomic event\nFor continuous variables we use probability density function\n(we cannot enumerate values)\nWill talk about these in detail later\nConditional probability P(a|b) = the probability of a given\nthat all we know is b\nExample: P(cavity|toothache) = 0.8 means that if patient is\nobserved to have toothache, then there is an 80% chance that\nhe has a cavity\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 9\n\n",
        "lecture": " 20c: Probability: Notation and Axioms",
        "page": "4"
    },
    "20c_5": {
        "content": "Basic probability notation\nThe axioms of probability\nSummary\n\nPropositions & atomic events\nConditional probability\n\nConditional probabilities\n\nP(b)\n\nCan be defined using unconditional probabilities:\nP(a|b) = P(a\u2227b)\nOften written as product rule P(a\u2227 b) = P(a|b)P(b)\nGood for describing JPDs (which then become \u201cCPDs\u201d) as\nP(X , Y ) = P(X|Y )P(Y )\nSet of equations, not matrix multiplication (!):\n\nP(X = x1 \u2227 Y = y1) = P(X = x1|Y = y1)P(Y = y1)\nP(X = x1 \u2227 Y = y2) = P(X = x1|Y = y2)P(Y = y2)\n\n...\n\nP(X = xn \u2227 Y = ym) = P(X = xn|Y = ym)P(Y = ym)\nConditional probability does not mean logical implication!\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 9\n\n",
        "lecture": " 20c: Probability: Notation and Axioms",
        "page": "5"
    },
    "20c_6": {
        "content": "Basic probability notation\nThe axioms of probability\nSummary\n\nThe axioms of probability\n\nKolmogorov\u2019s axioms define basic semantics for probabilities:\n1. 0 \u2264 P(a) \u2264 1 for any proposition a\n2. P(true) = 1 and P(false) = 0\n3. P(a\u2228 b) = P(a) + P(b)\u2212 P(a\u2227 b)\nFrom this, a number of useful facts can be derived, e.g:\n\nP(\u00aca) = 1\u2212 P(a)\nFor variable D with domain (cid:104)d1, . . . , dn(cid:105), \u2211n\nAnd so any JPD over finite variables sums to 1\nIf e(a) is the set of atomic events that entail a, then (because\nthey are mutually exclusive) it holds that\n\ni =1 P(D = di ) = 1\n\nP(a) = \u2211\nei\u2208e(a)\n\nP(ei )\n\nWith this, we can calculate the probability of any proposition\nfrom a JPD\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 9\n\n",
        "lecture": " 20c: Probability: Notation and Axioms",
        "page": "6"
    },
    "20c_7": {
        "content": "Basic probability notation\nThe axioms of probability\nSummary\n\nExample deriviation: P(\u00aca) = 1\u2212 P(a)\n\nP(\u00aca\u2228 a) = P(true)\n\n= 1\n= P(\u00aca) + P(a)\u2212 P(\u00aca\u2227 a)\n= P(\u00aca) + P(a)\u2212 P(false)\n= P(\u00aca) + P(a)\u2212 0\n= P(\u00aca) + P(a)\n\nP(\u00aca) = 1\u2212 P(a)\n\nlogic\naxiom1\naxiom3\nlogic\naxiom1\narithmetic\narithmetic\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 9\n\n",
        "lecture": " 20c: Probability: Notation and Axioms",
        "page": "7"
    },
    "20c_8": {
        "content": "Basic probability notation\nThe axioms of probability\nSummary\n\nSummary\n\nProbability theory provides useful tools for quantifying degree\nof belief/uncertainty in propositions\nAtomic events, propositions, random variables\nProbability distributions, conditional probabilities\nAxioms of probability\nNext time: Introduction to Coursework 2\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 9\n\n",
        "lecture": " 20c: Probability: Notation and Axioms",
        "page": "8"
    },
    "22a_1": {
        "content": "Introduction\nInference with JPDs\nSummary\n\nWhere are we?\n\nSo far . . .\n\nIntroduced basics of decision theory\n(probability theory + utility)\nTalked about random variables, probability distributions\nIntroduced basic probability notation and axioms\n\nToday . . .\n\nProbabilities and Bayes\u2019 Rule\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 8\n\n",
        "lecture": " 22a: Joint Probability Distributions (JPDs)",
        "page": "1"
    },
    "22a_2": {
        "content": "Introduction\nInference with JPDs\nSummary\n\nInference with joint probability distributions\n\nLast time we talked about joint probability distributions\n(JPDs) but didn\u2019t present a method for probabilistic\ninference using them\nProblem: Given some observed evidence and a query\nproposition, how can we compute the posterior probability of\nthat proposition?\nWe will first discuss a simple method using a JPD as\n\u201cknowledge base\u201d\nAlthough not very useful in practice, it helps us to discuss\ninteresting issues along the way\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\n",
        "lecture": " 22a: Joint Probability Distributions (JPDs)",
        "page": "2"
    },
    "22a_3": {
        "content": "Introduction\nInference with JPDs\nSummary\n\nExample\n\nDomain consisting only of Boolean variables Toothache,\nCavity and Catch (steel probe catches in tooth)\nConsider the following JPD:\n\ntoothache\n\ncatch \u00accatch\n0.012\n0.108\n0.016\n0.064\n\ncavity\n\u00accavity\n\n\u00actoothache\ncatch \u00accatch\n0.008\n0.072\n0.144\n0.576\n\nProbabilities (table entries) sum to 1\nWe can compute probability of any proposition, e.g.\nP(catch\u2228 cavity ) =\n0.108 + 0.016 + 0.072 + 0.144 + 0.012 + 0.008 = 0.36\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 8\n\n",
        "lecture": " 22a: Joint Probability Distributions (JPDs)",
        "page": "3"
    },
    "22a_4": {
        "content": "Introduction\nInference with JPDs\nSummary\n\nMarginalisation, conditioning & normalisation\n\nExtracting distribution of subset of variables is called\nmarginalisation: P(Y) = \u2211z P(Y,z)\nExample:\nP(cavity ) = P(cavity , toothache, catch) + P(cavity , toothache,\u00accatch)\n\n+ P(cavity ,\u00actoothache, catch) + P(cavity ,\u00actoothache,\u00accatch)\n= 0.108 + 0.012 + 0.072 + 0.008 = 0.2\n\nConditioning \u2013 variant using the product rule:\n\nP(Y) = \u2211\n\nz\n\nP(Y|z)P(z)\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 8\n\n",
        "lecture": " 22a: Joint Probability Distributions (JPDs)",
        "page": "4"
    },
    "22a_5": {
        "content": "Introduction\nInference with JPDs\nSummary\n\nMarginalisation, conditioning & normalisation\n\nComputing conditional probabilities:\n\nP(cavity|toothache) =\n\nP(cavity \u2227 toothache)\n\nP(toothache)\n\n0.108 + 0.012\n\n=\n\n0.108 + 0.012 + 0.016 + 0.064 = 0.6\nNormalisation ensures probabilities sum to 1, normalisation\nconstants often denoted by \u03b1\nExample:\n\nP(Cavity|toothache) = \u03b1P(Cavity , toothache)\n= \u03b1[P(Cavity , toothache, catch)+P(Cavity , toothache,\u00accatch)]\n= \u03b1[(cid:104)0.108,0.016(cid:105)+(cid:104)0.012,0.064(cid:105)] = \u03b1(cid:104)0.12,0.08(cid:105) =(cid:104)0.6,0.4(cid:105)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 8\n\n",
        "lecture": " 22a: Joint Probability Distributions (JPDs)",
        "page": "5"
    },
    "22a_6": {
        "content": "Introduction\nInference with JPDs\nSummary\n\nA general inference procedure\n\nLet X be a query variable (e.g. Cavity), E set of evidence\nvariables (e.g. {Toothache}) and e their observed values, Y\nremaining unobserved variables\nQuery evaluation: P(X|e) = \u03b1P(X ,e) = \u03b1 \u2211y P(X ,e,y)\nNote that X , E, and Y constitute complete set of variables,\ni.e. P(x,e,y) simply a subset of probabilities from the JPD\nFor every value xi of X , sum over all values of every variable in\nY and normalise the resulting probability vector\nOnly theoretically relevant, it requires O(2n) steps (and\nentries) for n Boolean variables\nBasically, all methods we will talk about deal with tackling this\nproblem!\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 22a: Joint Probability Distributions (JPDs)",
        "page": "6"
    },
    "22a_7": {
        "content": "Introduction\nInference with JPDs\nSummary\n\nSummary\n\nYou can use a JPD to answer any query\n\nMarginalisation\nNormalisation\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 8\n\n",
        "lecture": " 22a: Joint Probability Distributions (JPDs)",
        "page": "7"
    },
    "22b_1": {
        "content": "Independence\nBayes\u2019 rule\nSummary\n\nWhere are we?\n\nLast time. . .\n\nUsed JPDs to answer queries\nToday: Independence and Bayes\u2019 Rule\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 7\n\n",
        "lecture": " 22b: Independence and Bayes\u2019 Rule",
        "page": "1"
    },
    "22b_2": {
        "content": "Independence\nBayes\u2019 rule\nSummary\n\nIndependence\n\nSuppose we extend our example with the variable Weather\nWhat is the relationship between old and new JPD?\nCan compute P(toothache, catch, cavity , Weather = cloudy )\nas:\nP(Weather = cloudy|toothache, catch, cavity )P(toothache, catch, cavity )\nAnd since the weather does not depend on dental stu\ufb00, we\nexpect that\nP(Weather = cloudy|toothache, catch, cavity ) = P(Weather = cloudy )\nSo\n\nP(toothache, catch, cavity , Weather = cloudy ) =\n\nP(Weather = cloudy )P(toothache, catch, cavity )\n\nOne 8-element and one 4-element table rather than a 32-table!\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 7\n\n",
        "lecture": " 22b: Independence and Bayes\u2019 Rule",
        "page": "2"
    },
    "22b_3": {
        "content": "Independence\nBayes\u2019 rule\nSummary\n\nIndependence\n\nThis is called independence, usually written as\nP(X|Y ) = P(X ) or P(Y|X ) = P(Y ) or P(X , Y ) = P(X )P(Y )\nDepends on domain knowledge; can factor distributions\n\nSuch independence assumptions can help to dramatically\nreduce complexity\nIndependence assumptions are sometimes necessary even when\nnot entirely justified, so as to make probabilistic reasoning in\nthe domain practical (more later).\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 7\n\n",
        "lecture": " 22b: Independence and Bayes\u2019 Rule",
        "page": "3"
    },
    "22b_4": {
        "content": "Independence\nBayes\u2019 rule\nSummary\n\nApplying Bayes\u2019 rule\n\nBayes\u2019 rule\n\nBayes\u2019 rule is derived by writing the product rule in two forms\nand equating them:\n\n(cid:27)\n\nP(a\u2227 b) = P(a|b)P(b)\nP(a\u2227 b) = P(b|a)P(a)\n\n\u21d2 P(b|a) =\n\nP(a|b)P(b)\n\nP(a)\n\nGeneral case for multivaried variables using background\nevidence e:\n\nP(Y|X ,e) =\n\nP(X|Y ,e)P(Y|e)\n\nP(X|e)\n\nUseful because often we have good estimates for three terms\non the right and are interested in the fourth\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 7\n\n",
        "lecture": " 22b: Independence and Bayes\u2019 Rule",
        "page": "4"
    },
    "22b_5": {
        "content": "Independence\nBayes\u2019 rule\nSummary\n\nApplying Bayes\u2019 rule\n\nApplying Bayes\u2019 rule\n\nExample: meningitis causes sti\ufb00 neck with 50%, probability of\nmeningitis (m) 1/50000, probability of sti\ufb00 neck (s) 1/20\n\nP(m|s) =\n\nP(s|m)P(m)\n\nP(s)\n\n=\n\n1\n\n2 \u00d7 1\n\n50000\n1\n20\n\n=\n\n1\n\n5000\n\nPreviously, we were able to avoid calculating probability of\nevidence (P(s)) by using normalisation\nWith Bayes\u2019 rule: P(M|s) = \u03b1(cid:104)P(s|m)P(m), P(s|\u00acm)P(\u00acm)(cid:105)\nUsefulness of this depends on whether P(s|\u00acm) is easier to\ncalculate than P(s)\nObvious question: why would conditional probability be\navailable in one direction and not in the other?\nDiagnostic knowledge (from symptoms to causes) is often\nfragile\n(e.g. P(m|s) will go up if P(m) goes up due to epidemic)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 7\n\n",
        "lecture": " 22b: Independence and Bayes\u2019 Rule",
        "page": "5"
    },
    "22b_6": {
        "content": "Independence\nBayes\u2019 rule\nSummary\n\nSummary\n\nIndependence is critical for making probabilistic reasoning\ntractable.\n\nSometimes you must assume independence that\u2019s false to\nachieve practical reasoning\n\nBayes Rule critical for optimising use of available data for\nanswering your queries.\nNext time: Combining evidence to answer queries\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 7\n\n",
        "lecture": " 22b: Independence and Bayes\u2019 Rule",
        "page": "6"
    },
    "23a_1": {
        "content": "Introduction\nRepresenting knowledge in an uncertain domain\nSummary\n\nWhere are we?\n\nLast time . . .\n\nUsing JPD tables for probabilistic inference\nConcepts of absolute and conditional independence\nBayes\u2019 rule\n\nToday . . .\n\nIntroduction to Bayesian Networks\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 9\n\n",
        "lecture": " 23a: Introduction to Bayesian Networks",
        "page": "1"
    },
    "23a_2": {
        "content": "Introduction\nRepresenting knowledge in an uncertain domain\nSummary\n\nRepresenting knowledge in an uncertain domain\n\nFull joint probability distributions can become intractably large\nvery quickly\nConditional independence helps to reduce the number of\nprobabilities required to specify the JPD\nNow we will introduce Bayesian networks (BNs) to\nsystematically describe dependencies between random variables\nRoughly speaking, BNs are graphs that connect nodes\nrepresenting variables with each other whenever they depend\non each other\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 9\n\n",
        "lecture": " 23a: Introduction to Bayesian Networks",
        "page": "2"
    },
    "23a_3": {
        "content": "Introduction\nRepresenting knowledge in an uncertain domain\nSummary\n\nBayesian networks\n\nA BN is a directed acyclic graph (DAG) with nodes annotated\nwith probability information\nThe nodes represent random variables (discrete/continuous)\nLinks connect nodes. If there is an arrow from X to Y , we call\nX a parent of Y\nEach node Xi has a conditional probability distribution (CPD)\nattached to it\nThe CPD describes how Xi depends on its parents, i.e. its\nentries describe P(Xi|Parents(Xi ))\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 9\n\n",
        "lecture": " 23a: Introduction to Bayesian Networks",
        "page": "3"
    },
    "23a_4": {
        "content": "Introduction\nRepresenting knowledge in an uncertain domain\nSummary\n\nBayesian networks\n\nTopology of graphs describes conditional independence\nrelationships\nIntuitively, links describe direct e\ufb00ects of variables on each\nother in the domain\nAssumption: anything that is not directly connected does not\ndirectly depend on each other\nIn previous dentist/weather example:\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 9\n\n",
        "lecture": " 23a: Introduction to Bayesian Networks",
        "page": "4"
    },
    "23a_5": {
        "content": "Introduction\nRepresenting knowledge in an uncertain domain\nSummary\n\nArcs and Independence\n\nEach variable is conditionally independent of its non-descendants,\ngiven its parents.\n\nIf X (cid:54)\u2208 Parents\u2217(Y ), then\n\nP(X|Parents(X ), Y ) = P(X|Parents(X ))\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 9\n\n",
        "lecture": " 23a: Introduction to Bayesian Networks",
        "page": "5"
    },
    "23a_6": {
        "content": "Introduction\nRepresenting knowledge in an uncertain domain\nSummary\n\nExample\n\nNew burglar alarm has been fitted, fairly reliable but\nsometimes reacts to earthquakes\nNeighbours John and Mary promise to call when they hear\nalarm\nJohn sometimes mistakes phone for alarm, and Mary listens to\nloud music and sometimes doesn\u2019t hear alarm\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 9\n\nBTTFFETFTFP(A).95.29.001.001P(B).002P(E)AlarmEarthquakeMaryCallsJohnCallsBurglaryAP(J)TF.90.05AP(M)TF.70.01.94",
        "lecture": " 23a: Introduction to Bayesian Networks",
        "page": "6"
    },
    "23a_7": {
        "content": "Introduction\nRepresenting knowledge in an uncertain domain\nSummary\n\nExample \u2013 things to note\n\nNo perception of earthquake by John or Mary\nNo explicit modelling of phone ring confusing John, or of\nMary\u2019s loud music\n(summarised in uncertainty regarding their reaction)\nActually this uncertainty summarises any kind of failure\n\nalmost impossible to enumerate all possible causes,\nand we don\u2019t have estimates for their probabilities anyway\n\nEach row in CPTs contains a conditioning case,\none row for each possible combination of values of the parents.\nWe often omit P(\u00acxi|Parents(Xi )) from CPT for node Xi\n(computes as 1\u2212 P(xi|Parents(Xi )))\nP(J|M, A, B, E ) = P(J|A) and P(M|J, A, B, E ) = P(M|A)\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 9\n\n",
        "lecture": " 23a: Introduction to Bayesian Networks",
        "page": "7"
    },
    "23a_8": {
        "content": "Introduction\nRepresenting knowledge in an uncertain domain\nSummary\n\nSummary\n\nBNs consist of two components:\n\n1 A graphical that captures conditional independence among\n2 A CPT for each RV: P(X|Parents(X ))\n\nRVs (more later)\n\nProbabilities/uncertainty in a BN can be due to:\n\n1 Your choice not to model certain factors\n2 Genuine ignorance about what factors are relevant\n\nNext time: BNs are a compact representation of JPDs\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 9\n\n",
        "lecture": " 23a: Introduction to Bayesian Networks",
        "page": "8"
    },
    "23b_1": {
        "content": "The semantics of Bayesian Networks\nSummary\n\nWhere are we?\n\nLast time. . .\n\nAn introduction to Bayesian Networks:\n\n1 Graph of random variables: arcs showing dependencies (more\n2 P(X|Parents(X )) for each random variable X\n\ntoday)\n\nToday: Semantics of Bayesian Networks\nCompact representation of JPDs\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 8\n\n",
        "lecture": " 23b: The Semantics of Bayesian Networks",
        "page": "1"
    },
    "23b_2": {
        "content": "The semantics of Bayesian Networks\nSummary\n\nReminder of the example BN\n\nNew burglar alarm has been fitted, fairly reliable but\nsometimes reacts to earthquakes\nNeighbours John and Mary promise to call when they hear\nalarm\nJohn sometimes mistakes phone for alarm, and Mary listens to\nloud music and sometimes doesn\u2019t hear alarm\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\nBTTFFETFTFP(A).95.29.001.001P(B).002P(E)AlarmEarthquakeMaryCallsJohnCallsBurglaryAP(J)TF.90.05AP(M)TF.70.01.94",
        "lecture": " 23b: The Semantics of Bayesian Networks",
        "page": "2"
    },
    "23b_3": {
        "content": "The semantics of Bayesian Networks\nSummary\n\nRepresenting a full JPD\nConstructing Bayesian networks\nConditional independence relations in BNs\n\nThe semantics of Bayesian Networks\n\nTwo views:\n\nBN as representation of JPD (useful for constructing BNs)\nBN as collection of conditional independence statements\n(useful for designing inference procedures)\nEvery entry P(X1 = x1 \u2227 . . .\u2227 Xn = xn) in the JPD can be\ncalculated from a BN (abbreviate by P(x1, . . . , xn))\nP(x1, . . . , xn) = \u220fn\nExample:\n\ni=1 P(xi|parents(Xi ))\n\nP(j\u2227m\u2227 a\u2227\u00acb\u2227\u00ace)\n\n= P(j|a)P(m|a)P(a|\u00acb\u2227\u00ace)P(\u00acb)P(\u00ace)\n= 0.9\u00d7 0.7\u00d7 0.001\u00d7 0.999\u00d7 0.998 = 0.00062\n\nAs before, this can be used to answer any query\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 8\n\n",
        "lecture": " 23b: The Semantics of Bayesian Networks",
        "page": "3"
    },
    "23b_4": {
        "content": "The semantics of Bayesian Networks\nSummary\n\nRepresenting a full JPD\nConstructing Bayesian networks\nConditional independence relations in BNs\n\nA method for constructing BNs\n\nRecall product rule for n variables:\n\nP(x1, . . . , xn) = P(xn|xn\u22121, . . . , x1)P(xn\u22121, . . . , x1)\n\nRepeated application of this yields the so-called chain rule:\nP(x1, . . . , xn) = P(xn|xn\u22121, . . . , x1)P(xn\u22121|xn\u22122, . . . , x1)\u00b7\u00b7\u00b7 P(x2|x1)P(x1)\n\n=\n\nn\u220f\n\ni =1\n\nP(xi|xi\u22121, . . . , x1)\n\nWith this we obtain P(Xi|Xi\u22121, . . . , X1) = P(Xi|Parents(Xi ))\nas long as Parents(Xi ) \u2286 {Xi\u22121, . . . , X1} (this can be ensured\nby labelling nodes appropriately)\nFor example, it is reasonable to assume that\n\nP(MaryCalls|JohnCalls, Alarm, Earthquake, Burglary ) = P(MaryCalls|Alarm)\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 8\n\n",
        "lecture": " 23b: The Semantics of Bayesian Networks",
        "page": "4"
    },
    "23b_5": {
        "content": "The semantics of Bayesian Networks\nSummary\n\nRepresenting a full JPD\nConstructing Bayesian networks\nConditional independence relations in BNs\n\nCompactness and node ordering\n\nBNs examples of locally structured (sparse) systems:\nsubcomponents only interact with small number of other\ncomponents\nE.g. if 30 nodes and every node depends on 5 nodes, BN will\nhave 30\u00d7 25 = 960 probabilities stored in the CPDs, while\nJPD would have 230 \u2248 10003 entries\nBut remember that this is based on designer\u2019s independence\nassumptions!\nAlso not trivial to determine good BN structure:\n\nAdd \u201croot causes\u201d first, then variables they in\ufb02uence, and so\non, until we reach \u201cleaves\u201d which have no in\ufb02uence on other\nvariables\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 8\n\n",
        "lecture": " 23b: The Semantics of Bayesian Networks",
        "page": "5"
    },
    "23b_6": {
        "content": "The semantics of Bayesian Networks\nSummary\n\nRepresenting a full JPD\nConstructing Bayesian networks\nConditional independence relations in BNs\n\nConditional independence relations in BNs\n\nHave provided \u201cnumerical\u201d semantics, but can also look at\n(equivalent) \u201ctopological\u201d semantics, namely:\n\n1. A node is conditionally independent of its non-descendants,\n\ngiven its parents\n\n2. A node is conditionally independent of all other nodes, given\n\nits parents, children and children\u2019s parents, i.e. its Markov\nblanket\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 23b: The Semantics of Bayesian Networks",
        "page": "6"
    },
    "23b_7": {
        "content": "The semantics of Bayesian Networks\nSummary\n\nSummary\n\nBNs are a compact representation of JPDs\nThey capture, and so enable us to exploit, conditional\nindependence among the random variables\nNext time: More on CPTs in BNs\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 8\n\n",
        "lecture": " 23b: The Semantics of Bayesian Networks",
        "page": "7"
    },
    "23c_1": {
        "content": "Efficient representations of CPDs\nSummary\n\nWhere are we?\n\nBNs are:\n\n1 A DAG that captures conditional independence\n2 CPTs for each variable: P(X|Parents(X ))\n\nBNs are a compact representation of JPDs, so can answer any\nquery with a BN (more next time)\nToday: Efficient representation of CPTs in BNs\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 8\n\n",
        "lecture": " 23c: E\ufb03cient Representations of CPTs in BNs",
        "page": "1"
    },
    "23c_2": {
        "content": "Efficient representations of CPDs\nSummary\n\nNoisy-OR relationships\nBNs with continuous variables\n\nEfficient representation of conditional distributions\n\nEven the 2k (k parents) conditioning cases that have to be\nprovided require a great deal of experience and knowledge of\nthe domain\nArbitrary relationships are unlikely, often describable by\ncanonical distributions that fit some standard pattern\nBy specifying pattern by a few parameters we can save a lot of\nspace!\nSimplest case: deterministic node that can be directly\ninferred from values of parents\nFor example, logical or mathematical functions\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\n",
        "lecture": " 23c: E\ufb03cient Representations of CPTs in BNs",
        "page": "2"
    },
    "23c_3": {
        "content": "Efficient representations of CPDs\nSummary\n\nNoisy-OR relationships\nBNs with continuous variables\n\nNoisy-OR relationships\n\nGeneralisation of logical OR\n\nAny cause can make e\ufb00ect true, but won\u2019t necessarily (e\ufb00ect\ninhibited; P(e\ufb00ect|cause) < 1)\nAssumes all causes are listed (leak node can be used to cater\nfor \u201cmiscellaneous\u201d unlisted causes)\nAlso assumes inhibitions are mutually conditionally\nindependent\n\nWhatever inhibits C1 from making E true is independent of\nwhat inhibits C2 from making E true.\n\nSo E is false only if each of its true parents are inhibited and\nwe can compute this likelihood from product of probabilities\nfor each individual cause inhibiting E.\nHow does this help?\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 8\n\n",
        "lecture": " 23c: E\ufb03cient Representations of CPTs in BNs",
        "page": "3"
    },
    "23c_4": {
        "content": "Efficient representations of CPDs\nSummary\n\nNoisy-OR relationships\nBNs with continuous variables\n\nExample of Noisy-OR\n\nFever is caused by Cold, Flu or Malaria and that\u2019s all (!!)\nInhibitions of Cold, Flu and Malaira are mutually conditionally\nindependent\nLikelihood that Cold is inhibited from causing Fever is\nP(\u00acfever|cold,\u00ac\ufb02u,\u00acmalaria)\n(similarly for other causes)\nIndividual inhibition probabilities:\n\nP(\u00acfever|cold,\u00ac\ufb02u,\u00acmalaria) = 0.6\nP(\u00acfever|\u00accold, \ufb02u,\u00acmalaria) = 0.2\nP(\u00acfever|\u00accold,\u00ac\ufb02u, malaria) = 0.1\n\nInhibitions mutually independent, so:\n\nP(\u00acfever|cold, \ufb02u,\u00acmalaria) =\n\nP(\u00acfever|cold,\u00ac\ufb02u,\u00acmalaria)P(\u00acfever|\u00accold, \ufb02u,\u00acmalaria)\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 8\n\n",
        "lecture": " 23c: E\ufb03cient Representations of CPTs in BNs",
        "page": "4"
    },
    "23c_5": {
        "content": "Efficient representations of CPDs\nSummary\n\nNoisy-OR relationships\nBNs with continuous variables\n\nNoisy-OR relationships\n\nWe can construct entire CPT from this information\n\nCold\n\nF\nF\nF\nF\nT\nT\nT\nT\n\nFlu Malaria P(Fever ) P(\u00acFever )\nF\nF\nT\nT\nF\nF\nT\nT\n\n1.0\n0.1\n0.2\n0.02=0.2 \u00d70.1\n0.6\n0.06=0.6 \u00d70.1\n0.12=0.6 \u00d70.2\n0.012 = 0.6 \u00d7 0.2 \u00d7 0.1\n\nF\nT\nF\nT\nF\nT\nF\nT\n\n0.0\n0.9\n0.8\n0.98\n0.4\n0.94\n0.88\n0.988\n\nEncodes CPT with k instead of 2k values!\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 8\n\n",
        "lecture": " 23c: E\ufb03cient Representations of CPTs in BNs",
        "page": "5"
    },
    "23c_6": {
        "content": "Efficient representations of CPDs\nSummary\n\nNoisy-OR relationships\nBNs with continuous variables\n\nBNs with continuous variables\n\nOften variables range over continuous domains\nDiscretisation one possible solution but often leads to\ninaccuracy or requires a lot of discrete values\nOther solution: use of standard families of probability\ndistributions specified in terms of a few parameters\nExample: normal/Gaussian distribution N(\u00b5,\u03c3 2)(x) defined in\nterms of mean \u00b5 and variance \u03c3 2 (needs just two parameters)\nHybrid Bayesian Networks use mixture of discrete and\ncontinuous variables (special methods to deal with links\nbetween di\ufb00erent types \u2013 not discussed here)\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 23c: E\ufb03cient Representations of CPTs in BNs",
        "page": "6"
    },
    "23c_7": {
        "content": "Efficient representations of CPDs\nSummary\n\nSummary\n\nIntroduced Bayesian Networks as a structured way of reasoning\nunder uncertainty using probabilities and independence\nDefined their semantics in terms of JPD representation, and\nconditional independence statements\nGave numerical and topological interpretation of semantics\nTalked about issues of efficient representation of CPTs\nDiscussed continuous variables and hybrid networks\nNext time: Exact Inference in Bayesian Networks\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 8\n\n",
        "lecture": " 23c: E\ufb03cient Representations of CPTs in BNs",
        "page": "7"
    },
    "24a_1": {
        "content": "Introduction\nInference by enumeration\nThe variable enumeration algorithm\nSummary\n\nWhere are we?\n\nLast time . . .\n\nIntroduced Bayesian networks\nA compact representation of JPDs\nMethods for efficient representations of CPTs\nBut how hard is inference in BNs?\n\nToday . . .\n\nInference in Bayesian networks\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 8\n\n",
        "lecture": " 24a: Exact Inference in Bayesian Networks",
        "page": "1"
    },
    "24a_2": {
        "content": "Introduction\nInference by enumeration\nThe variable enumeration algorithm\nSummary\n\nInference in BNs\n\nBasic task: compute posterior distribution for set of query\nvariables given some observed event (i.e. assignment of\nvalues to evidence variables)\nFormally: determine P(X|e) given query variables X, evidence\nvariables E (and non-evidence or hidden variables Y)\nExample: P(Burglary|JohnCalls = true, MaryCalls = true) =\n(cid:104)0.284,0.716(cid:105)\nFirst we will discuss exact algorithms for computing posterior\nprobabilities then approximate methods later\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\n",
        "lecture": " 24a: Exact Inference in Bayesian Networks",
        "page": "2"
    },
    "24a_3": {
        "content": "Introduction\nInference by enumeration\nThe variable enumeration algorithm\nSummary\n\nInference by enumeration\n\nWe have seen that any conditional probability can be\ncomputed from a full JPD by summing terms\nP(X|e) = \u03b1P(X ,e) = \u03b1 \u2211y P(X ,e,y)\nSince BN gives complete representation of full JPD, we must\nbe able to answer a query by computing sums of products of\nconditional probabilities from the BN\nConsider query\nP(Burglary|JohnCalls = true, MaryCalls = true) = P(B|j, m)\nP(B|j, m) = \u03b1P(B, j, m) = \u03b1 \u2211e \u2211a P(B, e, a, j, m)\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 8\n\n",
        "lecture": " 24a: Exact Inference in Bayesian Networks",
        "page": "3"
    },
    "24a_4": {
        "content": "Introduction\nInference by enumeration\nThe variable enumeration algorithm\nSummary\n\nInference by enumeration\n\ni=1 P(xi|parents(Xi ))\n\nRecall P(x1, . . . , xn) = \u220fn\nWe can use CPTs to simplify this exploiting BN structure\nFor Burglary = true:\nP(b|j, m) = \u03b1 \u2211\n\nP(b)P(e)P(a|b, e)P(j|a)P(m|a)\n\n\u2211\n\ne\n\na\n\nBut we can improve efficiency of this by moving terms outside\nthat don\u2019t depend on sums\nP(b|j, m) = \u03b1P(b)\u2211\n\nP(a|b, e)P(j|a)P(m|a)\n\nP(e)\u2211\n\ne\n\na\n\nTo compute this, we need to loop through variables in order\nand multiply CPT entries; for each summation we need to loop\nover variable\u2019s possible values\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 8\n\n",
        "lecture": " 24a: Exact Inference in Bayesian Networks",
        "page": "4"
    },
    "24a_5": {
        "content": "Introduction\nInference by enumeration\nThe variable enumeration algorithm\nSummary\n\nExample\n\nNew burglar alarm has been fitted, fairly reliable but\nsometimes reacts to earthquakes\nNeighbours John and Mary promise to call when they hear\nalarm\nJohn sometimes mistakes phone for alarm, and Mary listens to\nloud music and sometimes doesn\u2019t hear alarm\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 8\n\nBTTFFETFTFP(A).95.29.001.001P(B).002P(E)AlarmEarthquakeMaryCallsJohnCallsBurglaryAP(J)TF.90.05AP(M)TF.70.01.94",
        "lecture": " 24a: Exact Inference in Bayesian Networks",
        "page": "5"
    },
    "24a_6": {
        "content": "Introduction\nInference by enumeration\nThe variable enumeration algorithm\nSummary\n\nThe variable enumeration algorithm\n\nEnumeration method is computationally quite hard.\nYou often compute the same thing several times;\ne.g. P(j|a)P(m|a) and P(j|\u00aca)P(m|\u00aca) for each value of e\nEvaluation of expression shown in the following tree:\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 24a: Exact Inference in Bayesian Networks",
        "page": "6"
    },
    "24a_7": {
        "content": "Introduction\nInference by enumeration\nThe variable enumeration algorithm\nSummary\n\nSummary\n\nVariable enumeration: an exact inference in BNs\nNeedless repetition of some calculations\nNext time: A (slightly) more efficient exact inference\nmethod\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 8\n\n",
        "lecture": " 24a: Exact Inference in Bayesian Networks",
        "page": "7"
    },
    "24b_1": {
        "content": "Introduction\nThe variable elimination algorithm\nSummary\n\nWhere are we?\n\nLast time. . .\n\nExact inference in BNs using variable enumeration\nAlgorithm repeats some calculations\nWe would like to avoid that!\nToday: Exact inference in BNs using variable elimination\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 9\n\n",
        "lecture": " 24b: Exact Inference in Bayesian Networks",
        "page": "1"
    },
    "24b_2": {
        "content": "Introduction\nThe variable elimination algorithm\nSummary\n\nReminder of our example BN\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 9\n\nBTTFFETFTFP(A).95.29.001.001P(B).002P(E)AlarmEarthquakeMaryCallsJohnCallsBurglaryAP(J)TF.90.05AP(M)TF.70.01.94",
        "lecture": " 24b: Exact Inference in Bayesian Networks",
        "page": "2"
    },
    "24b_3": {
        "content": "Introduction\nThe variable elimination algorithm\nSummary\n\nThe variable elimination algorithm\n\nIdea of variable elimination: avoid repeated calculations\nBasic idea: store results after doing calculation once\nWorks bottom-up by evaluating subexpressions\nAssume we want to evaluate\n\u2211\n\nP(e)\n\n\u2211\n\nP(B|j, m) = \u03b1 P(B)\nf1(B)\n\n(cid:124)(cid:123)(cid:122)(cid:125)\n\n(cid:124) (cid:123)(cid:122) (cid:125)\n\nP(m|a)\nf5(A)\n\n(cid:124) (cid:123)(cid:122) (cid:125)\n\nP(j|a)\nf4(A)\n\n(cid:124)(cid:123)(cid:122)(cid:125)\n\nf2(E )\n\n(cid:124)\n\n(cid:123)(cid:122)\n\nP(a|B, e)\nf3(A,B,E )\n\n(cid:125)\n\ne\n\na\n\nWe\u2019ve annotated each part with a factor.\nA factor is a matrix, indexed with its argument variables. E.g:\n\nFactor f5(A) corresponds to P(m|a) and depends just on A\nbecause m is fixed (it\u2019s a 2\u00d7 1 matrix).\n\nf5(A) = (cid:104)P(m|a), P(m|\u00aca)(cid:105)\nf3(A, B, E ) is a 2\u00d7 2\u00d7 2 matrix for P(a|B, e)\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 9\n\n",
        "lecture": " 24b: Exact Inference in Bayesian Networks",
        "page": "3"
    },
    "24b_4": {
        "content": "Introduction\nThe variable elimination algorithm\nSummary\n\nThe variable elimination algorithm\n\nP(B|j, m) = \u03b1f1(B)\u00d7 \u2211e f2(E )\u2211a f3(A, B, E )\u00d7 f4(A)\u00d7 f5(A)\n\nSumming out A produces a 2\u00d7 2 matrix\n(via pointwise product):\nf6(B, E ) = \u2211a f3(A, B, E )\u00d7 f4(A)\u00d7 f5(A)\n= (f3(a, B, E )\u00d7 f4(a)\u00d7 f5(a))+\n\n(f3(\u00aca, B, E )\u00d7 f4(\u00aca)\u00d7 f5(\u00aca))\n\nSo now we have\nP(B|j, m) = \u03b1f1(B)\u00d7 \u2211e f2(E )\u00d7 f6(B, E )\nSum out E in the same way:\nf7(B) = (f2(e)\u00d7 f6(B, e)) + (f2(\u00ace)\u00d7 f6(B,\u00ace))\nUsing f1(B) = P(B), we can finally compute\nP(B|j, m) = \u03b1f1(B)\u00d7 f7(B)\n\nRemains to define pointwise product and summing out\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 9\n\n",
        "lecture": " 24b: Exact Inference in Bayesian Networks",
        "page": "4"
    },
    "24b_5": {
        "content": "Introduction\nThe variable elimination algorithm\nSummary\n\nAn example\n\nPointwise product yields product for union of variables in its\narguments:\n\nf(X1 . . . Xi , Y1 . . . Yj , Z1 . . . Zk ) = f1(X1 . . . Xi , Y1 . . . Yj )f2(Y1 . . . Yj , Z1 . . . Zk )\n\n0.3\n0.7\n0.9\n0.1\n\nB C\nT T\nT F\nF T\nF\nF\n\nA B f1(A, B)\nT T\nT F\nF T\nF\nF\n\nf(A, B, C )\nA B C\nT T T 0.3 \u00d7 0.2\n0.3 \u00d7 0.8\nT T F\nT F T 0.7 \u00d7 0.6\n0.7 \u00d7 0.4\nT F\nF T T 0.9 \u00d7 0.2\n0.9 \u00d7 0.8\nF T F\nF T 0.1 \u00d7 0.6\nF\n0.1 \u00d7 0.4\nF\nF\nFor example f(T , T , F ) = f1(T , T )\u00d7 f2(T , F )\n\nf2(B, C )\n\n0.2\n0.8\n0.6\n0.4\n\nF\n\nF\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 9\n\n",
        "lecture": " 24b: Exact Inference in Bayesian Networks",
        "page": "5"
    },
    "24b_6": {
        "content": "Introduction\nThe variable elimination algorithm\nSummary\n\nAn example\n\nSumming out is similarly straightforward\nTrick: any factor that does not depend on the variable to be\nsummed out can be moved outside the summation process\nFor example\n\u2211\n\ne\n\nf2(E )\u00d7 f3(A, B, E )\u00d7 f4(A)\u00d7 f5(A)\n= f4(A)\u00d7 f5(A)\u00d7\u2211\n\nf2(E )\u00d7 f3(A, B, E )\n\ne\n\nMatrices are only multiplied when we need to sum out a\nvariable from the accumulated product\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 9\n\n",
        "lecture": " 24b: Exact Inference in Bayesian Networks",
        "page": "6"
    },
    "24b_7": {
        "content": "Introduction\nThe variable elimination algorithm\nSummary\n\nAnother Example: P(J|b) = (cid:104)P(j|b), P(\u00acj|b)(cid:105)\n\n(cid:124)(cid:123)(cid:122)(cid:125)\n\nP(J|b) = \u03b1 \u2211e \u2211a \u2211m P(J, b, e, a, m)\n(cid:125)\n(cid:123)(cid:122)\n(cid:124)\n\u2211a P(a|b, e)\nf2(A, E )\n\n= \u03b1 \u2211e \u2211a \u2211m P(b)P(e)P(a|b, e)P(J|a)P(m|a)\n(cid:123)(cid:122)\n= \u03b1(cid:48) \u2211e P(e)\nf1(E )\n= \u03b1(cid:48) \u2211e f1(E )\n2\u00d7 1\n= \u03b1(cid:48) \u2211e f1(E )\n2\u00d7 1\n= \u03b1(cid:48)f5(J)\n\n\u2211a f2(A, E )\n2\u00d7 2\nf4(J, E )\n2\u00d7 2\n\n(cid:124) (cid:123)(cid:122) (cid:125)\n\nP(J|a)\nf3(J, A)\n\nf3(J, A)\n2\u00d7 2\n\n(cid:124)\n\u2211\n\nm\n\nP(m|a)\n\n(cid:125)\n\n= 1\n\nprod., marg.\ncond. indep.\nmove terms\n\nCan eliminate all variables that aren\u2019t ancestors of query or\nevidence variables!\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 9\n\n",
        "lecture": " 24b: Exact Inference in Bayesian Networks",
        "page": "7"
    },
    "24b_8": {
        "content": "Introduction\nThe variable elimination algorithm\nSummary\n\nSummary\n\nInference in Bayesian Networks\nExact methods: enumeration, variable elimination algorithm\nComputationally intractable in the worst case\nNext time: Approximate inference in Bayesian Networks\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 9\n\n",
        "lecture": " 24b: Exact Inference in Bayesian Networks",
        "page": "8"
    },
    "25a_1": {
        "content": "Introduction\nDirect sampling methods\nSummary\n\nWhere are we?\n\nLast time . . .\n\nInference in Bayesian Networks\nExact methods: enumeration, variable elimination algorithm\nComputationally intractable in the worst case\n\nToday . . .\n\nApproximate Inference in Bayesian Networks\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 10\n\n",
        "lecture": " 25a: Approximate inference in BNs",
        "page": "1"
    },
    "25a_2": {
        "content": "Introduction\nDirect sampling methods\nSummary\n\nApproximate inference in BNs\n\nExact inference computationally very hard\nApproximate methods important, here randomised sampling\nalgorithms\nMonte Carlo algorithms\nWe will talk about two types of MC algorithms:\n\n1 Direct sampling methods (today)\n2 Markov chain sampling (next time)\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 10\n\n",
        "lecture": " 25a: Approximate inference in BNs",
        "page": "2"
    },
    "25a_3": {
        "content": "Introduction\nDirect sampling methods\nSummary\n\nRejection sampling\n\nDirect sampling methods\n\nBasic idea: generate samples from a known probability\ndistribution\nConsider an unbiased coin as a random variable \u2013 sampling\nfrom the distribution is like \ufb02ipping the coin\nIt is possible to sample any distribution on a single variable\ngiven a set of random numbers from [0,1]\nSimplest method: generate events from network without\nevidence\n\nSample each variable in \u2018topological order\u2019\nProbability distribution for sampled value is conditioned on\nvalues assigned to parents\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 10\n\n",
        "lecture": " 25a: Approximate inference in BNs",
        "page": "3"
    },
    "25a_4": {
        "content": "Introduction\nDirect sampling methods\nSummary\n\nRejection sampling\n\nExample\n\nConsider the following BN and ordering\n[Cloudy , Sprinkler , Rain, WetGrass]:\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 10\n\n",
        "lecture": " 25a: Approximate inference in BNs",
        "page": "4"
    },
    "25a_5": {
        "content": "Introduction\nDirect sampling methods\nSummary\n\nRejection sampling\n\nExample\n\nDirect sampling process:\n\nSample from P(Cloudy ) = (cid:104)0.5,0.5(cid:105), suppose this returns true\nSample from P(Sprinkler|Cloudy = true) = (cid:104)0.1,0.9(cid:105), suppose\nthis returns false\nSample from P(Rain|Cloudy = true) = (cid:104)0.8,0.2(cid:105),\nsuppose this returns true\nSample from\nP(WetGrass|Sprinkler = false, Rain = true) = (cid:104)0.9,0.1(cid:105),\nsuppose this returns true\n\nEvent returned=[true, false, true, true]\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 10\n\n",
        "lecture": " 25a: Approximate inference in BNs",
        "page": "5"
    },
    "25a_6": {
        "content": "Introduction\nDirect sampling methods\nSummary\n\nRejection sampling\n\nDirect sampling methods\n\nGenerates samples with probability S(x1, . . . , xn)\n\nP(xi|parents(Xi ))\n\nS(x1, . . . , xn) = P(x1, . . . , xn) =\n\nn\u220f\ni =1\ni.e. in accordance with the distribution\nAnswers are computed by counting the number N(x1, . . . , xn)\nof the times event x1, . . . , xn was generated and dividing by\ntotal number N of all samples\nIn the limit, we should get\n\nlim\nn\u2192\u221e\n\nN(x1, . . . , xn)\n\nN\n\n= S(x1, . . . , xn) = P(x1, . . . , xn)\n\nIf the estimated probability \u02c6P becomes exact in the limit we\ncall the estimate consistent and we write \u201c\u2248\u201d in this sense,\ne.g.\n\nP(x1, . . . , xn) \u2248 N(x1, . . . , xn)/N\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 10\n\n",
        "lecture": " 25a: Approximate inference in BNs",
        "page": "6"
    },
    "25a_7": {
        "content": "Introduction\nDirect sampling methods\nSummary\n\nRejection sampling\n\nRejection sampling\n\nPurpose: to produce samples for hard-to-sample distribution\nfrom easy-to-sample distribution\nTo determine P(X|e) generate samples from the prior\ndistribution specified by the BN first\nThen reject those that do not match the evidence\nThe estimate \u02c6P(X = x|e) is obtained by counting how often\nX = x occurs in the remaining samples\nRejection sampling is consistent because, by definition:\n\n\u02c6P(X|e) =\n\nN(X ,e)\nN(e)\n\n\u2248 P(X ,e)\nP(e)\n\n= P(X|e)\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 10\n\n",
        "lecture": " 25a: Approximate inference in BNs",
        "page": "7"
    },
    "25a_8": {
        "content": "Introduction\nDirect sampling methods\nSummary\n\nRejection sampling\n\nBack to our example\n\nAssume we want to estimate P(Rain|Sprinkler = true), using\n100 samples\n\n73 have Sprinkler = false (rejected), 27 have Sprinkler = true\nOf these 27, 8 have Rain = true and 19 have Rain = false\n\nP(Rain|Sprinkler = true) \u2248 \u03b1(cid:104)8,19(cid:105) = (cid:104)0.296,0.704(cid:105)\nTrue answer would be (cid:104)0.3,0.7(cid:105)\nBut the procedure rejects too many samples that are not\nconsistent with e (exponential in number of variables)\nNot really usable (similar to naively estimating conditional\nprobabilities from observation)\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 10\n\n",
        "lecture": " 25a: Approximate inference in BNs",
        "page": "8"
    },
    "25a_9": {
        "content": "Introduction\nDirect sampling methods\nSummary\n\nSummary\n\nApproximate inference in BNs\n\nDirect sampling\nrejection sampling\n\nCan answer query of the form P(X|e)\nBut wasteful. . .\n\nNext time: likelihood weighting and MCMC\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 10\n\n",
        "lecture": " 25a: Approximate inference in BNs",
        "page": "9"
    },
    "25b_1": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nWhere are we?\n\nLast time. . .\n\nApproximate inference in BNs: Direct sampling\nRejection sampling for queries P(X|e)\nVery wasteful!\nToday: Likelihood weighting and MCMC\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "1"
    },
    "25b_2": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nReminder\n\n[Cloudy , Sprinkler , Rain, WetGrass]:\n\nQuery P(X|e);\nwhere Y are the non-query and non-evidence variables.\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "2"
    },
    "25b_3": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nLikelihood weighting\n\nA direct sampling method that avoids inefficiency of rejection\nsampling,\nby generating only samples consistent with evidence\nFixes the values for evidence variables E and samples only the\nremaining variables X and Y\nSince not all events are equally probable, each event has to be\nweighted by its likelihood that it accords to the evidence\nLikelihood is measured by product of conditional probabilities\nfor each evidence variable, given its parents\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "3"
    },
    "25b_4": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nLikelihood weighting\n\nConsider query P(Rain|Sprinkler = true, WetGrass = true) in\nour example; initially set weight w = 1, then event is\ngenerated:\n\nSample from P(Cloudy ) = (cid:104)0.5,0.5(cid:105), suppose this returns true\nSprinkler is evidence variable with value true, we set\n\nw \u2190 w \u00d7 P(Sprinkler = true|Cloudy = true) = 0.1\n\nSample from P(Rain|Cloudy = true) = (cid:104)0.8,0.2(cid:105), suppose this\nreturns true\nWetGrass is evidence variable with value true, we set\nw \u2190 w \u00d7P(WetGrass = true|Sprinkler = true, Rain = true) = 0.099\n\nSample returned=[true, true, true, true] with weight 0.099\ntallied under Rain = true\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "4"
    },
    "25b_5": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nLikelihood weighting \u2013 why it works\n\ni=1 P(zi|parents(Zi ))\n\nS(z,e) = \u220fl\nS\u2019s sample values for each Zi is in\ufb02uenced by the evidence\namong Zi\u2019s ancestors\nBut S pays no attention when sampling Zi\u2019s value to evidence\nfrom Zi\u2019s non-ancestors; so it\u2019s not sampling from the true\nposterior probability distribution!\nBut the likelihood weight w makes up for the di\ufb00erence\nbetween the actual and desired sampling distributions:\n\nw (z,e) =\n\nm\u220f\n\ni=1\n\nP(ei|parents(Ei ))\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "5"
    },
    "25b_6": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nLikelihood weighting \u2013 why it works\n\nSince two products cover all the variables in the network, we\ncan write\n\nP(zi|parents(Zi ))\n\nP(ei|parents(Ei ))\n\nP(z,e) =\n\nl\u220f\n(cid:124)\n\ni =1\n\n(cid:123)(cid:122)\n\nS(z,e)\n\n(cid:125)\n\nm\u220f\n(cid:124)\n\ni =1\n\n(cid:123)(cid:122)\n\nw (z,e)\n\n(cid:125)\n\nWith this, it is easy to derive that likelihood weighting is\nconsistent (tutorial exercise)\nProblem: most samples will have very small weights as the\nnumber of evidence variables increases\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "6"
    },
    "25b_7": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nThe Markov chain Monte Carlo (MCMC) algorithm\n\nMCMC algorithm: create an event from a previous event,\nrather than generate all events from scratch\nHelpful to think of the BN as having a current state\nspecifying a value for each variable\nConsecutive state is generated by sampling a value for one of\nthe non-evidence variables Xi conditioned on the current\nvalues of variables in the Markov blanket of Xi\nRecall that Markov blanket consists of parents, children, and\nchildren\u2019s parents\nAlgorithm randomly wanders around state space \ufb02ipping one\nvariable at a time and keeping evidence variables fixed\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "7"
    },
    "25b_8": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nThe MCMC algorithm\n\nConsider query P(Rain|Sprinkler = true, WetGrass = true)\nonce more\nSprinkler and WetGrass (evidence variables) are fixed to their\nobserved values, hidden variables Cloudy and Rain are\ninitialised randomly (e.g. true and false)\nInitial state is [true, true, false, true]\nExecute repeatedly:\n\nSample Cloudy given values of Markov blanket, i.e. sample\nfrom P(Cloudy|Sprinkler = true, Rain = false)\nSuppose result is false, new state is [false, true, false, true]\nSample Rain given values of Markov blanket, i.e. sample from\nP(Rain|Sprinkler = true, Cloudy = false, WetGrass = true)\nSuppose we obtain Rain = true, new state\n[false, true, true, true]\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "8"
    },
    "25b_9": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nThe MCMC algorithm \u2013 why it works\n\nEach state is a sample, contributes to estimate of query\nvariable Rain (count samples to compute estimate as before)\nBasic idea of proof that MCMC is consistent:\n\nThe sampling process settles into a \u201cdynamic equilibrium\u201d\nin which the long-term fraction of time spent in each state\nis exactly proportional to its posterior probability\n\nMCMC is a very powerful method used for all kinds of things\ninvolving probabilities\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "9"
    },
    "25b_10": {
        "content": "Likelihood weighting\nInference by Markov chain simulation\nSummary\n\nSummary\n\nApproximate inference in BNs\nrejection sampling (last time)\nLikelihood working and why it works\nMCMC algorithm and why it works\nNext time: Time and Uncertainty I\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 11\n\n",
        "lecture": " 25b: Approximate inference in BNs:",
        "page": "10"
    },
    "26a_1": {
        "content": "Time and uncertainty\nSummary\n\nWhere are we?\n\nSo far. . .\n\nCompleted our account of Bayesian Networks\nDealt with methods for exact and approximate inference in\nBNs\nEnumeration, variable elimination, sampling, MCMC\n\nToday . . .\n\nTime and uncertainty I\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "1"
    },
    "26a_2": {
        "content": "Time and uncertainty\nSummary\n\nStates and observations\nStationary processes and the Markov assumption\n\nTime and uncertainty\n\nSo far we have only seen methods for describing uncertainty in\nstatic environments\nEvery variable had a fixed value, we assumed that nothing\nchanges during evidence collection or diagnosis\nMany practical domains involve uncertainty about processes\nthat can be modelled with probabilistic methods\nBasic idea straightforward: imagine one BN model of the\nproblem for every time step and reason about changes between\nthem\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "2"
    },
    "26a_3": {
        "content": "Time and uncertainty\nSummary\n\nStates and observations\nStationary processes and the Markov assumption\n\nStates and observations\n\nAdopted approach similar to situation calculus: series of\nsnapshots (time slices) will be used to describe process of\nchange\nSnapshots consist of observable random variables Et and\nnon-observable ones Xt\nFor simplicity, we assume sets of (non)observable variables\nremain constant over time, but this is not necessary\nObservation at t will be Et = et for some set of values et\nAssume that states start at t = 0 and evidence starts arriving\nat t = 1\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "3"
    },
    "26a_4": {
        "content": "Time and uncertainty\nSummary\n\nStates and observations\nStationary processes and the Markov assumption\n\nStates and observations\n\nExample: underground security guard wants to predict whether\nit is raining but only observes every morning whether director\ncomes in carrying umbrella\nFor each day, Et contains variable Ut (whether the umbrella\nappears) and Xt contains state variable Rt (whether it\u2019s\nraining)\nEvidence U1, U2, . . ., state variables R0, R1, . . .\nUse notation a : b to denote sequences of integers,\ne.g. U1, U2, U3 = U1:3\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "4"
    },
    "26a_5": {
        "content": "Time and uncertainty\nSummary\n\nStates and observations\nStationary processes and the Markov assumption\n\nStationary processes and the Markov assumption\n\nHow do we specify dependencies among variables?\nNatural to arrange them in temporal order (causes usually\nprecede e\ufb00ects)\nProblem: set of variables is unbounded (one for each time\nslice), so we would have to\n\nspecify unbounded number of conditional probability tables\nspecify an unbounded number of parents for each of these\n\nSolution to first problem: we assume that changes are caused\nby a stationary process \u2013 the laws that govern the process do\nnot change themselves over time (not to be confused with\n\u201cstatic\u201d)\nFor example, P(Ut|Parents(Ut)) does not depend on t\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "5"
    },
    "26a_6": {
        "content": "Time and uncertainty\nSummary\n\nStates and observations\nStationary processes and the Markov assumption\n\nStationary processes and the Markov assumption\n\nSolution to second problem: Markov assumption \u2013 the\ncurrent state only depends on a finite history of previous states\nSuch processes are called Markov processes or Markov chains\nSimplest form: first-order Markov processes, every state\ndepends only on predecessor state\nWe can write this as P(Xt|X0:t\u22121) = P(Xt|Xt\u22121)\nThis conditional distribution is called transition model\nDi\ufb00erence between first-order and second-order Markov\nprocesses:\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "6"
    },
    "26a_7": {
        "content": "Time and uncertainty\nSummary\n\nStates and observations\nStationary processes and the Markov assumption\n\nStationary processes and the Markov assumption\n\nAssume that evidence variables are conditionally independent\nof other stu\ufb00 given the current state:\n\nP(Et|X0:t,E0:t\u22121) = P(Et|Xt)\n\nThis is called the sensor model (observation model) of the\nsystem\nNotice direction of dependence: state causes evidence (but\ninference goes in other direction!)\nIn umbrella world, rain causes umbrella to appear\nFinally, we need a prior distribution over initial states P(X0)\nThese three distributions give a specification of the complete\nJPD:\n\nP(X0,X1, . . . ,Xt,E1, . . . ,Et) = P(X0)\n\nP(Xi|Xi\u22121)P(Ei|Xi )\n\nt\u220f\n\ni=1\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "7"
    },
    "26a_8": {
        "content": "Time and uncertainty\nSummary\n\nStates and observations\nStationary processes and the Markov assumption\n\nUmbrella world example\n\nBayesian network structure and conditional distributions\nTransition model P(Raint|Raint\u22121), sensor model\nP(Umbrellat|Raint)\n\nRain depends only on rainfall on previous day, whether this is\nreasonable depends on domain!\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "8"
    },
    "26a_9": {
        "content": "Time and uncertainty\nSummary\n\nStates and observations\nStationary processes and the Markov assumption\n\nStationary processes and the Markov assumption\n\nIf Markov assumptions seems too simplistic for some domains\n(and hence, inaccurate), two measures can be taken\n\nWe can increase the order of the Markov process model\nWe can increase the set of state variables\n\nFor example, add information about season, pressure or\nhumidity\nBut this will also increase prediction requirements (problem\nalleviated if we add new sensors)\nExample: dependency of predicting movement of robot on\nbattery power level\n\nadd battery level sensor\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "9"
    },
    "26a_10": {
        "content": "Time and uncertainty\nSummary\n\nSummary\n\nTime and Uncertainty:\n\nIn a dynamic environment, random variables change values\nover time\nThere are (Latent) state variables and variables whose values\nare observed.\nStationarity and Markov assumptions are important for\nobtaining a compact representation of an unbounded process\nThey\u2019re also important for practical inference!\nNext time: Time and Uncertainty: Inference\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 11\n\n",
        "lecture": " 26a: Time and Uncertainty:",
        "page": "10"
    },
    "26b_1": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nWhere are we?\n\nLast time. . .\n\nDBNs represent uncertainty in dynamic environments\nTwo assumptions\n\nChange is a stationary process\nMarkov assumption\n\njustify treating each time slice as a BN\n(with links from Xt to Xt+1)\nToday: DBN Inference I\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "1"
    },
    "26b_2": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nReminder\n\nBayesian network structure and conditional distributions\nTransition model P(Raint|Raint\u22121), sensor model\nP(Umbrellat|Raint)\n\nRain depends only on rainfall on previous day, whether this is\nreasonable depends on domain!\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "2"
    },
    "26b_3": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nInference tasks in temporal models\n\nNow that we have described general model, we need inference\nmethods for a number of tasks\nFiltering/monitoring: compute belief state given evidence\nto date, i.e. P(Xt|e1:t)\nInterestingly, an almost identical calculation yields the\nlikelihood of the evidence sequence P(e1:t)\nPrediction: computing posterior distribution over a future\nstate given evidence to date: P(Xt+k|e1:t)\nSmoothing/hindsight: compute posterior distribution of past\nstate, P(Xk|e1:t), 0 \u2264 k < t\nMost likely explanation: compute arg maxx1:t P(x1:t|e1:t)\ni.e. the most likely sequence of states given evidence\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "3"
    },
    "26b_4": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nFiltering and prediction\n\nDone by recursive estimation: compute result for t + 1 by\ndoing it for t and then updating with new evidence et+1. That\nis, for some function f :\n\nP(Xt+1|e1:t+1) = f (et+1,P(Xt|e1:t ))\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "4"
    },
    "26b_5": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nWhy recursion works\n\nP(Xt+1|e1:t+1) = P(Xt+1|e1:t ,et+1)\n\n= \u03b1P(Xt+1,e1:t ,et+1)\n= \u03b1P(et+1|Xt+1,e1:t )P(Xt+1,e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1,e1:t )P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nP(e1:t )\n\nP(Xt+1,xt|e1:t )\n\nP(Xt+1,xt ,e1:t )\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\n(split notation)\n(Bayes)\n(Bayes)\n(Bayes)\n(Markov)\n(marginalisation)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "5"
    },
    "26b_6": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nWhy recursion works\n\nP(Xt+1|e1:t+1) = P(Xt+1|e1:t ,et+1)\n\n= \u03b1P(Xt+1,e1:t ,et+1)\n= \u03b1P(et+1|Xt+1,e1:t )P(Xt+1,e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1,e1:t )P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nP(e1:t )\n\nP(Xt+1,xt|e1:t )\n\nP(Xt+1,xt ,e1:t )\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\n(split notation)\n(Bayes)\n(Bayes)\n(Bayes)\n(Markov)\n(marginalisation)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "6"
    },
    "26b_7": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nWhy recursion works\n\nP(Xt+1|e1:t+1) = P(Xt+1|e1:t ,et+1)\n\n= \u03b1P(Xt+1,e1:t ,et+1)\n= \u03b1P(et+1|Xt+1,e1:t )P(Xt+1,e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1,e1:t )P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nP(e1:t )\n\nP(Xt+1,xt|e1:t )\n\nP(Xt+1,xt ,e1:t )\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\n(split notation)\n(Bayes)\n(Bayes)\n(Bayes)\n(Markov)\n(marginalisation)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "7"
    },
    "26b_8": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nWhy recursion works\n\nP(Xt+1|e1:t+1) = P(Xt+1|e1:t ,et+1)\n\n= \u03b1P(Xt+1,e1:t ,et+1)\n= \u03b1P(et+1|Xt+1,e1:t )P(Xt+1,e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1,e1:t )P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nP(e1:t )\n\nP(Xt+1,xt|e1:t )\n\nP(Xt+1,xt ,e1:t )\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\n(split notation)\n(Bayes)\n(Bayes)\n(Bayes)\n(Markov)\n(marginalisation)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "8"
    },
    "26b_9": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nWhy recursion works\n\nP(Xt+1|e1:t+1) = P(Xt+1|e1:t ,et+1)\n\n= \u03b1P(Xt+1,e1:t ,et+1)\n= \u03b1P(et+1|Xt+1,e1:t )P(Xt+1,e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1,e1:t )P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nP(e1:t )\n\nP(Xt+1,xt|e1:t )\n\nP(Xt+1,xt ,e1:t )\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\n(split notation)\n(Bayes)\n(Bayes)\n(Bayes)\n(Markov)\n(marginalisation)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "9"
    },
    "26b_10": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nWhy recursion works\n\nP(Xt+1|e1:t+1) = P(Xt+1|e1:t ,et+1)\n\n= \u03b1P(Xt+1,e1:t ,et+1)\n= \u03b1P(et+1|Xt+1,e1:t )P(Xt+1,e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1,e1:t )P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nP(e1:t )\n\nP(Xt+1,xt|e1:t )\n\nP(Xt+1,xt ,e1:t )\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\n(split notation)\n(Bayes)\n(Bayes)\n(Bayes)\n(Markov)\n(marginalisation)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "10"
    },
    "26b_11": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nWhy recursion works\n\nP(Xt+1|e1:t+1) = P(Xt+1|e1:t ,et+1)\n\n= \u03b1P(Xt+1,e1:t ,et+1)\n= \u03b1P(et+1|Xt+1,e1:t )P(Xt+1,e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1,e1:t )P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)P(Xt+1|e1:t )\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nP(e1:t )\n\nP(Xt+1,xt|e1:t )\n\nP(Xt+1,xt ,e1:t )\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\n(split notation)\n(Bayes)\n(Bayes)\n(Bayes)\n(Markov)\n(marginalisation)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "11"
    },
    "26b_12": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nDerivation continued. . .\n\nP(Xt+1|e1:t+1) =\n\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\nP(Xt+1|xt ,e1:t )P(xt|e1:t )\nP(Xt+1|xt )P(xt|e1:t )\n\n(last slide!)\n\n(Bayes)\n\n(Markov)\n\nP(et+1|Xt+1) is sensor model; P(Xt+1|xt) is transition model,\nP(xt|e1:t) is recursive bit.\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "12"
    },
    "26b_13": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nDerivation continued. . .\n\nP(Xt+1|e1:t+1) =\n\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\nP(Xt+1|xt ,e1:t )P(xt|e1:t )\nP(Xt+1|xt )P(xt|e1:t )\n\n(last slide!)\n\n(Bayes)\n\n(Markov)\n\nP(et+1|Xt+1) is sensor model; P(Xt+1|xt) is transition model,\nP(xt|e1:t) is recursive bit.\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "13"
    },
    "26b_14": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nDerivation continued. . .\n\nP(Xt+1|e1:t+1) =\n\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n= \u03b1(cid:48)P(et+1|Xt+1)\u2211\n\nxt\n\nxt\n\nxt\n\nP(Xt+1|xt ,e1:t )P(xt ,e1:t )\n\nP(e1:t )\n\nP(Xt+1|xt ,e1:t )P(xt|e1:t )\nP(Xt+1|xt )P(xt|e1:t )\n\n(last slide!)\n\n(Bayes)\n\n(Markov)\n\nP(et+1|Xt+1) is sensor model; P(Xt+1|xt) is transition model,\nP(xt|e1:t) is recursive bit.\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "14"
    },
    "26b_15": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nFiltering and prediction\n\nWe can view estimate P(Xt|e1:t) as \u201cmessage\u201d f1:t propagated\nand updated through sequence\nWe write this process as f1:t+1 = \u03b1Forward(f1:t,et+1)\nTime and space requirements for this are constant regardless\nof length of sequence\nThis is extremely important for agent design!\nAll this is very abstract, let\u2019s look at an example\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "15"
    },
    "26b_16": {
        "content": "Example\n\nIntroduction\nInference in temporal models\nSummary\n\nCompute P(R2|u1:2), U1 = true, U2 = true\n\nSuppose P(R0) = (cid:104)0.5,0.5(cid:105)\nRecursive equations:\nP(R2|u1, u2) = \u03b1P(u2|R2)\u2211r1 P(R2|r1)P(r1|u1)\nP(R1|u1) = \u03b1(cid:48)P(u1|R1)\u2211r0 P(R1|r0)P(r0)\n\n= \u03b1(cid:48)(cid:104)0.9,0.2(cid:105)((cid:104)0.7,0.3(cid:105)\u00d7 0.5 +(cid:104)0.3,0.7(cid:105)\u00d7 0.5)\n= \u03b1(cid:48)(cid:104)0.9,0.2(cid:105)(cid:104)0.5,0.5(cid:105)\n= (cid:104)0.818,0.182(cid:105)\n\nP(R2|u1, u2) = \u03b1(cid:104)0.9,0.2(cid:105)((cid:104)0.7,0.3(cid:105)\u00d7 0.818 +(cid:104)0.3,0.7(cid:105)\u00d7 0.182)\n\n= \u03b1(cid:104)0.9,0.2(cid:105)(cid:104)0.627,0.373(cid:105)\n= \u03b1(cid:104)0.565,0.075(cid:105)\n= (cid:104)0.883,0.117(cid:105)\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "16"
    },
    "26b_17": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nFiltering and prediction\n\nPrediction works like filtering without new evidence\nComputation involves only transition model and not sensor\nmodel:\n\nP(Xt+k+1|e1:t ) = \u2211\n\nxt+k\n\nP(Xt+k+1|xt+k )P(xt+k|e1:t )\n\nAs we predict further and further into the future, distribution\nof rain converges to (cid:104)0.5,0.5(cid:105)\nThis is called the stationary distribution of the Markov\nprocess (the more uncertainty, the quicker it will converge)\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "17"
    },
    "26b_18": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nFiltering and prediction\n\nWe can use the above method to compute likelihood of\nevidence sequence P(e1:t)\nUseful to compare di\ufb00erent temporal models\nUse a likelihood message l1:t = P(Xt,e1:t) and compute\n\nl1:t+1 = \u03b1Forward(l1:t,et+1)\n\nOnce we compute l1:t, summing out yields likelihood\n\nL1:t = P(e1:t) = \u2211\nxt\n\nl1:t(xt,e1:t)\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "18"
    },
    "26b_19": {
        "content": "Introduction\nInference in temporal models\nSummary\n\nSummary\n\nDBNs for reasoning about Time and uncertainty\nInference: Filtering and prediction\nRecursion\nNext time: Time and Uncertainty: Inference II\n\nAlex Lascarides\n\nInformatics 2D\n\n12 / 12\n\n",
        "lecture": " 26b: Time and Uncertainty:",
        "page": "19"
    },
    "27a_1": {
        "content": "Introduction\nSmoothing\nSummary\n\nWhere are we?\n\nSo far. . .\n\nRepresenting uncertainty in dynamic environments\nReasoning in uncertain dynamic environments\n\nFiltering and prediction\n\nToday: Time and uncertainty: Inference II\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "1"
    },
    "27a_2": {
        "content": "Introduction\nSmoothing\nSummary\n\nSmoothing\n\nSmoothing is computation of distribution of past states given\ncurrent evidence, i.e. P(Xk|e1:t), 1 \u2264 k < t\n\nEasiest to view as 2-step process (up to k, then k + 1 to t)\nP(Xk|e1:t ) = P(Xk|e1:k ,ek+1:t )\n\n= \u03b1P(Xk|e1:k )P(ek+1:t|Xk ,e1:k )\n= \u03b1P(Xk|e1:k )P(ek+1:t|Xk )\n= \u03b1f1:kbk+1:t\n\n(split notation)\n(Bayes)\n(conditional independence)\n\nHere \u201cbackward\u201d message is bk+1:t = P(ek+1:t|Xk ) analogous\nto forward message\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "2"
    },
    "27a_3": {
        "content": "Introduction\nSmoothing\nSummary\n\nSmoothing\n\nFormula for backward message:\nP(ek+1:t|Xk ) = \u2211\n\nxk+1\n\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\n(I\u2019ll show this is true shortly)\nFirst term is sensor model; third term is transition model;\nsecond is \u2018recursive call\u2019\nDefine bk+1:t = Backward(bk+2:t,ek+1:t)\nThe backward phase has to be initialised with\nbt+1:t = P(et+1:t|Xt) = 1 (a vector of 1s) because probability\nof observing empty sequence is 1\nAs before, all this is quite abstract, back to our example\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "3"
    },
    "27a_4": {
        "content": "Umbrella World: Compute P(R1|u1, u2)\n\nIntroduction\nSmoothing\nSummary\n\nWe have P(R1|u1, u2) = \u03b1P(R1|u1)P(u2|R1)\nSo we\u2019ll need to remind ourselves of P(R1|u1) from last lecture:\nP(R1) = \u2211r0 P(R1|r0)P(r0) = (cid:104)0.7,0.3(cid:105)\u00d7 0.5 +(cid:104)0.3,0.7(cid:105)\u00d7 0.5 =\n(cid:104)0.5,0.5(cid:105)\nUpdate with evidence U1 = true yields:\nP(R1|u1) = \u03b1P(u1|R1)P(R1) = \u03b1(cid:104)0.9,0.2(cid:105)(cid:104)0.5,0.5(cid:105) \u2248 (cid:104)0.818,0.182(cid:105)\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "4"
    },
    "27a_5": {
        "content": "Introduction\nSmoothing\nSummary\n\nSmoothing Example Continued\nP(R1|u1, u2) = \u03b1P(R1|u1)P(u2|R1)\n\nForward filtering process yielded (cid:104)0.818,0.182(cid:105) for first term\nThe second term can be obtained through backward recursion:\n\nP(u2|R1) = \u2211\n\nP(u2|r2)P(|r2)P(r2|R1)\n\nr2\n\n= (0.9\u00d7 1\u00d7(cid:104)0.7,0.3(cid:105)) + (0.2\u00d7 1\u00d7(cid:104)0.3,0.7(cid:105)) = (cid:104)0.69,0.41(cid:105)\n\nPlugged into the above equation this yields\n\nP(R1|u1, u2) = \u03b1(cid:104)0.818,0.182(cid:105)\u00d7(cid:104)0.69,0.41(cid:105) \u2248 (cid:104)0.883,0.117(cid:105)\nSo our confidence that it rained on Day 1 increases when we\nsee the umbrella on the second day as well as the first.\nA simple improved version of this that stores results runs in\nlinear time (forward-backward algorithm)\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "5"
    },
    "27a_6": {
        "content": "Introduction\nSmoothing\nSummary\n\nDeriving the backward message\n\nP(ek+1:t|Xk ) = \u2211\n\nxk+1\n\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\nP(ek+1:t|Xk ) = \u2211\n\nP(ek+1:t , xk+1|Xk )\n\nxk+1\n\nP(ek+1,ek+2:t , xk+1|Xk )\nP(ek+1|ek+2:t , xk+1,Xk )P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1,Xk )P(xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n\n(marginalisation)\n\n(split notation)\n\n(Bayes)\n\n(independence)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "6"
    },
    "27a_7": {
        "content": "Introduction\nSmoothing\nSummary\n\nDeriving the backward message\n\nP(ek+1:t|Xk ) = \u2211\n\nxk+1\n\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\nP(ek+1:t|Xk ) = \u2211\n\nP(ek+1:t , xk+1|Xk )\n\nxk+1\n\nP(ek+1,ek+2:t , xk+1|Xk )\nP(ek+1|ek+2:t , xk+1,Xk )P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1,Xk )P(xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n\n(marginalisation)\n\n(split notation)\n\n(Bayes)\n\n(independence)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "7"
    },
    "27a_8": {
        "content": "Introduction\nSmoothing\nSummary\n\nDeriving the backward message\n\nP(ek+1:t|Xk ) = \u2211\n\nxk+1\n\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\nP(ek+1:t|Xk ) = \u2211\n\nP(ek+1:t , xk+1|Xk )\n\nxk+1\n\nP(ek+1,ek+2:t , xk+1|Xk )\nP(ek+1|ek+2:t , xk+1,Xk )P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1,Xk )P(xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n\n(marginalisation)\n\n(split notation)\n\n(Bayes)\n\n(independence)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "8"
    },
    "27a_9": {
        "content": "Introduction\nSmoothing\nSummary\n\nDeriving the backward message\n\nP(ek+1:t|Xk ) = \u2211\n\nxk+1\n\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\nP(ek+1:t|Xk ) = \u2211\n\nP(ek+1:t , xk+1|Xk )\n\nxk+1\n\nP(ek+1,ek+2:t , xk+1|Xk )\nP(ek+1|ek+2:t , xk+1,Xk )P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1,Xk )P(xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n\n(marginalisation)\n\n(split notation)\n\n(Bayes)\n\n(independence)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "9"
    },
    "27a_10": {
        "content": "Introduction\nSmoothing\nSummary\n\nDeriving the backward message\n\nP(ek+1:t|Xk ) = \u2211\n\nxk+1\n\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\nP(ek+1:t|Xk ) = \u2211\n\nP(ek+1:t , xk+1|Xk )\n\nxk+1\n\nP(ek+1,ek+2:t , xk+1|Xk )\nP(ek+1|ek+2:t , xk+1,Xk )P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1,Xk )P(xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n\n(marginalisation)\n\n(split notation)\n\n(Bayes)\n\n(independence)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "10"
    },
    "27a_11": {
        "content": "Introduction\nSmoothing\nSummary\n\nDeriving the backward message\n\nP(ek+1:t|Xk ) = \u2211\n\nxk+1\n\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\nP(ek+1:t|Xk ) = \u2211\n\nP(ek+1:t , xk+1|Xk )\n\nxk+1\n\nP(ek+1,ek+2:t , xk+1|Xk )\nP(ek+1|ek+2:t , xk+1,Xk )P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t , xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1,Xk )P(xk+1|Xk )\nP(ek+1|xk+1)P(ek+2:t|xk+1)P(xk+1|Xk )\n\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n= \u2211\nxk+1\n\n(marginalisation)\n\n(split notation)\n\n(Bayes)\n\n(independence)\n\n(Bayes)\n\n(Bayes)\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "11"
    },
    "27a_12": {
        "content": "Introduction\nSmoothing\nSummary\n\nSummary\n\nHindsight computable via the forward backward algorithm.\nThe equations involve recursion\n(as with filtering and prediction)\nNext time: Time and Uncertainty: Inference III\n\nFinding the most likely sequence (Viterbi algorithm)\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 8\n\n",
        "lecture": " 27a: Time and Uncertainty:",
        "page": "12"
    },
    "27b_1": {
        "content": "Introduction\nFinding the most likely sequence\nHidden Markov Models\nSummary\n\nWhere are we?\n\nSo far. . .\n\nDynamic Bayesian Networks\nInference:\n\nFiltering: P(Xt|e1:t)\nLikelihood: P(e1:t)\nPrediction: P(Xt+k|e1:t)\nHindsight: P(Xt\u2212k|e1:t)\n\nToday: Inference III: Estimating the most probable\nexplanation for your observed evidence\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 9\n\n",
        "lecture": " 27b: Time and Uncertainty:",
        "page": "1"
    },
    "27b_2": {
        "content": "Introduction\nFinding the most likely sequence\nHidden Markov Models\nSummary\n\nReminder of example DBN\n\nTransition model P(Raint|Raint\u22121), sensor model\nP(Umbrellat|Raint)\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 9\n\n",
        "lecture": " 27b: Time and Uncertainty:",
        "page": "2"
    },
    "27b_3": {
        "content": "Introduction\nFinding the most likely sequence\nHidden Markov Models\nSummary\n\nFinding the most likely sequence\n\nSuppose [true, true, false, true, true] is the umbrella sequence\nfor first five days, what is the most likely weather sequence\nthat caused it?\nCould use smoothing procedure to find posterior distribution\nfor weather at each step and then use most likely weather at\neach step to construct sequence\nNO! Smoothing considers distributions over individual time\nsteps, but we must consider joint probabilities over all time\nsteps\nActual algorithm is based on viewing each sequence as path\nthrough a graph (nodes=states at each time step)\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 9\n\n",
        "lecture": " 27b: Time and Uncertainty:",
        "page": "3"
    },
    "27b_4": {
        "content": "Introduction\nFinding the most likely sequence\nHidden Markov Models\nSummary\n\nFinding the most likely sequence\n\nIn umbrella example:\n\nLook at states with Rain5 = true (part (a)), Markov property\nmost likely path to this state consists of most likely path to\nstate at time 4 followed by transition to Rain5 = true\nstate at time 4 that will become part of the path is whichever\nmaximises likelihood of the path\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 9\n\n",
        "lecture": " 27b: Time and Uncertainty:",
        "page": "4"
    },
    "27b_5": {
        "content": "Introduction\nFinding the most likely sequence\nHidden Markov Models\nSummary\n\nFinding the most likely sequence\n\nThere is a recursive relationship between most likely paths to\nxt+1 and most likely paths to each state xt\nmaxx1...xt P(x1, . . . ,xt ,Xt+1|e1:t+1)\n\n= \u03b1P(et+1|Xt+1) max\nxt\n\n(P(Xt+1|xt ) max\nx1...xt\u22121\n\nP(x1, . . . ,xt\u22121,xt|e1:t ))\n\nThis is like filtering only that the forward message is replaced\nby\n\nm1:t = max\nx1...xt\u22121\n\nP(x1, . . . ,xt\u22121,Xt|e1:t )\n\nAnd summing (marginalisation) is now replaced by\nmaximisation\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 9\n\n",
        "lecture": " 27b: Time and Uncertainty:",
        "page": "5"
    },
    "27b_6": {
        "content": "Introduction\nFinding the most likely sequence\nHidden Markov Models\nSummary\n\nFinding the most likely sequence\n\nThis algorithm (Viterbi algorithm) is similar to filtering\nRuns forward along sequence computing m message in each\nstep\nProgress in example shown in part (b) of diagram above\nIn the end it has probability for most likely sequence for\nreaching each final state\nEasy to determine overall most likely sequence\nHas to keep pointers from each state back to the best state\nthat leads to it\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 9\n\n",
        "lecture": " 27b: Time and Uncertainty:",
        "page": "6"
    },
    "27b_7": {
        "content": "Introduction\nFinding the most likely sequence\nHidden Markov Models\nSummary\n\nHidden Markov Models\n\nSo far, we have seen a general model for temporal probabilistic\nreasoning (independent of transition/sensor models)\nIn this and the following lecture we are going to look at more\nconcrete models and applications\nHidden Markov Models (HMMs): temporal probabilistic\nmodel in which state of the process is described by a single\nvariable\nLike our umbrella example (single variable Raint)\nMore than one variable can be accommodated, but only by\ncombining them into a single \u201cmega-variable\u201d\nStructure of HMMs allows for a very simple and elegant matrix\nimplementation of basic algorithms\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 9\n\n",
        "lecture": " 27b: Time and Uncertainty:",
        "page": "7"
    },
    "27b_8": {
        "content": "Introduction\nFinding the most likely sequence\nHidden Markov Models\nSummary\n\nSummary\n\nFinding the most likely sequence (Viterbi algorithm)\nTalked about HMMs\nHMMs: single state variable, simplifies algorithms (see other\ncourses for these)\nHuge significance, for example in speech recognition:\nP(words|signal) = \u03b1P(signal|words)P(words)\n\nVast array of applications, but also limits.\nNext time: Dynamic Bayesian Networks: Model design\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 9\n\n",
        "lecture": " 27b: Time and Uncertainty:",
        "page": "8"
    },
    "28a_1": {
        "content": "Introduction\nConstructing DBNs\nSummary\n\nWhere are we?\n\nSo far. . .\n\nInference in temporal models\nFiltering, hindsight, Viterbi etc.\nSpecific instances: HMMs\n\nToday. . .\n\nDynamic Bayesian Networks\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 10\n\n",
        "lecture": " 28a: Dynamic Bayesian Networks I",
        "page": "1"
    },
    "28a_2": {
        "content": "Introduction\nConstructing DBNs\nSummary\n\nDynamic Bayesian Networks\n\nWe\u2019ve already seen an example of a DBN\u2014Umbrella World\nA DBN is a BN describing a temporal probability model that\ncan have any number of state variables Xt and evidence\nvariables Et\nHMMs are DBNs with a single state and a single evidence\nvariable\nBut recall that one can combine a set of discrete (evidence or\nstate) variables into a single variable (whose values are tuples).\nSo every discrete-variable DBN can be described as a HMM.\nSo why bother with DBNs?\nBecause decomposing a complex system into constituent\nvariables, as a DBN does, ameliorates sparseness in the\ntemporal probability model\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 10\n\n",
        "lecture": " 28a: Dynamic Bayesian Networks I",
        "page": "2"
    },
    "28a_3": {
        "content": "Introduction\nConstructing DBNs\nSummary\n\nTransient failure\n\nConstructing DBNs\n\nWe have to specify prior distribution of state variables P(X0),\ntransition model P(Xt+1|Xt), and sensor model P(Et|Xt)\nAlso, we have to fix topology of nodes\nStationarity assumption\nmost convenient to specify topology for first slice\nUmbrella world example:\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 10\n\n",
        "lecture": " 28a: Dynamic Bayesian Networks I",
        "page": "3"
    },
    "28a_4": {
        "content": "Introduction\nConstructing DBNs\nSummary\n\nTransient failure\n\nAn example\n\nConsider a battery-driven robot moving in the X \u00d7 Y plane\nLet Xt = (Xt, Yt) and \u02d9Xt = ( \u02d9Xt, \u02d9Yt) state variables for position\nand velocity, and Zt measurements of position (e.g. GPS)\nAdd Batteryt for battery charge level and BMetert for the\nmeasurement of it\nWe obtain the following basic model:\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 10\n\n",
        "lecture": " 28a: Dynamic Bayesian Networks I",
        "page": "4"
    },
    "28a_5": {
        "content": "Introduction\nConstructing DBNs\nSummary\n\nTransient failure\n\nModelling failure\n\nAssume Batteryt and BMetert take on discrete values\n(e.g. integer between 0 and 5)\nThese variables should be identically distributed\n(CPT=identity matrix) unless error creeps in\nOne way to model error is through Gaussian error model,\ni.e. a small Gaussian error is added to the meter reading\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 10\n\n",
        "lecture": " 28a: Dynamic Bayesian Networks I",
        "page": "5"
    },
    "28a_6": {
        "content": "Introduction\nConstructing DBNs\nSummary\n\nTransient failure\n\nGaussian Error model\n\nThe bigger the \u03c3, the more expected errors there are.\nWe can approximate this also for the discrete case through an\nappropriate distribution.\nError \u2018on one side\u2019 when meter reading is 5 or 0.\nBut problem can be much worse:\nsensor failure rather than inaccurate measurements.\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 10\n\n",
        "lecture": " 28a: Dynamic Bayesian Networks I",
        "page": "6"
    },
    "28a_7": {
        "content": "Introduction\nConstructing DBNs\nSummary\n\nTransient failure\n\nTransient failure\n\nTransient failure: sensor occasionally sends inaccurate data\nRobot example: after 20 consecutive readings of 5 suddenly\nBMeter21 = 0\nIn Gaussian error model belief about Battery21 depends on:\n\nSensor model: P(BMeter21 = 0|Battery21) and\nPrediction model: P(Battery21|BMeter1:20)\n\nIf probability of large sensor error is smaller than sudden\ntransition to 0, then with high probability battery is considered\nempty\nA measurement of zero at t = 22 will make this (almost)\ncertain\nAfter a reading of 5 at t = 23 the probability of full battery\nwill go back to high level\nBut robot made completely wrong judgement . . .\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 10\n\n",
        "lecture": " 28a: Dynamic Bayesian Networks I",
        "page": "7"
    },
    "28a_8": {
        "content": "Introduction\nConstructing DBNs\nSummary\n\nTransient failure\n\nTransient failure\n\nCurves for prediction depending on whether BMetert is only 0\nfor t = 22/23 or whether it stays 0 indefinitely\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 10\n\n",
        "lecture": " 28a: Dynamic Bayesian Networks I",
        "page": "8"
    },
    "28a_9": {
        "content": "Introduction\nConstructing DBNs\nSummary\n\nSummary\n\nDynamic Bayesian Networks:\n\nIn constructing a model, you must decide:\n\nThe set of random variables (observable and latent)\nTheir dependencies\nThe kind of probability distributions for each CPT\n\nWe\u2019ve seen an example where the choice is suboptimal\nNext time: How do we improve the model?\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 10\n\n",
        "lecture": " 28a: Dynamic Bayesian Networks I",
        "page": "9"
    },
    "28b_1": {
        "content": "Inference in DBNs\nSummary\n\nPersistent failure\n\nWhere are we?\n\nLast time. . .\n\nDynamic Bayesian Networks:\n\nWhich random variables and dependencies (the graphical\ncomponent)?\nWhat kind of probability distributions?\n\nSaw an example where the model is deficient in a scenario\nthat\u2019s likely to occur.\nToday: What can you do about that?\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "1"
    },
    "28b_2": {
        "content": "Inference in DBNs\nSummary\n\nPersistent failure\n\nReminder: Transient failure model\n\nBMeter and Z are observed; all others are latent\nGaussian Error model for P(BMeter t|Battery t)\nWe saw last time that a sudden drop from 5 to 0 for BMeter\nleads to poor quality inference.\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "2"
    },
    "28b_3": {
        "content": "Inference in DBNs\nSummary\n\nPersistent failure\n\nHow to fix inferences about errors: first attempt\n\nTo handle failure properly, sensor model must include\npossibility of (radical) meter failure\nSimplest failure model: assign larger probability to 0 reading\nthan a Gaussian error model would; e.g.\nP(BMetert = 0|Batteryt = 5) = 0.03\nWhen faced with 0 reading, provided that predicted probability\nof empty battery is much less than 0.03, best explanation is\nmeter failure\nThis model is much less susceptible to poor inference, because\nan explanation is available\nHowever, it cannot cope with persistent failure either\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "3"
    },
    "28b_4": {
        "content": "Inference in DBNs\nSummary\n\nPersistent failure\n\nTransient failure model\n\nHandling transient failure with explicit error models\nIn case of permanent failure the robot will (wrongly) believe\nthe battery is empty\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "4"
    },
    "28b_5": {
        "content": "Inference in DBNs\nSummary\n\nPersistent failure\n\nHow to fix inferences about errors: second attempt\n\nPersistent failure models describe how sensor behaves under\nnormal conditions and after failure\nAdd additional variable BMBroken, and CPT to next\nBMBroken state has a very small probability if not broken, but\n1.0 if broken before (persistence arc)\nWhen BMBroken is true, BMeter will be 0 regardless of\nBattery:\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "5"
    },
    "28b_6": {
        "content": "Inference in DBNs\nSummary\n\nPersistent failure\n\nPersistent failure\n\nIn case of temporary blip probability of broken sensor rises\nquickly but goes back if 5 is observed\nIn case of persistent failure, robot assumes discharge of\nbattery at \u201cnormal\u201d rate\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "6"
    },
    "28b_7": {
        "content": "Inference in DBNs\nSummary\n\nExact inference in DBNs\n\nExact inference in DBNs\n\nSince DBNs are BNs, we already have inference algorithms like\nvariable elimination\nEssentially DBN equivalent to infinite \u201cunfolded\u201d BN, but\nslices beyond required inference period are irrelevant\nUnrolling: reproducing basic time slice to accommodate\nobservation sequence\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "7"
    },
    "28b_8": {
        "content": "Inference in DBNs\nSummary\n\nExact inference in DBNs\n\nExact inference in DBNs\n\nExact inference in DBNs is intractable, and this is a major\nproblem.\nThere are approximate inference methods that work well in\npractice.\nThis issue is currently a hot topic in AI. . .\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "8"
    },
    "28b_9": {
        "content": "Inference in DBNs\nSummary\n\nSummary\n\nAccount of time and uncertainty complete\nLooked at general Markovian models\nHMMs\nDBNs as general case\nQuite intractable, but powerful\nNext time: Decision Making under Uncertainty\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "9"
    },
    "28b_10": {
        "content": "Inference in DBNs\nSummary\n\nQUESTION:\nWrite down the equation for the query P(BMBroken2|Battery1 = 5,BMeter2 = 0,BMBroken1 = 0) in\nterms of the sum and product of conditional probabilities that are specified in the probabilities tables of\nthe DBN given in the lecture.\n\nP(BMB2, b2, B1 = 5, BM2 = 0, BMB1 = 0)\n\nP(BM2 = 0|BMB2, B2, B1 = 5, BMB1 = 0)P(BMB2, b2, B1 = 5, BMB1 = 0)\n\nP(BM2 = 0|BMB2, B2)P(BMB2, b2, B1 = 5, BMB1 = 0)\n\nP(BMB2|B1 = 5, BM2 = 0, BMB1 = 0) =\n\u03b1P(BMB2, B1 = 5, BM2 = 0, BMB1 = 0)\n= \u03b1 \u2211\nb2\n= \u03b1 \u2211\nb2\n= \u03b1 \u2211\nb2\n= \u03b1 \u2211\nb2\n= \u03b1(cid:48) \u2211\nb2\n= \u03b1(cid:48) \u2211\nb2\n\nP(BM2 = 0|BMB2, b2)P(BMB2|b2, B1 = 5, BMB1 = 0)P(b2, B1 = 5, BMB1 = 0)\n\nP(BM2 = 0|BMB2, b2)P(BMB2|b2, B1 = 5, BMB1 = 0)P(b2|B1 = 5, BMB1 = 0)\n\nP(BM2 = 0|BMB2, b2)P(BMB2|BMB1 = 0)P(b2|B1 = 5)\n\nBayes\nmarginalisation\n\nBayes\n\nMarkov\n\nBayes\n\nBayes\n\nMarkov\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 10\n\n",
        "lecture": " 28b: Dynamic Bayesian Networks II",
        "page": "10"
    },
    "29a_1": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nWhere are we?\n\nLast time . . .\n\nLooked at Dynamic Bayesian Networks\nGeneral, powerful method for describing temporal probabilistic\nproblems\nUnfortunately exact inference computationally too hard\n\nToday . . .\n\nDecision Making under Uncertainty\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "1"
    },
    "29a_2": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nCombining beliefs and desires\n\nRational agents do things that are an optimal tradeo\ufb00\nbetween:\n\nthe likelihood of reaching a particular resultant state (given\none\u2019s actions) and\nThe desirability of that state\n\nSo far we have done the \u2018likelihood\u2019 bit: we know how to\nevaluate the probability of being in a particular state at a\nparticular time.\nBut we\u2019ve not looked at an agent\u2019s preferences or desires\nNow we will discuss utility theory in more detail to obtain a\nfull picture of decision-theoretic agent design\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "2"
    },
    "29a_3": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nUtility theory & utility functions\n\nAgent\u2019s preferences between world states are described using a\nutility function\nUF assigns some numerical value U(S) to each state S to\nexpress its desirability for the agent\nNondeterministic action a has results Result(a) and\nprobabilities P(Result(a) = s(cid:48)|a,e) summarise agent\u2019s\nknowledge about its e\ufb00ects given evidence observations e.\nCan be combined with probabilities for outcomes to obtain\nexpected utility of action:\nEU(A|E ) = \u2211\ns(cid:48)\n\nP(Result(a) = s(cid:48)|a,e)U(s(cid:48))\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "3"
    },
    "29a_4": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nUtility theory & utility functions\n\nPrinciple of maximum expected utility (MEU) says agent\nshould use action that maximises expected utility\nIn a sense, this summarises the whole endeavour of AI:\n\nIf agent maximises utility function that correctly re\ufb02ects\nthe performance measure applied to it, then optimal perfor-\nmance will be achieved by averaging over all environments\nin which agent could be placed\n\nOf course, this doesn\u2019t tell us how to define utility function or\nhow to determine probabilities for any sequence of actions in a\ncomplex environment\nFor now we will only look at one-shot decisions, not\nsequential decisions (next lecture)\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "4"
    },
    "29a_5": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nConstraints on rational preferences\n\nMEU sounds reasonable, but why should this be the best\nquantity to maximise? Why are numerical utilities sensible?\nWhy single number?\nQuestions can be answered by looking at constraints on\npreferences\nNotation:\n\nA (cid:31) B A is preferred to B\nA \u223c B the agent is indi\ufb00erent between A and B\nA (cid:37) B the agent prefers A to B or is indi\ufb00erent between them\n\nBut what are A and B? Introduce lotteries with outcomes\nC1 . . . Cn and accompanying probabilities\nL = [p1, C1; p2, C2; . . . ; pn, Cn]\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "5"
    },
    "29a_6": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nConstraints on rational preferences\n\nOutcome of a lottery can be state or another lottery\nCan be used to understand how preferences between complex\nlotteries are defined in terms of preferences among their\n(outcome) states\nThe following are considered reasonable axioms of utility\ntheory\nOrderability: (A (cid:31) B)\u2228 (B (cid:31) A)\u2228 (A \u223c B)\nTransitivity: If agent prefers A over B and B over C then he\nmust prefer A over C: (A (cid:31) B)\u2227 (B (cid:31) C ) \u21d2 (A (cid:31) C )\nExample: Assume A (cid:31) B (cid:31) C (cid:31) A and A, B, C are goods\nAgent might trade A and some money for C if he has A\nWe then o\ufb00er B for C and some cash and then trade A for B\nAgent would lose all his money over time\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "6"
    },
    "29a_7": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nConstraints on rational preferences\n\nContinuity: If B is between A and C in preference, then with\nsome probability agent will be indi\ufb00erent between getting B\nfor sure and a lottery over A and C\n\nA (cid:31) B (cid:31) C \u21d2 \u2203p [p, A;1\u2212 p, C ] \u223c B\n\nSubstitutability: Indi\ufb00erence between lotteries leads to\nindi\ufb00erence between complex lotteries built from them\n\nA \u223c B \u21d2 [p, A;1\u2212 p, C ] \u223c [p, B;1\u2212 p, C ]\n\nMonotonicity: Preferring A to B implies preference for any\nlottery that assigns higher probability to A\n\nA (cid:31) B \u21d2 (p \u2265 q \u21d4 [p, A;1\u2212 p, B] (cid:37) [q, A;1\u2212 q, B]\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "7"
    },
    "29a_8": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nDecomposability example\n\nDecomposability: Compound lotteries can be reduced to\nsimpler one\n\n[p, A;1\u2212 p, [q, B;1\u2212 q, C ]] \u223c [p, A; (1\u2212 p)q, B; (1\u2212 p)(1\u2212 q), C ]\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "8"
    },
    "29a_9": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nFrom preferences to utility\n\nThe following axioms of utility ensure that utility functions\nfollow the above axioms on preference:\n\nUtility principle: there exists a function such that\n\nU(A) > U(B) \u21d4 A (cid:31) B U(A) = U(B) \u21d4 A \u223c B\n\nMEU principle: utility of lottery is sum of probability of\noutcomes times their utilities\n\nU([p1, S1; . . . ; pn, Sn]) = \u2211\n\ni\n\npi U(Si )\n\nBut an agent might not know even his own utilities!\nBut you can work out his (or even your own!) utilities by\nobserving his (your) behaviour and assuming that he (you)\nchooses to MEU.\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "9"
    },
    "29a_10": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nUtility functions\n\nAccording to the above axioms, arbitrary preferences can be\nexpressed by utility functions\n\nI prefer to have a prime number of \u00a3in my bank account; when\nI have \u00a310 I will give away \u00a33.\n\nBut usually preferences are more systematic, a typical example\nbeing money (roughly, we like to maximise our money)\nAgents exhibit monotonic preference toward money, but how\nabout lotteries involving money?\n\u201cWho wants to be a millionaire\u201d-type problem, is pocketing a\nsmaller amount irrational?\nExpected monetary value (EMV) is actual expectation of\noutcome\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "10"
    },
    "29a_11": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nUtility of money\n\nAssume you can keep 1 million or risk it with the prospect of\ngetting three millions at the toss of a (fair) coin\nEMV of accepting gamble is 0.5\u00d7 0 + 0.5\u00d7 3,000,000 which is\ngreater than 1,000,000\nUse Sn to denote state of possessing wealth \u201cn dollars\u201d,\ncurrent wealth Sk+1M\nExpected utilities become:\n\nEU(Accept) = 1\n2 U(Sk ) + 1\nEU(Decline) = U(Sk+1M )\n\n2 U(Sk+3M )\n\nBut it all depends on utility values you assign to levels of\nmonetary wealth (is first million more valuable than second?)\n\nAlex Lascarides\n\nInformatics 2D\n\n12 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "11"
    },
    "29a_12": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nUtility of money (empirical study)\n\nIt turns out that for most people this is usually concave (curve\n(a)), showing that going into debt is considered disastrous\nrelative to small gains in money\u2014risk averse.\n\nBut if you\u2019re already $10M in debt, your utility curve is more\nlike (b)\u2014risk seeking when desperate!\n\nAlex Lascarides\n\nInformatics 2D\n\n13 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "12"
    },
    "29a_13": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nConstraints on rational preferences\nConstraints on rational preferences\nUtility functions\n\nUtility scales\n\nAxioms don\u2019t say anything about scales\nFor example transformation of U(S) into U(cid:48)(S) = k1 + k2U(S)\n(k2 positive) doesn\u2019t a\ufb00ect behaviour\nIn deterministic contexts behaviour is unchanged by any\nmonotonic transformation (utility function is value\nfunction/ordinal function)\nOne procedure for assessing utilities is to use normalised\nutility between \u201cbest possible prize\u201d (u(cid:62) = 1) and \u201cworst\npossible catastrophe\u201d (u\u22a5 = 0)\nAsk agent to indicate preference between S and the standard\nlottery [p, u(cid:62) : (1\u2212 p), u\u22a5], adjust p until agent is indi\ufb00erent\nbetween S and standard lottery, set U(S) = p\n\nAlex Lascarides\n\nInformatics 2D\n\n14 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "13"
    },
    "29a_14": {
        "content": "Introduction\nUtility theory & utility functions\nSummary\n\nSummary\n\nFoundations of rational decision making: Maximise Expected\nUtility (MEU)\nUtility theory and its axioms, utility functions\n\nAlex Lascarides\n\nInformatics 2D\n\n15 / 15\n\n",
        "lecture": " 29a: Decision Making under Uncertainty:",
        "page": "14"
    },
    "29b_1": {
        "content": "Where are we?\n\nSummary\n\nUtility functions\n\nLast time. . .\n\nMaximise Expected Utility=Rational behaviour\nAxioms of preferences, utility functions\nToday: Decision Networks\nCombine utility with Bayesian Net representing (uncertain)\nbeliefs\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 8\n\n",
        "lecture": " 29b: Decision Making under Uncertainty",
        "page": "1"
    },
    "29b_2": {
        "content": "Decision networks\n\nSummary\n\nWhat we now need is a way of integrating utilities into our\nview of probabilistic reasoning\nDecision networks (in\ufb02uence diagrams) combine BNs with\nadditional node types for actions and utilities\nIllustrate with airport siting problem:\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\n",
        "lecture": " 29b: Decision Making under Uncertainty",
        "page": "2"
    },
    "29b_3": {
        "content": "Decision networks\n\nSummary\n\nWhat we now need is a way of integrating utilities into our\nview of probabilistic reasoning\nDecision networks (in\ufb02uence diagrams) combine BNs with\nadditional node types for actions and utilities\nIllustrate with airport siting problem:\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\n",
        "lecture": " 29b: Decision Making under Uncertainty",
        "page": "3"
    },
    "29b_4": {
        "content": "Decision networks\n\nSummary\n\nWhat we now need is a way of integrating utilities into our\nview of probabilistic reasoning\nDecision networks (in\ufb02uence diagrams) combine BNs with\nadditional node types for actions and utilities\nIllustrate with airport siting problem:\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 8\n\n",
        "lecture": " 29b: Decision Making under Uncertainty",
        "page": "4"
    },
    "29b_5": {
        "content": "Representing decision problems with DNs\n\nSummary\n\nChance nodes (ovals) represent random variables with CPTs,\nparents can be decision nodes\nDecision nodes represent decision-making points at which\nactions are available\nUtility nodes represent utility function connected to all nodes\nthat a\ufb00ect utility directly\nOften nodes describing outcome states are omitted and\nexpected utility associated with actions is expressed (rather\nthan states) \u2013 action-utility tables\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 8\n\n",
        "lecture": " 29b: Decision Making under Uncertainty",
        "page": "5"
    },
    "29b_6": {
        "content": "Representing decision problems with DNs\n\nSummary\n\nSimplified version with action-utility tables\nLess \ufb02exible but simpler (like pre-compiled version of general\ncase)\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 8\n\n",
        "lecture": " 29b: Decision Making under Uncertainty",
        "page": "6"
    },
    "29b_7": {
        "content": "Evaluating decision networks\n\nSummary\n\nEvaluation of a DN works by setting decision node to every\npossible value\n\u201cAlgorithm\u201d:\n\n1 Set evidence variables for current state\n2 For each value of decision node:\n\n1 Set decision node to that value\n2 Calculate posterior probabilities for parents of utility node\n3 Calculate resulting (expected) utility for action\n\n3 Return action with highest (expected) utility\n\nUsing any algorithm for BN inference, this yields a simple\nframework for building agents that make single-shot decisions\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 8\n\n",
        "lecture": " 29b: Decision Making under Uncertainty",
        "page": "7"
    },
    "29b_8": {
        "content": "Summary\n\nSummary\n\nFoundations for rational decision making under uncertainty\nDecision networks nicely blend with our BN framework\nOnly looked at one-shot decisions so far\nNext time: Markov Decision Processes\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 8\n\n",
        "lecture": " 29b: Decision Making under Uncertainty",
        "page": "8"
    },
    "30a_1": {
        "content": "Introduction\nSummary\n\nSequential decision problems\n\nWhere are we?\n\nLast time . . .\n\nTalked about decision making under uncertainty\nLooked at utility theory\nDiscussed axioms of utility theory\nDescribed di\ufb00erent utility functions\nIntroduced decision networks\n\nToday . . .\n\nMarkov Decision Processes\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 7\n\n",
        "lecture": " 30a: Markov Decision Processes",
        "page": "1"
    },
    "30a_2": {
        "content": "Sequential decision problems\n\nIntroduction\nSummary\n\nSequential decision problems\n\nSo far we have only looked at one-shot decisions, but decision\nprocess are often sequential\nExample scenario: a 4x3-grid in which agent moves around\n(fully observable) and obtains utility of +1 or -1 in terminal\nstates\n\nActions are somewhat unreliable (in deterministic world,\nsolution would be trivial)\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 7\n\n",
        "lecture": " 30a: Markov Decision Processes",
        "page": "2"
    },
    "30a_3": {
        "content": "Markov decision processes\n\nIntroduction\nSummary\n\nSequential decision problems\n\nTo describe such worlds, we can use a (transition) model\nT (s, a, s(cid:48)) denoting the probability that action a in s will lead\nto state s(cid:48)\nModel is Markovian: probability of reaching s(cid:48) depends only on\ns and not on history of earlier states\nThink of T as big three-dimensional table (actually a DBN)\nUtility function now depends on environment history\n\nagent receives a reward R(s) in each state s (e.g. -0.04 apart\nfrom terminal states in our example)\n(for now) utility of environment history is the sum of state\nrewards\n\nIn a sense, stochastic generalisation of search algorithms!\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 7\n\n",
        "lecture": " 30a: Markov Decision Processes",
        "page": "3"
    },
    "30a_4": {
        "content": "Markov decision processes\n\nIntroduction\nSummary\n\nSequential decision problems\n\nDefinition of a Markov Decision Process (MDP):\n\nInitial state: S0\nTransition model: T (s, a, s(cid:48))\nUtility function: R(s)\n\nSolution should describe what agent does in every state\nThis is called policy, written as \u03c0\n\u03c0(s) for an individual state describes which action should be\ntaken in s\nOptimal policy is one that yields the highest expected utility\n(denoted by \u03c0\u2217)\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 7\n\n",
        "lecture": " 30a: Markov Decision Processes",
        "page": "4"
    },
    "30a_5": {
        "content": "Example\n\nIntroduction\nSummary\n\nSequential decision problems\n\nOptimal policies in the 4x3-grid environment\n(a) With cost of -0.04 per intermediate state \u03c0\u2217 is conservative for\n\n(3,1)\n\n(b) Di\ufb00erent cost induces direct run to terminal state/shortcut at\n\n(3,1)/no risk/avoid both exits\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 7\n\n",
        "lecture": " 30a: Markov Decision Processes",
        "page": "5"
    },
    "30a_6": {
        "content": "Summary\n\nIntroduction\nSummary\n\nSequential decision making\nDefined Markov Decision Processes\nDefined policies, and optimal policies\nNext time: Computing optimal policies from MDPs\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 7\n\n",
        "lecture": " 30a: Markov Decision Processes",
        "page": "6"
    },
    "30b_1": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nWhere are we?\n\nLast time. . .\n\nMarkov Decision Processes for representing sequential decision\nproblems\n\nOptimal policy: mapping from state to action that maximises\nexpected utility\n\nToday: computing optimal policies\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "1"
    },
    "30b_2": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nReminder of our example\n\nA 4x3-grid in which agent moves around (fully observable) and\nobtains utility of +1 or -1 in terminal states\n\n3\n\n2\n\n1\n\nSTART\n\n+ 1\n\n\u20131\n\n0.8\n\n0.1\n\n0.1\n\n1\n\n2\n\n3\n\n4\n\n(a)\n\n(b)\n\nActions are somewhat unreliable (in deterministic world,\nsolution would be trivial)\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "2"
    },
    "30b_3": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nOptimality in sequential decision problems\n\nMDPs very popular in various disciplines, di\ufb00erent algorithms\nfor finding optimal policies\n\nBefore we present some of them, let us look at utility functions\nmore closely\n\nWe have used sum of rewards as utility of environment history\nuntil now, but what are the alternatives?\n\nFirst question: finite horizon or infinite horizon\n\nFinite means there is a fixed time N after which nothing\nmatters:\n\n\u2200k Uh([s0, s1, . . . , sN+k ]) = Uh([s0, s1, . . . , sN ])\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "3"
    },
    "30b_4": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nOptimality in sequential decision problems\n\nThis leads to non-stationary optimal policies (N matters)\n\nWith infinite horizon, we get stationary optimal policies (time\nat state doesn\u2019t matter)\n\nWe are mainly going to use infinite horizon utility functions\n\nNOTE: sequences to terminal states can be finite even under\ninfinite horizon utility calculation\n\nSecond issue: how to calculate utility of sequences\nStationarity here is reasonable assumption:\n\ns0 = s \u2032\n\n0 \u2227 [s0, s1, s2 . . .] \u227b [s \u2032\n\n2, . . .]\n\n0, s \u2032\n\n1, s \u2032\n\n2, . . .] \u21d2 [s1, s2 . . .] \u227b [s \u2032\n\n1, s \u2032\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "4"
    },
    "30b_5": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nOptimality in sequential decision problems\n\nStationarity may look harmless, but there are only two ways to\nassign utilities to sequences under stationarity assumptions\n\nAdditive rewards:\n\nUh([s0, s1, s2 . . .]) = R(s0) + R(S1) + R(S2) + . . .\n\nDiscounted rewards (for discount factor 0 \u2264 \u03b3 \u2264 1)\n\nUh([s0, s1, s2 . . .]) = R(s0) + \u03b3R(S1) + \u03b32R(S2) + . . .\n\nDiscount factor makes more distant future rewards less\nsignificant\n\nWe will mostly use discounted rewards in what follows\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "5"
    },
    "30b_6": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nOptimality in sequential decision problems\n\nChoosing infinite horizon rewards creates a problem\n\nSome sequences will be infinite with infinite (additive) reward,\nhow do we compare them?\nSolution 1: with discounted rewards the utility is bounded if\nsingle-state rewards are\n\nUh([s0, s1, s2 . . .]) =\n\n\u221e\n\u2211\n\nt=0\n\n\u03b3t R(st ) \u2264\n\n\u221e\n\u2211\n\nt=0\n\n\u03b3t Rmax = Rmax /(1 \u2212 \u03b3)\n\nSolution 2: under proper policies, i.e. if agent will eventually\nvisit terminal state, additive rewards are finite\n\nSolution 3: compare average reward per time step\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "6"
    },
    "30b_7": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nUtilities of states\nThe value iteration algorithm\n\nValue iteration\n\nValue iteration is an algorithm for\ncalculating optimal policy in MDPs\n\nCalculate the utility of each state and then select optimal\naction based on these utilities\n\nSince discounted rewards seemed to create no problems, we\nwill use\n\n\u03c0\u2217 = arg max\n\u03c0\n\nE\" \u221e\n\n\u2211\n\n\u03b3t R(st)|\u03c0#\n\nt=0\n\nas a criterion for optimal policy\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "7"
    },
    "30b_8": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nUtilities of states\nThe value iteration algorithm\n\nExplaining \u03c0\u2217 = arg max\u03c0 E [\u2211\u221e\n\nt=0 \u03b3t R(st)|\u03c0]\n\nEach policy \u03c0 yields a tree, with root node s0, and daughters\nto a node s are the possible successor states given the action\n\u03c0(s).\n\nT (s, a, s \u2032) gives the probability of traversing an arc from s to\ndaughter s \u2032.\n\ns0\n\ns 1\n\n1\n\ns 2\n\n1\n\n1,1\n2\n\ns\n\n1,2\n2\n\ns\n\n2,1\n2\n\ns\n\n2,2\n2\n\ns\n\nE is computed by:\n(a) For each path p in the tree, getting the product of the (joint)\nprobability of the path in this tree with its discounted reward,\nand then\n\n(b) Summing over all the products from (a)\nSo this is just a generalisation of single shot decision theory.\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "8"
    },
    "30b_9": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nUtilities of states\nThe value iteration algorithm\n\nUtilities of states: : U(s) 6= R(s)!\n\nR(s) is reward for being in s now.\n\nBy making U(s) the utility of the states that might follow it,\nU(s) captures long-term advantages from being in s\n\nU(s) re\ufb02ects what you can do from s;\nR(s) does not.\n\nStates that follow depend on \u03c0. So utility of s given \u03c0 is:\n\nU\u03c0(s) = E\" \u221e\n\n\u2211\n\nt=0\n\n\u03b3t R(st )|\u03c0, s0 = s#\n\nWith this, \u201ctrue\u201d utility U(s) is U\u03c0\u2217\ndiscounted rewards if executing optimal policy)\n\n(s) (expected sum of\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "9"
    },
    "30b_10": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nUtilities of states\nThe value iteration algorithm\n\nUtilities in our example\n\nU(s) computed for our example from algorithms to come.\n\n\u03b3 = 1, R(s) = \u22120.04 for nonterminals.\n\n3\n\n0.812\n\n0.868\n\n0.918\n\n+ 1\n\n2\n\n0.762\n\n0.660\n\n\u20131\n\n1\n\n0.705\n\n0.655\n\n0.611\n\n 0.388\n\n1\n\n2\n\n3\n\n4\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "10"
    },
    "30b_11": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nUtilities of states\nThe value iteration algorithm\n\nUtilities of states\n\nGiven U(s), we can easily determine optimal policy:\n\n\u03c0\u2217(s) = arg max\n\na \u2211\n\ns \u2032\n\nT (s, a, s \u2032)U(s \u2032)\n\nDirect relationship between\nutility of a state and that of its neighbours:\n\nUtility of a state is immediate reward plus expected utility\nof subsequent states if agent chooses optimal action\n\nThis can be written as the famous Bellman equations:\n\nU(s) = R(s) + \u03b3max\n\na \u2211\n\ns \u2032\n\nT (s, a, s \u2032)U(s \u2032)\n\nAlex Lascarides\n\nInformatics 2D\n\n12 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "11"
    },
    "30b_12": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nUtilities of states\nThe value iteration algorithm\n\nThe value iteration algorithm\n\nFor n states we have n Bellman equations with n unknowns\n(utilities of states)\n\nValue iteration is an iterative approach to solving the n\nequations.\n\nStart with arbitrary values and update them as follows:\n\nUi +1(s) \u2190 R(s) + \u03b3max\n\na \u2211\n\ns \u2032\n\nT (s, a, s \u2032)Ui (s \u2032)\n\nThe algorithm converges to right and unique solution\n\nLike propagating values through network or utilities\n\nAlex Lascarides\n\nInformatics 2D\n\n13 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "12"
    },
    "30b_13": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nUtilities of states\nThe value iteration algorithm\n\nThe value iteration algorithm\n\nValue iteration in our example: evolution of utility values of\nstates\n\ns\ne\nt\na\nm\n\ni\nt\ns\ne\n \n\ny\nt\ni\nl\ni\nt\n\nU\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n-0.2\n\n(4,3)\n(3,3)\n\n(1,1)\n(3,1)\n\n(4,1)\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\nNumber of iterations\n\nAlex Lascarides\n\nInformatics 2D\n\n14 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "13"
    },
    "30b_14": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nDecision-theoretic agents\n\nWe now have (tediously) gathered all the ingredients to build\ndecision-theoretic agents\n\nTransition and observation models will be described by a DBN\n\nThey will be augmented by decision and utility nodes to obtain\na dynamic DN\n\nDecisions will be made by projecting forward possible action\nsequences and choosing the best one\n\nPractical design for a utility-based agent\n\nAlex Lascarides\n\nInformatics 2D\n\n15 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "14"
    },
    "30b_15": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nDecision-theoretic agents\n\nDynamic decision networks look something like this\n\nGeneral form of everything we have talked about in uncertainty\npart\n\nAt\u20132\n\nAt\u20131\n\nAt\n\nAt+1\n\nAt+2\n\nXt\u20131\n\nEt\u20131\n\nRt\u20131\n\nXt\n\nEt\n\nXt+1\n\nXt+2\n\nXt+3\n\nUt+3\n\nRt\n\nRt+1\n\nRt+2\n\nEt+1\n\nEt+2\n\nEt+3\n\nAlex Lascarides\n\nInformatics 2D\n\n16 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "15"
    },
    "30b_16": {
        "content": "Introduction\nOptimality in sequential decision problems\nValue iteration\nDecision-theoretic agents\nSummary\n\nSummary\n\nSequential decision making\n\nDefined MDPs to model stochastic multi-step decision making\nprocesses\n\nValue iteration and policy iteration algorithms\n\nDesign of decision-theoretic utility-based agents based on\nDDNs\n\nCompletes our account of reasoning under uncertainty\n\nNext time: AI and Ethics\n\nAlex Lascarides\n\nInformatics 2D\n\n17 / 17\n\n",
        "lecture": " 30b: Markov Decision Processes",
        "page": "16"
    },
    "30c_1": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nWhere are we?\n\nWe have (deep breath) seen how to model AI decision making\nwhile coping with:\n\nUncertainty\nDynamic environments\nCon\ufb02icting preferences\n\nThese are powerful tools! Used in:\n\nMedicine, autonomous driving, finance,. . .\n\nToday: AI and ethics\n\nAlex Lascarides\n\nInformatics 2D\n\n2 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "1"
    },
    "30c_2": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nWhy ethics matters for AI\n\nAI cannot advance without public trust\n\nFull benefits only if perception is positive\nAnti-vaccine movement shows sound science not enough.\nProgress of AI will stall if researchers don\u2019t address human\nnorms, values\nIntelligence itself is inseperable from moral and social\ndimensions of living.\n\nAlex Lascarides\n\nInformatics 2D\n\n3 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "2"
    },
    "30c_3": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nEthics matters. . .\n\nAI already used to automate decisions with profound ethical\nconsequences\n\nlending, hiring, parole, bail, education, etc.\n\nSo AI must be well aligned with human values\nAI is a powerful technology;\nanything that creates or redistributes power needs ethical\nattention\n\nAlex Lascarides\n\nInformatics 2D\n\n4 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "3"
    },
    "30c_4": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nEthics and Tech are interwoven\n\nTechnologies are not value-neutral:\n\nre\ufb02ect human needs, wants, expectations, judgments.\nWe use tech to bring our values into the world.\n\nAlex Lascarides\n\nInformatics 2D\n\n5 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "4"
    },
    "30c_5": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nEthics in Preferences\n\nEthics and engineering share a common task:\n\n1 Translate abstract goals (happiness, justice, duty; utility,\n\nefficiency, optimization) into functional material forms (i.e.,\nactions and artifacts)\n\n2 Cope with achieving these goals in a messy, noisy, unstable\n\nworld.\n\nAlex Lascarides\n\nInformatics 2D\n\n6 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "5"
    },
    "30c_6": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nAI Augments Cognition\n\nAI can counter harmful biases in human thinking\nbut only if designed with this aim.\nAI learning/inference/memory can surpass limits in human\ncognition\n\nAlex Lascarides\n\nInformatics 2D\n\n7 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "6"
    },
    "30c_7": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nEthical Opportunities for AI\n\nNew medical and scientific breakthroughs\nImproved materials, designs and processes\nBetter forecasting of complex dynamic systems\nMore a\ufb00ordable goods and services\nFreedom from routine/repetitive tasks\nCognitive/creative/social \u2018upskilling\u2019\n\nAlex Lascarides\n\nInformatics 2D\n\n8 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "7"
    },
    "30c_8": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nCore concerns in AI ethics vs. Machine values\n\nHuman Values\nSafety, Value Alignment, Privacy, Autonomy and Liberty, Future of\nWork Accountability/Responsibility, Meaningful Human Control,\nTransparency, Explainability, Power and Justice, Fairness, Bias, and\nEquity, Diversity and Inclusion, Wisdom\n\nMachine Values\nOptimality, efficiency, speed, precision, predictability, reliability,\nreadability, compressibility, replicability, invulnerability\n\nAlex Lascarides\n\nInformatics 2D\n\n9 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "8"
    },
    "30c_9": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nHow does AI fare on (ethical) human values?\n\nProblem with traditonal reward functions:\n\nR : S \u2192 R\nR : F1 \u00d7 Fn \u2192 R\n\nor\n\nThis hides structure among basic human values:\n\nWhat\u2019s more important? Safety or comfort? Health or wealth?\nWhat are acceptable margins for any of these?\n\nDilemma of lockdown vs. easing lockdown\n\nHow do we test and audit AI against our values?\n\nTransparancy/interpretability of AI models is needed\n\nAlex Lascarides\n\nInformatics 2D\n\n10 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "9"
    },
    "30c_10": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nReal example\n\nUber autonomous vehicle fatality in Arizona in 2018\n\nRides were \u2018too bumpy\u2019 for passengers\nSo developers (manually) tweaked the reward function to give\na more comfortable ride\nThe fatality was a direct result of this change.\n\nAlex Lascarides\n\nInformatics 2D\n\n11 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "10"
    },
    "30c_11": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nFairness/Bias\n\nBias in training data can have concrete ethical impacts:\n\nTraining data permeated with human prejudices on gender,\nrace, etc. can lead to human prejudice baked into the machine\ndecisions (lending, recruitment)\nOr leads to excluding users. . .\n\nhttps://www.youtube.com/watch?v=J3lYLphzAnw\n\nAlex Lascarides\n\nInformatics 2D\n\n12 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "11"
    },
    "30c_12": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nEthical Leadership\n\n1 Modelling ethics as a shared professional task\n2 Evaluating ethical implications of our research\n3 Aligning our decisions with our professed values.\n4 Using ethics to inform and refine our technical and scientific\n\nwork.\n\nAlex Lascarides\n\nInformatics 2D\n\n13 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "12"
    },
    "30c_13": {
        "content": "Why?\nEthics and Tech\nThe Positive\nThe Challenges\nSummary\n\nSummary\n\nYou\u2019ve learned a lot of AI methods for intelligent decision\nmaking.\nBut ethics must be baked into the design of AI agents at every\nstage.\nWe need much more research on how to align preference\nmodels with human values\nAI researchers must talk to philosophers and sociologists!\n\nAlex Lascarides\n\nInformatics 2D\n\n14 / 14\n\n",
        "lecture": " 30c: AI and Ethics",
        "page": "13"
    }
}