{
    "Dealing with high dimensions \u2013 PCA_0": {
        "content": "Inf2 \u2013 Foundations of Data Science 2021\nTopic: Dealing with high dimensions \u2013 PCA\n\nDavid C. Sterratt\n\n5th November 2021\n\nIntended learning objectives\nBy the end of this topic you should be able to:\n\n1. Explain what sorts of datasets principal components analysis (PCA) can help us to under-\n\nstand\n\n2. Explain the principle of how PCA works\n\n3. Outline the steps involved in the derivation of PCA\n\n4. Interpret the results of a PCA analysis\n\nRecommended reading:\nNeither of the course textbooks cover PCA. Witten, Frank, Hall and Pal Data mining, 4th 3d,\npp 304\u2013307 contains an overview. Di\ufb00erent sources use di\ufb00erent notation, so it may be least\nconfusing just to follow these notes.\n\n1 Video: The principle of Principal Components Analysis (PCA)\n\nThe challenges of high dimensions\nIn the multiple regression topic, in the student grade prediction\nexample, we were beginning to see two challenges of dealing with more than one independent variable:\n\nThe challenge of visualisation We can see a lot in the paired correlation plots. With 4 independent\nvariables, the visualisation works, but what about if we had 26 variables? The Scottish Index\nof Multiple Deprivation (SIMD, Table 1) records 26 variables for each of 6527 data zones in\nScotland. A 26\u00d726 grid of scatter plots is going to be di\ufb03cult to read.\n\nThe challenge of interpretation In the grades example, the test grades (independent variables) were\ncorrelated, which made the interpretation of the regression coe\ufb03cients challenging \u2013 and this\nwas with only 4 independent variables. In the SIMD example, we might expect many of the 26\n\n1\n\n\f",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "0"
    },
    "Dealing with high dimensions \u2013 PCA_1": {
        "content": "Table 1: Scottish Index of Multiple Deprivation, 2016 edition (Scottish Government, 2016). https:\n//simd.scot. It has n = 6527 data points (postcode zones), each associated with D = 26 variables.\n\nDrive\n\nCrime\n\n. . .\n\nLocation Employ-\n\nment\n\nMacdu\ufb00\nKemnay\nHilton\nRuchill\nBelmont\n. . .\n\n10\n3\n0\n8\n2\n. . .\n\nIllness Attain- Drive\nment\n5.3\n5.3\n6.3\n4.9\n6.1\n. . .\n\nPrimary Secondary\n6.6\n2.4\n3.0\n5.6\n3.2\n. . .\n\n95\n40\n10\n130\n50\n. . .\n\n1.5\n2.4\n2.2\n1.7\n3.1\n. . .\n\n249\n168\n144\n318\n129\n. . .\n\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n\nvariables to be correlated, e.g. the time it takes to drive to the nearest primary school and the\ntime it takes to drive to the nearest secondary school.\n\nThere is also another problem with high-dimensional data, called the curse of dimensionality: essen-\ntially a large number of dimensions makes is harder for distance-based methods such as clustering\nand nearest neighbours to work e\ufb00ectively \u2013 we\u2019ll come back to the curse of dimensionality in the\nfollowing lectures on clustering and nearest-neighbour methods.\n\nIn dimensionality reduction methods these challenges are addressed by reducing the number of\ndimensions in the data while retaining as much useful information as possible. There are a number of\ndimensionality reduction methods which di\ufb00er in what aspects of the data they preserve.\n\nPrincipal components analysis We are going to discuss one method of dimensionality reduction\ncalled principal components analysis (PCA).\n\nPCA can be applied to a set of D numeric variables with n datapoints. In contrast to linear regression,\nall variables are treated equally: there is no dependent variable that we are trying to predict, just a set\nof variables whose structure we\u2019re trying to understand better. The result of PCA is a set of up to D\nnew variables (with n datapoints). We can keep k \u2264 D of the most informative new variables.\n\nIn PCA the objectives are:\n\n1. change the angle we view the data from to see things clearly\n\n2. ignore small details in the data that don\u2019t a\ufb00ect the big picture.\n\nWe\u2019ll specify these objectives more precisely and explain how PCA works later. First, we will show\nthe results when PCA is applied to the SIMD example (Table 1).\n\nExample of PCA We can use PCA to reduce the number of variables D in the SIMD data from\nD = 26 to k = 2, allowing us to visualise all n = 6527 data points (Figure 1). In this plot, the ith\ndatapoint has coordinates (ti1, ti2) in which each coordinate is a linear combination of the standardised1\ndata zi j shown in Table 1:\n\nti1 = p11zi1 + p21zi2 + \u00b7 \u00b7 \u00b7 + pD1ziD\nti2 = p12zi1 + p22zi2 + \u00b7 \u00b7 \u00b7 + pD2ziD\n\n(1)\n\n1Remember from the video on variance that we standardise the jth variable x( j) by subtracting its mean x( j) and dividing\nby its standard deviation s j, so that zi j = (xi j \u2212 x( j))/s j. Generally, the data we supply to PCA do not need to be standardised,\nwe still do need to subtract the mean in order to compute the component scores. In the video lecture we use xs in the\nequivalent equation, and state that they are zero-mean versions of the variables.\n\n2\n\n",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "1"
    },
    "Dealing with high dimensions \u2013 PCA_2": {
        "content": "Figure 1: Scatter plot of first and second principal component scores (PC1 and PC2) of 6527 data\npoints in the SIMD dataset (blue dots). Locations of 4 data zones are indicated in orange dots next to\nan image from that data zone. All photos released under CC licence from geograph.co.uk. Credits:\nOrkney \u00a9 Des Colhoun; Possil Park \u00a9 Stephen Sweeney; Brunstfield \u00a9 Leslie Barrie; Fairmilehead\n\u00a9 Jim Barton.\n\nThe weights p11, p21, . . . , pD1 are elements of the first principal component and ti1 is the first prin-\ncipal component score of the ith datapoint; we will explain how to find them later. Likewise,\np12, p22, . . . , pD2 form the second principal component and ti1 is the second principal component score\nof datapoint i. The weights in the principal component indicate how much in\ufb02uence each original\nvariable has over each principal component score \u2013 sometimes they are referred to as loadings or\nweights. The axes in Figure 1 are labelled PC1 and PC2 (PC stands for principal component).\n\nTo see the in\ufb02uence of each original variable on PC1 and PC2 scores, we can project the jth original\nvariable onto the plot by setting zi j to 1 and all the other z\u2019s to 0 in Equation 1. In this case, the\ncoordinates we\u2019ll be plotting are (p j1, p j2). The orange arrows in Figure 2 show the projections of\nthe variables for unemployment, overcrowding (in housing) and school attendance. Unemployment\nand overcrowding have high PC1 scores. In contrast, school attendance has a low PC1 score. This all\nmakes sense if we identify the first component score with \u201cDeprivation\u201d. We can rephrase the previous\nsentences as \u201cunemployment and overcrowding are found in areas of high deprivation and high school\nattendance is found in areas of low deprivation\u201d.\n\nThe red arrows in Figure 2 show the projections of the time to drive to the nearest retail outlet and time\nto drive to the nearest secondary school. These vectors have higher magnitude PC2 scores than PC1\nscores. We therefore identify PC2 as being to do with \u201cremoteness\u201d \u2013 low values of PC2 indicate the\nzone is more remote.\n\n3\n\n1050510PC110.07.55.02.50.02.55.07.510.0PC2Glasgow Possil ParkOrkneyBruntsfieldFairmilehead",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "2"
    },
    "Dealing with high dimensions \u2013 PCA_3": {
        "content": "Figure 2: Scatter plots of first and second principal component scores of SIMD data zones (blue dots).\nThe projection of three original variables related to deprivation are shown as orange arrows emanating\nfrom the origin. High unemployment and overcrowded rate are found in areas with higher deprivation,\nwhereas high school attendance is found in areas with low deprivation. These vectors are more closely\naligned with the first principal component (PC1), which we therefore interpret as \u201cDeprivation\u201d. Red\narrows indicate the projections of the time take to drive to the nearest secondary school or retail outlet.\nAs these are aligned with PC2, we therefore interpret PC2 as being related to distance to services, or\n\u201cRemoteness\u201d.\n\nNote that the correlation between the PC1 and PC2 scores is zero. It is a general property of PCA there\nare no correlations between the scores of di\ufb00erent principal components.\nIn this particular example, the visualisation shows a unimodal distribution of data with little obvious\nstructure. Later on in the course we will see examples where PCA reveals clusters of data \u2013 though\nstill with zero correlation.\n\nEven if no structure is apparent, reducing the dimensionality of the data can be useful for further\nanalysis. For example, suppose we have data on cancer screening rates in each data SIMD zone, we\ncould then do multiple regression of the cancer screening rate on the new deprivation and remoteness\nvariables. This is probably going to give us coefficients that are a lot more interpretable than regressing\non all 26 variables.\n\nProjecting principal component scores back into the data space (EXTRA \u2013 not in video) Sup-\npose we have identified the first two principal component scores ti1 and ti2 of area i. We might wish to\nproject them back into the data space, to see what the original variables looked like. To do this we can\nuse the following equations to give approximations (indicated by the tilde) to the original standardised\n\n4\n\n1050510PC110.07.55.02.50.02.55.07.510.0PC2Unemploymentovercrowded_rateAttendancedrive_retaildrive_secondary",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "3"
    },
    "Dealing with high dimensions \u2013 PCA_4": {
        "content": "variables:\n\n\u02dczi1 = p11ti1 + p12ti2\n\u02dczi2 = p21ti1 + p22ti2\n\n...\n\n\u02dcziD = pD1ti1 + pD2ti2\n\n(2)\n\nWe can include more terms for higher PCs, right up to the Dth PC. In general, the jth component of\nthe i data point is given:\n\nzi j = p j1ti1 + p j2ti2 + . . . p jDtiD\n\n=\n\nD(cid:88)\n\nk=1\n\np jktik\n\n(3)\n\nOnce we\u2019ve got the standardised variables, we can convert back to the original variables using the\nformula xi j = zi js j + x j.\n\nPrincipal component equations in vector notation (EXTRA \u2013 not in video) The equations used\nso far may make more sense when expressed as vectors. The jth principal component is actually a\nvector of length 1 in the original data space:\n\np j = (p1 j, p2 j, . . . , pD j)T\n\n(4)\n\nAll the principal component vectors are orthogonal to each other. With this notation we can write\nEquation 2 as a linear combination of the principal component vectors, weighted by the principal\ncomponent scores:\n\nzi = ti1p1 + ti2p2 + . . .\n\n(5)\n\nThe dots indicate that we could go up to tiDpD.\nWe can rewrite Equation 1, in which we computed the scores, as the scalar product of the ith standard-\nised data point and the jth principal component:\n\nti j = zi \u00b7 p j\n\n(6)\n\nWe\u2019ll extend this notation to matrix notation in the derivation.\n\n2 Video: Principle of finding principal components\n\nA 2D example We\u2019ll now discuss the principle of how to determine the principal components with\nan imaginary 2D example. Suppose we ask if is there are di\ufb00erent types of Informatics students,\nperhaps based on their preferences for programming languages and for drinks. We ask students if they\nprefer, on a scale of 1\u20139, Haskell (1) to Java (9), and if they prefer Tea (1) to Co\ufb00ee (9), and find the\ndata in Table 2.\n\nPlotting the data (Figure 3 left) shows that students\u2019 preferences for drinks and programming languages\nare correlated. It seems that we could characterise every Informatics student by one number that is low\nif they like Haskell and tea, and high if they like Java and co\ufb00ee. If we could rotate the axes (Figure 3\nright), the new x-axis would give us this number.\n\n5\n\n",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "4"
    },
    "Dealing with high dimensions \u2013 PCA_5": {
        "content": "Table 2: Imaginary data about Informatics students\u2019 preferences for programming languages and\ndrinks.\n\nStudent ID Language Drink\n8\n1\n1\n2\n7\n3\n2\n4\n3\n5\n6\n6\n3\n7\n8\n8\n2\n9\n7\n10\n\n9\n3\n8\n2\n3\n8\n2\n8\n1\n6\n\nFigure 3: Informatics students\u2019 preferences for drinks and programming languages, as plotted initially\n(left), and rotated (right).\n\n6\n\nllllllllll24682468Haskell         No pref.         JavaTea             No pref.         Coffeellllllllll24682468Haskell         No pref.         JavaTea             No pref.         Coffee",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "5"
    },
    "Dealing with high dimensions \u2013 PCA_6": {
        "content": "1. Change the angle we view the data from to see things clearly\n\n2. Ignore small details in the data that don\u2019t a\ufb00ect the big picture\n\nFigure 4: Visualisation of how PCA achieves the two objectives in the text.\n\nOnce we\u2019ve done the rotation (changed the angle), we end up with the data plotted against a new set of\naxes, which are the principal components (Figure 4, top). The new x-axis, which tells us a lot about\nthe students\u2019 preferences for Java and co\ufb00ee or tea and Haskell, is the first principal component (PC1).\nThe new y axis is the second principal component (PC2). It is worth noting two points:\n\n\u2022 The correlation between the new PC1 and PC2 scores is zero. It is a general property of PCA\n\nthat correlations between scores is zero.\n\n\u2022 We have not lost any information about the data; we can reconstruct the original data by reversing\nthe rotation. It is a general property of PCA that it is possible to reconstruct the data if scores of\nall D principal components are retained.\n\nThe second principal component doesn\u2019t seem so informative, so we could just ignore it altogether\n(Figure 4, bottom). Thus, we have ignored small details in the data that don\u2019t a\ufb00ect the big picture. We\nhave performed dimensionality reduction by reducing the number of values describing each data point\nfrom two to one.\n\nObjective of rotation There are two questions that we haven\u2019t answered so far:\n\n1. How do we choose how much to rotate the axes?\n\n2. What counts as \u201cinformative\u201d?\n\nThe answer to both questions is \u201cvariance\u201d. In Figure 4 (top), the variance of the data in the PC1\ndirection is much greater than the variance of the data in the PC2 direction. The high variance PC1 is\n\n7\n\nllllllllll24682468Haskell         No pref.         JavaTea             No pref.         Coffeel\u22126\u22124\u221220246\u22126\u22124\u221220246PC 1PC 2llllllllllllllllllll\u22124\u22122024\u22124\u22122024Haskell         No pref.         JavaTea             No pref.         Coffeel\u22126\u22124\u221220246PC 1llllllllll",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "6"
    },
    "Dealing with high dimensions \u2013 PCA_7": {
        "content": "Figure 5: Scree plot for PCA applied to SIMD example (left). The elbow (or knee) is indicated in red.\nThe Cumulative scree plot (right).\n\ntelling us a lot about the informatics students, whereas the low variance PC2 tells us little. Therefore,\nin order to choose how to rotate the axes, we use the variance as an objective. In fact there are two\nways of formulating PCA:\n\n1. Maximum variance formulation: find an axis that maximises the variance of the data projected\n\nonto it\n\n2. Minimum variance formulation: find an axis that minimises the variance of the data projected\n\nonto it\n\nIt doesn\u2019t matter which formulation we use; the answer is the same either way.\n\nExplained variance The variance in the original x (Programming language) and y (Drink) directions\nwas 9.7 and 7.7. The sum of these two variances is the total variance, i.e. 17.4. It turns out that the sum\nof the variance along the principal components is exactly the same. However, the variance of the PC1\nscores is 16.5, i.e. 96% of the total variance. We therefore say the PC1 explains 96% of the variance.\n\nMore than 2D In general, we can find D principal components in D dimensions. The principal\ncomponents are all orthogonal to each other, and each principal component explains a certain fraction\nof the variance. We order the principal components from the one that explains most variance to the one\nthat explains least.\n\nIn the SIMD example, the first principal component explains 41.7% of the data and the second explains\na further 15.2%. Thus, the first two principal components together explain 56.9% of the variance. We\ncan visualise how much each principal component explains in a scree plot or cumulative scree plot\n(Figure 5).\n\nHow many components to choose? Obviously if we are visualising data, we can only look straight-\nforwardly at up to 3 dimensions. The scree plot helps us to choose how many components to include\nif we are using PCA as a preprocessing step. A rule of thumb is to use the components to the left of\n\n8\n\n01020PC010203040% variance explainedElbow or Knee01020PC020406080100Cumulative % variance explained",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "7"
    },
    "Dealing with high dimensions \u2013 PCA_8": {
        "content": "Table 3: Coefficient of determination and adjusted coefficient of determination for regression of grades\non original variables and on 2 or 4 PC scores.\n\n4 Original variables\n0.289\n0.251\n\n4 PC scores 2 PC scores\n0.282\n0.263\n\n0.289\n0.251\n\nR2\nR2\na\n\nthe \u201cknee\u201d or \u201celbow\u201d of the scree plot, i.e. the point where the gradient changes sharply. In Figure 5\nthis point is indicated in red, and the rule of thumb would suggest that we use PC1 and PC2. There\nare more principled ways of choosing, which we won\u2019t cover at this point, and it may also be that\nsuccessful application of PCA requires more components.\n\nIn the next video, we\u2019ll look at the maths of how to find the directions of the principal components and\nthe associated variances. However, you should already know enough to skip to the video after, which\nis about applying PCA to help with a regression problem.\n\n3 Video: PCA and regression\n\nPCA as preprocessing PCA is often used as a preprocessing step before another method, e.g. linear\nregression or K-means. Here we\u2019ll see how it can help simplify the grades example from the linear\nregression lecture. Figure 6 shows the results of applying PCA to the independent variables in this\nexample. Note the correlations between the PC scores are all zero; the general property of PCA already\nmentioned. However, the correlations between the PC scores and the Grade are non-zero.\nWe can regress the Grade y on the principal component scores t(1), t(2) . . . :\n\ny = \u02c6\u03b20 + \u02c6\u03b21t(1) + \u02c6\u03b22t(2) + . . .\n\n(7)\n\nWhen we regress on all 4 PC scores, we get exactly the same predictions and coefficient of determi-\nnation as we do for regressing on all variables (Table 3). This makes sense, since by keeping all 4\ncomponents we have not lost any information about the data. It is more surprising that the coefficient\nof determination with if we regress on only the first two PC scores is almost as high. Furthermore,\nthe adjusted coefficient of determination is actually higher when regression on the first two principal\ncomponents, due to there being fewer variables. There is no combination of any two of the original\nvariables that gives as high a coefficient of determination.\nThis example demonstrates that PCA can be a useful preprocessing step for regression, by decorrelating\nthe variables.\n\nPCA and linear regression lines Thinking back to linear regression, we remember the distinction\nbetween the regression lines of y on x and x on y. In two dimensions there is now a 3rd line: the first\nprincipal component. This goes right between the regression lines, and is probably what you would\nthink the line of best fit to the data is. In fact, it is a line of best fit. It\u2019s the line that minimises the sum\nof the squared distances from the data points to the line, rather than minimising the error in predicting\ny or x.\n\n4 Video: Derivation of PCA\n\nOverview of derivation Here are the steps we\u2019ll take in our derivation:\n\n9\n\n",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "8"
    },
    "Dealing with high dimensions \u2013 PCA_9": {
        "content": "Figure 6: PCA scores of independent variables in Grades example (see Multiple regression lecture\nnotes).\n\n10\n\n2.50.02.5PC1202PC220PC302PC42.50.02.5PC16080100Grade0.02.5PC22.50.0PC302PC450100Grade",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "9"
    },
    "Dealing with high dimensions \u2013 PCA_10": {
        "content": "Figure 7: Regression of y on x, x on y and the first principal component of data with a correlation\ncoefficient r = 0.5.\n\n1. Define variance along the original axes\n\n2. Project data onto rotated axes\n\n3. Compute variance in these axes\n\n4. Find direction of the axis that maximises variance of data projected onto it (1st principal\n\ncomponent, PC 1)\n\n5. Interpret\n\n6. Find the 2nd principal component (PC 2)\n\n7. Quantify what is lost by dimensionality reduction\n\nDefining variance along original axes We\u2019ve already met a lot of the mathematical machinery we\nneed in the multiple regression topic. We\u2019ll assume now that we have D variables x(1), . . . , x(D), and\n= xi j \u2212 x( j). Usually, as in the SIMD\nthat we have defined zero-mean versions of the variables x\u2217\ni j\nexample, we start o\ufb00 with standardised variables anyway. We can arrange these zero-mean variables in\nan n \u00d7 D matrix,\n\nX =\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\nx\u2217\n11\n...\nx\u2217\nn1\n\n\u00b7 \u00b7 \u00b7 x\u2217\n1D\n...\n\u00b7 \u00b7 \u00b7 x\u2217\nnD\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\nxT\n1\n...\nxT\nn\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n= (cid:16)\n\nx(1)\n\n\u00b7 \u00b7 \u00b7 x(D) (cid:17) =\n\n(8)\n\nwhich it can be helpful to write in terms of the D n \u00d7 1 vectors representing all the data in each\ndimension, or as the transposes of the n D \u00d7 1 vectors representing each data point.\nWe\u2019ve also met the covariance matrix, the D \u00d7 D matrix that\u2019s derived from the data matrix:\n\nS =\n\n\uf8eb\n\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\ns11\n...\nsD1\n\n\u00b7 \u00b7 \u00b7\n\n\u00b7 \u00b7 \u00b7\n\n\uf8f6\n\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\ns1D\n...\nsDD\n\n= 1\n\nn \u2212 1\n\nXT X\n\n11\n\n(9)\n\n21012x21012yy on xx on yPC1",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "10"
    },
    "Dealing with high dimensions \u2013 PCA_11": {
        "content": "Figure 8: Projection of a data point x onto a unit vector p.\n\nThe variance in the original axes is s11 and 22. The covariance matrix in our toy example is:\n\n(cid:32)\n\nS =\n\n(cid:33)\n\n9.7 8.0\n8.0 7.7\n\nStep 2: Project data onto a new axis We\u2019ll define the new axis by the unit vector p (Figure 8).\nThe projection of a data point xi onto this axis (its component score) is (as per Equation 1)\n\nti = pT xi = p1xi1 + p2xi2\n\nStep 3: Compute variance in these axes The definition of the variance of the t(1) is:\n\nIf we substitute Equation 10 into this equation we get the following:\n\ns2\nt\n\n= 1\n\nn \u2212 1\n\nn(cid:88)\n\ni=1\n\nt2\ni\n\nn(cid:88)\n\n(p1xi1 + p2xi2)2\n\ni=1\nn(cid:88)\n\ni=1\n\n(p1 p2)\n\n(cid:32) (cid:80)\n(cid:80)\n\ni xi1xx1\ni xi2xx1\n\n(cid:80)\n(cid:80)\n\ni xi1xx2\ni xi2xx2\n\n(cid:33)\n\n(cid:32)\n\n=\n\n(cid:33)\n\np1\np2\n\ns2\nt\n\n= 1\n\nn \u2212 1\n\n= 1\n\nn \u2212 1\n= pT Sp\n\n(10)\n\n(11)\n\n(12)\n\nOur old friend the covariance matrix has reappeared. Although we\u2019ve demonstrated this in 2 dimensions,\nthe equation is still valid in D dimension.\n\nStep 4: Find direction of axis that maximises variance of data projected onto it (1st principal\ncomponent, PC 1) We now have an expression for the variance in the component scores for any\ndirection of p we now want to find the direction of p that maximises that variance. We have a constraint\nthat p is of unit length, so |p| = 1.\nThis is a constrained optimisation problem, which we can solve using Lagrange multipliers and\ndi\ufb00erentiation. We won\u2019t show the details here, but the result is the following equation:\n\nHopefully you recognise this equation. Its solutions are:\n\nSp = \u03bbp\n\n12\n\nllllllllll\u22124\u22122024\u22124\u22122024Haskell         No pref.         JavaTea             No pref.         Coffeelxlt1p\u22126\u22124\u221220246t(1)lllllllllll",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "11"
    },
    "Dealing with high dimensions \u2013 PCA_12": {
        "content": "Figure 9: Projection into the original space.\n\n1. \u03bb = \u03bb1, p = e1, where \u03bb1 is the biggest eigenvalue of S and e1 is the associated eigenvector\n2. \u03bb = \u03bb2, p = e2, where \u03bb2 is the second biggest eigenvalue of S and e2 is the associated\n\neigenvector\n\nWe choose the first principal component p1 to be the eigenvector e1 with the largest eigenvalue \u03bb1.\n\u03bb1 is the variance of the 1st component scores t(1).\n\nStep 5: Interpret Finding the eigenvalues and eigenvectors for our example, we arrive at the first\nprincipal component being:\n\np1 =\n\n(cid:32)\n\n(cid:33)\n\n0.75\n0.66\n\nJava\nCo\ufb00ee\n\n\u03bb1 = s2\nt(1)\n\n= pT\n\n1 Sp1 = 16.5\n\nThe first component score t(1) is the \u201cJava-co\ufb00eeness\u201d of a student.\n\nStep 6: Find the 2nd principal component (PC 2)\nIn 2D our job is already done, since there is\nonly one direction perpendicular to p1, and eigenvectors (and therefore principal components) are\nalways orthogonal to each other. It\u2019s the other eigenvector of S, p2 = e2, with eigenvalue \u03bb2, which is\nthe variance of the 2nd component scores t(2).\nIn D dimensions, the principal components are the D eigenvectors of the D \u00d7 D matrix S. It\u2019s helpful\nto introduce more matrix notation here. We arrange the principal components in the rotation matrix:\n\nP = (cid:16)\n\np1 p2\n\n(cid:17)\n\nStep 7: Quantify what is lost by dimensionality reduction We can reverse the transformation\nfrom the scores to the original data\n\nX = TPT\nIf we drop the 2nd PC from P and the 2nd PC scores from T, we can reconstruct a 1-dimensional\nversion of the original data:\n\n\u02dcX(1) = t(1)pT\n1\n\nWe can see (Figure 9) that the first principal component (the \u201cJava-co\ufb00eeness\u201d) score of a student tells\nus a lot about them \u2013 but how much? Consider the total variance, the sum of the variances of the data:\n\nD(cid:88)\n\ni=1\n\n=\n\ns2\ni\n\nD(cid:88)\n\ni=1\n\n\u03bbi\n\n13\n\nllllllllll\u22122024\u22122024Haskell         No pref.         JavaTea             No pref.         Coffeel\u22126\u22124\u221220246t(1) (PC 1)llllllllll",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "12"
    },
    "Dealing with high dimensions \u2013 PCA_13": {
        "content": "It is equal to the sum of the eigenvalues of the covariance matrix. Thus the fraction of the total variance\n\u201cexplained\u201d by the ith principal component is:\n\nIn our toy example,\n\n\u03bbi\n(cid:80)D\nj=1 \u03bb j\n\n\u03bb1\n\u03bb1 + \u03bb2\n\n=\n\n16.5\n16.5 + 0.61\n\n= 96%\n\nThus we can now be more precise about how much the first principal component (the \u201cJava-co\ufb00eeness\u201d)\nscore of a student tells us about them: 96% of of the variance.\n\n14\n\n",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "13"
    },
    "Dealing with high dimensions \u2013 PCA_14": {
        "content": "References\n\nScottish Government\n\n\u2018Scottish index of multiple deprivation (SIMD) 2016\u2019.\nhttps://www.webarchive.org.uk/wayback/archive/20200117165925/https:\n\n(2016).\n\nURL\n//www2.gov.scot/SIMD\n\n15\n\n",
        "lecture": "Dealing with high dimensions \u2013 PCA",
        "page": "14"
    },
    "empty_0": {
        "content": "Inf2 \u2013 Foundations of Data Science 2021\nTopic: Intro to supervised learning: training, testing and\nvalidation: Nearest neighbours\n\nHiroshi Shimodaira*, Iain Murray*, David C. Sterratt\n\n14th November 2021\n\nFurther reading (not examinable):\nHastie et al. (2009) pp 463\u2013471\n\n1 Video: Classi\ufb01cation\n\nSupervised learning of classi\ufb01cations Suppose a bank has data on previous customers it has given\nloans to, including variables such as their income, housing status and employment status. Each of these\nsets of variables \u2013 also referred to as feature vectors \u2013 has a label, indicating whether the customer\ndid or didn\u2019t pay back their loan. The bank might want to predict whether a new customer will be able\npay back a bank loan from their features, i.e. to predict whether they belong to the class of customers\nwho paid or the class of customers who didn\u2019t pay. This is an example of the classi\ufb01cation problem,\nwhich we de\ufb01ne as the problem of predicting the correct label for an unlabelled input feature vector.\n\nSupervised and unsupervised learning Classi\ufb01cation is an example of a supervised learning pro-\ncess. In a supervised learning process, there is a training set of data in which each data item has\na number of features and a known label. The goal of supervised learning is to predict the label of\nan item that has not been previously seen from its features. In contrast, in unsupervised learning\nprocesses, the training set does not contain any labels, and the goal is to learn something about the\nstructure of the data. We will return to unsupervised learning in a later topic.\n\nVisualising the classi\ufb01cation problem To visualise the classi\ufb01cation problem, we\u2019ll use a toy\nexample: the fruit data set, collected by Iain Murray (Murray, 2006). He bought pieces of fruit and\nmeasured their height and width (features) and noted the type of fruit (the label). Figure 1 visualises\n\n*These lecture notes are based on the ones written by Iain Murray and Hiroshi Shimodaira for the Inf2B course, which\n\nran until the academic year 2019/20.\n\n1\n\n\f",
        "lecture": "empty",
        "page": "0"
    },
    "Randomness, sampling and simulation_0": {
        "content": "Inf2 \u2013 Foundations of Data Science 2021\nTopic: Randomness, sampling and simulation\n\nDavid C. Sterratt\n\n20th November 2021\n\nRecommended reading:\n\n\u2022 Computational and Inferential Thinking, Chapter 9\n\n\u2022 Computational and Inferential Thinking, Chapter 10\n\n\u2022 Computational and Inferential Thinking, Chapter 14\n\n\u2022 Modern Mathematical Statistics with Applications, Sections 6.1 and 6.2\n\n1 Video: Introduction to statistical inference\n\nStatistics The German word Statistik arose in the 18th century and originally referred to \u201cdata about\nthe state\u201d (country). The \ufb01rst use of \u201cstatistical\u201d in the English language was in 1791 in the Statistical\nAccount of Scotland. Sir John Sinclair, an elder in the Church of Scotland, sent a questionnaire to\nministers in every parish (church district) in Scotland. The questionnaire asked many questions about\nagriculture, industry, economics, employment, poverty and education, as well as \u201cThe state of the\nmanners, the morals, and the religious principles of the people\u201d. In fact empires and dynasties have\nbeen collecting data about population and trade for much longer than this, going back to the Han\ndynasty in China and the Roman Empire.\n\nInferential statistics This use of the word statistics above relates to the whole population. In contrast,\nback in the topic on Statistical Preliminaries, we looked at the di\ufb00erence between a sample and a\npopulation. We also considered a number of statistics that could apply to the population and to the\nsample: the mean, variance, standard deviation and median. Inferential statistics is the process of\ndrawing conclusions about quantities that are not observed (Gelman et al., 2004).\n\n1\n\n\f",
        "lecture": "Randomness, sampling and simulation",
        "page": "0"
    },
    "Randomness, sampling and simulation_1": {
        "content": "Figure 1: Uncertainty in regression line. The best estimate regression line (solid) and the lines at the\nedge of the 95% confidence interval (dashed lines). Note this plot is simplified, since the uncertainty in\nthe intercept is not represented.\n\nOne example of an inferential statistics task is estimation of a population statistic from a sample of that\npopulation. For example, suppose we\u2019ve weighed a sample of 10 wild cats from a population of 400.\nWe know what the sample mean and sample standard deviation is. On the basis of this information,\nwhat is the best estimate of the population mean (point estimation), and how confident can we be in\nthat estimate (confidence interval estimation)?\n\nInferential statistics has been around for much longer than the word \u201cstatistics\u201d. The 9th-century\nbook \u201cManuscript on Deciphering Cryptographic Messages\u201d written in Arabic by Al-Kindi (educated\nin Baghdad) shows how to decipher encrypted messages by frequency analysis, i.e. counting the\nfrequency of particular letters.\n\nInferential statistics tasks\nInferential statistics can seem like a toolbox full of tools with confusing\nnames such as \u201cstandard error of the mean\u201d, t-test, \u03c72 test, bootstrap, and a confusing set of rules about\nwhat to use each tool for. We\u2019re going to try to give you an idea of what task each tool is useful for,\nand how it works. There are three main tasks we will consider:\n\n1. Estimation\n\n2. Hypothesis testing\n\n3. Comparing two samples (A/B testing)\n\nEstimation We\u2019ve already given one example of estimating an unobserved quantity (the population\nmean). Another example of an unobserved quantity is linear regression coefficients. We already know\nhow to find (point) estimates of them, using the formulae we covered earlier in the course. But in part\nof the course we will learn how to estimate the confidence intervals around the point estimates, which\nwill tell us how much uncertainty there is in our estimates. For example, we\u2019ll be able to say that we\nestimate the mean weight of squirrels in the population to be 320 \u00b1 16g, with 95% confidence, i.e. in\nthe confidence interval [304, 336]g. In a linear regression of the weight of a sample of squirrels on\ntheir length (Figure 1), we will be able to say that the best estimate of the slope of the regression line is\n3.35 g/mm, but we are 95% confident that the slope lies in the interval [2.32, 4.38] g/mm.\n\n2\n\n210220230Length (mm)300350400Weight (g)",
        "lecture": "Randomness, sampling and simulation",
        "page": "1"
    },
    "Randomness, sampling and simulation_2": {
        "content": "Hypothesis testing\nIn hypothesis testing, we are trying to ascertain which of two or more competing\ntheories are the best explanation of the data. For example, in 1965 a court case was brought against the\nstate of Alabama (Swain versus Alabama, 1965) due to there being no Black members of the jury in a\ntrial. Part of the case concerned the fact that at one stage of the jury selection, 8 Black people were\nchosen for a jury panel of 100 people, but the fraction of Black people in the population was 26%. Our\nquestion is \u201cIs the jury selection system biased against Black people?\u201d.\n\nComparing two samples (also known as A/B testing) Here we have two samples that have been\ntreated di\ufb00erently, and we want to either test if the groups are di\ufb00erent, or estimate how di\ufb00erent they\nare. For example, to find out the e\ufb00ectiveness of a vaccine, we select a sample of volunteers from the\npopulation randomly, divide them randomly into two groups, give the vaccine to one group (Treatment\ngroup) and give the other group a placebo (Control Group). In the vaccine group 3 volunteers catch\nthe disease, but in the placebo group 95 volunteers catch the disease. Is the vaccine e\ufb00ective? How\nmuch would we expect the vaccine to cut the risk of catching the disease if we give it to the whole\npopulation?\nIn the context of user testing, often in web applications, this is called A/B testing. A famous example\nwas at Amazon, where a developer had the idea of presenting recommendations as you add items to\na shopping cart (Kohavi et al., 2007). Amazon managers forbid the developer to work on the idea,\nbut the developer disobeyed orders and ran a controlled experiment on users, by splitting them into\ntwo groups (\u201cA\u201d and \u201cB\u201d), one which had recommendations shown and one which didn\u2019t. The group\nwhich had recommendations shown bought more, and displaying recommendations quickly became a\npriority for Amazon.\n\nTwo approaches to statistical inference We are going to learn a number of techniques for under-\ntaking point and interval estimation, hypothesis testing and comparing samples. We will also think\ncarefully about the interpretation of these techniques. There are two main approaches to undertaking\nstatistical inference tasks:\n\n1. Statistical simulations. Here we use repeated random sampling to carry out the statistical\ninference procedures. The advantages of statistical simulation procedures are they often require\nfewer assumptions about the data and/or hypothesis, and they require somewhat less theory to\nunderstand. However, they can be compute-intensive, and care is still needed in their use.\n\n2. Statistical theory.. Here we use the properties of various well-known theoretical distributions\nto draw inferences about our data. We need to check that the assumptions behind the distribution\nmatch the statistical question we are trying to answer. For example, a distribution of delays\nto \ufb02ights is likely to be highly right-skewed, so we shouldn\u2019t assume a normal distribution\nwhen dealing with it. Typically, the process is not compute-intensive: very often it amounts to\narithmetic and then reading of a quantity from a distribution table. These procedures come as\nstandard in a number of stats packages, including R and Python\u2019s statsmodels.\n\nA number of fundamental concepts underpin both the statistical theory and statistical simulations.\n\nPlan for this semester The plan for the statistical inference topics in this semester will be:\n\n1. Fundamental theory (the rest of this topic):\n\n\u2022 We\u2019ll learn how we can use statistical simulations to generate samples from a model and\n\ncompute statistics for each of these samples to give a sampling distribution.\n\n3\n\n",
        "lecture": "Randomness, sampling and simulation",
        "page": "2"
    },
    "Randomness, sampling and simulation_3": {
        "content": "Figure 2: Histograms of 1,000 samples taken from normal, uniform and exponential distributions.\n\n\u2022 Learn about the distribution of the mean of repeated samples from a model. This will lead\nus to the central limit theorem, which can help us to estimate the uncertainty in our estimate\nof the mean, i.e. confidence intervals, and the law of large numbers, which also helps with\nestimation.\n\n2. Estimation (next lecture)\n\n3. Hypothesis testing (third lecture)\n\n4. An interlude - Logistic regression (fourth lecture)\n\n5. Comparing two samples with statistical simulations (fifth lecture)\n\n2 Video: Sampling, statistics and simulations\n\nSampling A prerequisite for statistical simulations is being able to sample from probability distribu-\ntions and from sets of discrete items, including observed data.\n\nRandom sample\nIn a random sample of size n from either a continuous probability distribution, or a\nfinite population of N items the random variables X1, . . . , Xn comprising the sample are all independent\nand all have the same probability distribution.\n\nSampling from probability distributions You should be familiar with sampling from random\nnumber generation. A standard random number generator produces numbers within an interval (e.g. [0,\n1]) with uniform probability for each number, i.e. it samples from a uniform distribution. We can\ndemonstrate the distribution of a standard random number generator by drawing many samples and\nplotting a histogram (Figure 2). We adapt these functions to sample from any univariate distribution,\ne.g. a normal distribution or an exponential distribution (Figure 2).\n\nSampling from a set of discrete items We can also sample from a population of discrete items. We\ncan select n items from a set of N items at random either without replacement or with replacement.\nIf we sample without replacement, it is as though we are pulling items of various types (e.g. coloured\nballs) at random out of a bag, and not replacing them. We can only sample up to N items, and also,\nas we remove items from the bag, the probabilities of drawing a particular type (colour) changes. If\n\n4\n\n01x02040FrequencyUniform2.50.02.5x050100FrequencyNormal05x0100200FrequencyExponential",
        "lecture": "Randomness, sampling and simulation",
        "page": "3"
    },
    "Randomness, sampling and simulation_4": {
        "content": "we sample with replacement, we put the item back in the bag, before making our next choice \u2013 we\ncan carry on doing this for ever. We could construct an algorithm for random sampling either with or\nwithout replacement from a uniform random number generator, but these functions are provided in\npackages such as numpy.random.choice in Python.\n\nA particular application of sampling from a set of discrete items is creating a sample of a larger data\nset.\n\nNon-random samples from a population We can also imagine ways of sampling that are not\nsystematically random. For example, we might have a list of the daily takings in a restaurant. We could\ntake the first n days. But suppose that the dataset has been sorted in terms of takings? We would then\nhave days with low takings at the start of the list, so the statistics of the sample would not resemble the\nstatistics of the population. We could try taking every 7th day in the list \u2013 but if the list is in date order\nwe will always be sampling from one day of the week, e.g. Mondays. Random sampling ensures that\nwe don\u2019t have this type of problem.\n\nSamples of convenience When we are collecting data, it might be tempting to sample from the data\nthat we can collect conveniently. For example, a polling company may find it easier to contact people\nwho have more time to answer the phone, which may tend to be retired people. If we don\u2019t correct for\nthis sort of bias, it\u2019s called a sample of convenience. One way of combating convenience sampling\nis stratified sampling, in which the sampling is targeted so that the proportions of attributes of the\nsample matches the proportions in the population.\n\nDefinition of a statistic Before going further, it\u2019s helpful to have the definition of statistic: \u201cA\nstatistic is any quantity whose value can be calculated from sample data.\u201d(Modern Mathematical\nStatistics with Applications 6). We probably recognise the mean, variance and median as statistics\nby this definition. But we\u2019ve also derived other quantities from sample data, such as the correlation\ncoefficient and regression coefficients \u2013 they are also statistics. We will follow Modern Mathematical\nStatistics with Applications and denote a statistic using an uppercase letter, to indicate that it is a\nrandom variable, since its value depends on the particular sample selected. E.g. X represents the mean\nand S 2 the variance.\n\nSimulations and sampling Before considering inferential statistics proper, we will focus on running\nstatistical simulations, i.e. using a computer program to make predictions from probabilistic models\nof real-world processes. For example, the probabilistic model of tossing a coin multiple times is that\nthe tosses are independent and that the probability of a head is 1/2 (or perhaps another value, if with\nthink the coin is loaded). The statistical simulation generates a sequence of heads and tails.\n\nTo do this we need to decide on:\n\n\u2022 The statistic of interest (X, S , etc.)\n\n\u2022 The population distribution (e.g. normal with particular mean and variance) or set of discrete\n\nitems\n\n\u2022 The sample size (denoted n)\n\n\u2022 The number of replications k\n\nThe simulation procedure is then:\n\n5\n\n",
        "lecture": "Randomness, sampling and simulation",
        "page": "4"
    },
    "Randomness, sampling and simulation_5": {
        "content": "Figure 3: Results of statistical simulations of the panel size in Swain versus Alabama (1965). The blue\nhistogram shows how many of 10 000 simulations produced jury panels of 100 with the given number\nof Black people on them. The red dot indicates the number of Black jurors in Swain versus Alabama\n(1965).\n\n1. For i in 1, . . . , k\n\n(a) Sample n items from the population distribution or set of discrete items\n\n(b) Compute and store the statistic of interest for this sample\n\n2. Generate a histogram of the k stored sample statistics\n\nExample of hypothesis testing using a simulation experiment To demonstrate the utility of the\nstatistical experiment we\u2019ve introduced, let\u2019s look again at the example in which 26% of the population\nis Black and 8 Black people are selected to be on a jury panel of 100 people. The null hypothesis H0 is\n\u201cThe jury panel was chosen at random from the population\u201d. We can map the null hypothesis onto the\ngeneral framework above as follows:\n\n\u2022 The statistic of interest is T0, the number of Black people in a sample of n = 100 panel members\n\n\u2022 The population distribution is a Bernoulli distribution with the sample space Black, Non-Black\n\nin which p(Black) = 0.26.\n\n\u2022 The sample size is n = 100\n\n\u2022 The number of replications k = 10 000\n\nWe follow the procedure described in the previous section to give the results shown in Figure 3. Coding\nthis up will be an exercise for you in the Labs. We can see that none of the 10 000 simulations of the\nnull hypothesis produced a jury with 8 members, suggesting that we should reject the null hypothesis in\nfavour of an alternative one. This looks like a clear-cut case; in the topic on Hypothesis testing, we\u2019ll\nconsider in more detail how to interpret the results when the data is less distinct from the simulations.\n\nDeriving the sampling distribution Note that in this example, we didn\u2019t have to go to the trouble\nof running a simulation experiment. We might have noticed that the total number of Black people will\nbe distributed according to a binomial distribution with n = 100 and p = 0.26.\n\n6\n\n10203040Number of black people on panel05001000FrequencySimulation010203040Number of black people on panel0.000.05ProbabilityBinomial distribution",
        "lecture": "Randomness, sampling and simulation",
        "page": "5"
    },
    "Randomness, sampling and simulation_6": {
        "content": "3 Video: Distributions of small samples statistics from probabil-\n\nity distributions\n\nExample of sampling from probability distributions\nIn the previous example, we\u2019ve sampled a\ntotal number of successes from a Bernoulli distribution. We\u2019ll now look at what happens when we\nsample the mean, standard deviation and median from the normal, uniform and exponential distributions\nby running the following simulations:\n\n\u2022 Statistics of interest: mean X, standard deviation S and median \u02dcX\n\n\u2022 Population distribution: Normal distribution with mean 0 and variance 1, Uniform distribution\n\non [0, 1], Exponential distribution p(x) = e\u2212x.\n\n\u2022 Sample size n = 10\n\n\u2022 Number of replications k = 10, 000\n\nFigure 4. There are a number of points to notice about this plot:\n\nSample mean (first column), all distributions The distribution of the mean is narrower than the\noriginal distribution in every case. This is because some of the variability in the individual\nsamples is averaged out. The standard deviation of this distribution is called the standard error\nof the mean.\n\nSample mean of normal distribution The distribution looks to be normal \u2013 it turns out that this is\n\neasy to prove.\n\nSample mean of uniform distribution The distribution is symmetric and looks to be near-normal.\n\nSample mean of exponential distribution The distribution is clearly skewed, but less so than the\n\noriginal exponential distribution.\n\nSample variance (second column) All these distributions are skewed, re\ufb02ecting the fact that it\u2019s very\nunlikely to get 10 samples that are all very close together, and therefore have low variance. It\nturns out that there is a theoretical distribution (the \u03c72 distribution) that describes the shape of\nsample variance from the normal distribution.\n\nMedian (third column) The main point to draw from this column is that we can use the simulation\nmethod to produce a distribution for any statistic, regardless of how easy it would be to calculate\na theoretical distribution for it.\n\nAs we will see later, we could generate the sampling distribution of the mean and the variance\nanalytically rather than by simulation. However, it is not always possible to compute sampling\ndistributions of the desired statistics analytically, but we can always run statistical simulations.\n\n4 Video: The distribution of the sample mean of large samples\n\nThe distribution of the sample mean A particularly common statistic of interest is the sample\nmean. It therefore makes sense to understand how the distribution of the sample mean depends on the\ndistribution from which we sample and the number of samples we take.\n\n7\n\n",
        "lecture": "Randomness, sampling and simulation",
        "page": "6"
    },
    "Randomness, sampling and simulation_7": {
        "content": "Figure 4: Sampling distribution generated by 10,000 simulations of the mean x, variance s2 and median\n\u02dcx of 10 samples drawn from a normal distribution (top row), uniform distribution (middle row) and\nexponential distribution (bottom row).\n\n8\n\n101x0250500Frequency0.02.5s20500FrequencyNormal101x0250500Frequency0.250.500.75x0250500Frequency0.050.100.15s20200400FrequencyUniform0.250.500.75x0200400Frequency12x0250500Frequency010s2010002000FrequencyExponential02x0250500Frequency",
        "lecture": "Randomness, sampling and simulation",
        "page": "7"
    },
    "Randomness, sampling and simulation_8": {
        "content": "Figure 5: Distributions of means from samples of size n = 1000 (top row) and n = 10000 (bottom row)\ndrawn from the normal, uniform and exponential distributions shown in Figure 2. The blue histograms\nshow the histograms obtained from k = 2000 simulations. The orange curves are normal distributions\nwith mean equal to the mean of the original distribution and variance \u03c3X equal to \u03c32/n, where \u03c32 is\nthe variance of the original distribution.\n\n9\n\n0.10.00.1x010Relative frequencyNormal n=10000.450.500.55x02040Uniform n=10000.91.01.1x0510Exponential n=10000.10.00.1x02040Relative frequencyNormal n=100000.450.500.55x0100Uniform n=100000.91.01.1x02040Exponential n=10000",
        "lecture": "Randomness, sampling and simulation",
        "page": "8"
    },
    "Randomness, sampling and simulation_9": {
        "content": "We\u2019ve already seen in Figure 4 that the sampling distribution of the mean of 10 items from a normal\ndistribution is itself a normal distribution, though with smaller variance. However, the sample mean\ndistributions for an exponential distribution in our simulation, was not normal. We can repeat the\nsimulation experiments for the three distributions, but with larger sample sizes of n = 1000 and\nn = 10000 (Figure 5). What we see is remarkable: the distributions of the sample means are all normal,\nregardless of whether they came from a normal, uniform or exponential distribution. Perhaps less\nremarkably, we also see that as the sample size gets larger the distributions get narrower.\n\nThese simulations and observations give us the intuition for two very important statistical laws that\napply to many non-normal distributions, as well as normal ones:\n\n\u2022 The Central Limit Theorem\n\n\u2022 The Law of Large Numbers\n\nCentral Limit Theorem Here is an informal statement of the Central Limit Theorem (CLT):\n\nThe distribution of the mean [or sum] of a random sample drawn from any distribution\nwill converge on a normal distribution. In the case of the sample mean, its expected value\nis the same as the mean of the population distribution, and its expected variance is a factor\nof n lower than the population variance. In the case of the sample sum, its expected value\nis the same as the product of the sample size n and the expected value of the distribution,\nand its expected variance is n times the variance of the population distribution.\n\n\u221a\n\nn.\n\n= \u03c3/\n\nWe denote the expected variance of the mean \u03c32\nand we call the standard deviation of the mean \u03c3X, or\nX\nthe standard error in the mean, often abbreviated as SEM. It\u2019s important to note that the SEM is not\nthe same as the standard deviation of the original distribution. According to the statement above, an\nestimate of the SEM is \u02c6\u03c3X\nWe can verify that this statement holds in the case of sampling a mean in Figure 5 by computing the\nmeans and SEM from the simulations and comparing with the expected values of \u00b5 (population mean)\nand \u03c3X\nThe Swain versus Alabama jury selection example demonstrates the CLT applied to a total T0 = (cid:80)n\ni=1 Xi,\nwhere Xi = 1 indicates a Black member of the population was selected, and Xi = 0 indicates non-Black.\nThe distribution is a Bernoulli distribution with population mean \u00b5 = p = 0.26, the probability of\npicking a Black person. We can see from Figure 3 that the mean of the total is n\u00b5 = 100 \u00d7 0.26 = 26,\n\u2248 n\u03c32 = 19.24, as expected for a Bernoulli distribution, giving\nand the variance is approximately \u03c32\nT0\na standard deviation of 4.38. Furthermore, the distribution is approximately normal.\n\n= \u03c3/\n\nn.\n\n\u221a\n\nThe law of large numbers Here is an informal statement of the law of large numbers:\n\nIn the limit of infinite n, the expected value of the sample mean X tends to the population\nmean \u00b5 and the variance of the sample mean X tends to 0.\n\nNote that sometimes the law of large numbers is referred to as the \u201claw of averages\u201d. This can lead to\nconfusion. The law of averages is sometimes called the \u201cGambler\u2019s fallacy\u201d, i.e. the idea that after\na run of bad luck, the chance of good luck increases. If the events that are being gambled on are\nindependent of each other (e.g. successive tosses of the same coin), the probability of a head will be\nthe same regardless of how many tails have preceded it.\n\n10\n\n",
        "lecture": "Randomness, sampling and simulation",
        "page": "9"
    },
    "Randomness, sampling and simulation_10": {
        "content": "In the second row of Figure 5 we can see that the distribution for n = 10000 is narrower than the\ndistribution for n = 1000, and that the sample means converge on the population means. The law of\nlarge numbers says that we could, in principle, continue this process by choosing an n as large as we\nwould like to make the variance as small as desired.\n\nFormal statement of the central limit theorem (Modern Mathematical Statistics with Applica-\ntions 6.2) Let X1, . . . , Xn be a random sample from a distribution with mean \u00b5 and variance \u03c32. Then,\nin the limit n \u2192 \u221e the standardised mean ((X \u2212 \u00b5)/(\u03c3/\nn\u03c3))\nhave a normal distribution. That is\n\nn)) and standardised total ((T0 \u2212 n\u00b5)/(\n\n\u221a\n\n\u221a\n\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\nX \u2212 \u00b5\n\u221a\nn\n\u03c3/\n\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\n\u2264 z\n\nlim\nn\u2192\u221e\n\nP\n\n= P(Z \u2264 z) = \u03a6(z)\n\nand\n\nP\n\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\nlim\nn\u2192\u221e\n\nT0 \u2212 n\u00b5\n\u221a\nn\u03c3\n\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2264 z\nwhere \u03a6(z) is the cumulative distribution function (cdf) of a normal distribution with mean 0 and s.d. 1.\n= \u00b5\nThus, when n is sufficiently large, X has an approximately normal distribution with mean \u00b5X\n= n\u00b5 and\nand variance \u03c32\nX\n= n\u03c32. We can also say that the standardised versions of X and T0 are asymptotically\nvariance \u03c32\nT0\nnormal.\n\n= \u03c32/n and the distribution of T0 is approximately normal with mean \u00b5T0\n\n= P(Z \u2264 z) = \u03a6(z)\n\nFormal statement of the (weak) law of large numbers\nApplications 6.2)\nLet X1, . . . , Xn be a random sample from a distribution with mean \u00b5 and variance \u03c32. As the number of\nobservations n increases, the expected value of the sample mean remains E[X] = \u00b5, but the expected\nvariance V[X] = E[(X \u2212 \u00b5)2] \u2192 0. We say that \u201cX converges in mean square to \u00b5\u201d.\nMore formally, the probability that the di\ufb00erence between the sample mean and population mean is\ngreater than an arbitrary value \u03b5 is\n\n(Modern Mathematical Statistics with\n\nP(|X \u2212 \u00b5| \u2265 \u03b5) \u2264\n\n\u03c32\nn\u03b52\n\nfor any value \u03b5. Thus, as n \u2192 \u221e, the probability approaches 0, regardless of the value of \u03b5.\n\nA proof of this statement relies on Chebyshev\u2019s inequality, and can be found in Modern Mathematical\nStatistics with Applications 6.2.\n\nNote that this is the statement of the weak law of large numbers. There is also a strong law, which\nhas somewhat more stringent requirements on convergence. All distributions that obey the strong law\nalso obey the weak law, but some distributions only obey the weak law and some obey neither law.\nA discussion of this topic is beyond the scope of this course; the distributions that do not obey the\ndistribution tend to be \u201cweird\u201d, e.g. having infinite variance.\n\nAppendix\n\nFrequentist versus Bayesian statistics You may have heard of the di\ufb00erence between Frequentist\nand Bayesian statistics. The two systems have di\ufb00erent philosophical bases, but, in simpler cases, often\nend with similar results. Roughly speaking, the di\ufb00erences between the two are:\n\n11\n\n",
        "lecture": "Randomness, sampling and simulation",
        "page": "10"
    },
    "Randomness, sampling and simulation_11": {
        "content": "Frequentist The population is a fundamental concept. There is just one possible value of the popula-\ntion mean and variance, i.e. the one that exists in the population. In estimation, we are trying to\nestimate these quantities, and in hypothesis testing, we are trying to compare our sample with\nthis population.\n\nBayesian A fundamental concept is the model of the likelihood of the data given parameters (such\nas the mean). The parameters themselves are uncertain. Conceptually, the population itself\nis generated from the model, so a number of combinations of parameters and luck may have\ngenerated the particular value of (say) the mean observed in a population. Before we have\nseen any data, we have an initial idea about the distribution of the parameters (the prior). The\ninference process involves using the data to update this prior distribution to give a distribution of\nthe parameters given the data.\n\nFor around a century, there has been controversy about which approach is best. Broadly speaking, we\nwill be using Frequentist approaches in this course. At the level we are working at here, it will give\nvery similar results to Bayesian approaches. The important thing is to understand the meaning and\ninterpretation of our inference.\n\nReferences\n\nGelman, A., Carlin, J. B. et al. (2004). Bayesian Data Analysis. Chapman & Hall/CRC, second ed.\n\nKohavi, R., Henne, R. M. et al. (2007). \u2018Practical guide to controlled experiments on the web: Listen\nto your customers not to the HiPPO\u2019. In P. Berkhin, R. Caruana, X. Wu and S. Ga\ufb00ney, eds.,\nKDD-2007 Proceedings of the thirteenth ACM SIGKDD international conference on knowledge\ndiscovery and data mining, pp. 959\u2013967. Association of Computing Machinery, New York, USA\n\n12\n\n",
        "lecture": "Randomness, sampling and simulation",
        "page": "11"
    },
    "Con\ufb01dence intervals_0": {
        "content": "Inf2 \u2013 Foundations of Data Science 2021\nTopic: Con\ufb01dence intervals\n\nDavid C. Sterratt\n\n17th May 2022\n\nRecommended reading:\n\n\u2022 Computational and Inferential Thinking, Chapter 13\n\n\u2022 Modern Mathematical Statistics with Applications, Sections 8.1\u20138.3 and 8.5\n\n1 Video: Principle of con\ufb01dence intervals\n\nIllustration: con\ufb01dence intervals for the mean From the Central Limit Theorem we know that for\nlarge samples, the distribution of the mean is normal, and that the estimated standard error of the mean\nshould be close to the standard error in the mean. We can then ask \u201cif we looked at an interval around\nour estimate for the mean, how often would the true value be contained in that interval\u201d?\n\nFigure 1 gives an illustrated answer to this question. Each blue horizontal line corresponds to one\nsample of size n from a population, and shows a range of estimates for the population mean based on\nthat sample \u2013 in other words a con\ufb01dence interval. We can see that the true value of the mean (black\nvertical line) is contained in most of the intervals, but not all of them.\n\nSize of con\ufb01dence interval We have chosen the length of the intervals to ensure that, if we carried\non estimating the mean and the interval, about 95% of intervals would contain the true mean. To\ndetermine this length, we use the z critical values of the normal distribution (Figure 2). We de\ufb01ne the\nz critical value z\u03b1 as the value of z in a normal distribution which has the area \u03b1 under the curve to its\nright. If we want the intervals to contain the true mean 95% of the time, we need to make sure that the\nmean is within the central 95% of the distribution. This implies that we need 2.5% of the area under\nthe curve to the right of the upper bound, so we look up z0.025 in a statistical table or a function in a\nstats package and \ufb01nd that z0.025 = 1.96 \u2013 we will show how to do this later. The z critical value of 1.96\n\n1\n\n\f",
        "lecture": "Con\ufb01dence intervals",
        "page": "0"
    },
    "Con\ufb01dence intervals_1": {
        "content": "Figure 1: Principle of confidence intervals. We repeat a simulation using a sample size of n = 100 to\nestimate the sample mean of a normal distribution with mean 0 and standard deviation 1. The black\nvertical line indicates the true mean, the blue dots indicate the sample means, and the blue horizontal\nlines indicate the 95% confidence intervals obtained in each of the 20 repetitions. It can be seen that\n19 of the confidence intervals do contain the population mean, but one of them does not.\n\nFigure 2: Confidence intervals of a normal distribution. The intervals containing various amounts\nof probability mass under a normal distribution are shown. The 95% confidence interval (blue) is\n[\u22121.96, 1.96] and has 2.5% of the probability mass in each tail. The 80% confidence interval is\n[\u22121.28, 1.28]. The amount of probability mass contained in one standard deviation is 68%. In general\nfor a confidence interval of 100(1 \u2212 \u03b1)%, the upper and lower boundaries are determined by the\nz critical value z\u03b1/2. E.g. with the 95% confidence interval \u03b1 = 0.05 and there is 2.5% of the area of\nthe curve above the upper boundary of the confidence interval.\n\n2\n\n0.40.20.00.20.4 and CIs for 05101520Repetition",
        "lecture": "Con\ufb01dence intervals",
        "page": "1"
    },
    "Con\ufb01dence intervals_2": {
        "content": "Figure 3: Concept of the z critical value. Top: z\u03b1 is the value of z such in a normal distribution such\nthat the area under the curve to the right of the z\u03b1 (green) is equal to \u03b1. i.e. \u03b1 = (cid:82) \u221e\np(z)dz. Bottom:\nz\u03b1\nthe blue curve shows the cumulative distribution function \u03a6(z) = (cid:82) z\n\u2212\u221e p(z)dz. The orange curve shows\nthe \u201csurvival function\u201d sf(z) = 1 \u2212 \u03a6(z). The survival function of z is exactly the area to the right of z\nunder the pdf. Therefore we want to look up the inverse survival function to determine z\u03b1 from \u03b1, as\nindicated by the green lines.\n\ntells us that the length of the lines on the side of each estimate of the mean should all be 1.96 times the\nstandard error of the mean (SEM).\n\nWe may want to be more or less certain of whether the mean is contained in a confidence interval. In\nthis case we can look up the z critical value for our chosen level of confidence. We can also decide to\nexpress the confidence interval in terms of the multiples of the SEM. For example confidence intervals\nof plus or minus one SEM correspond to a 68% confidence interval.\n\nReminder\nIt is worth remembering that these simulations are artificial in the sense that we can repeat\nmany samples. In real life we only get one sample, which does or does not contain the true value \u2013 but\nwe don\u2019t know.\n\nLooking up a z critical value (added after 2021/22 lectures) To look up a z critical value, you can\nuse the python scipy package. For example to find z0.2 you would use:\n\nfrom scipy .stats import norm\nalpha = 0.2\nprint (norm.isf(alpha ))\n\nThe function name isf stands for inverse survival function. As illustrated in Figure 3, it\u2019s the inverse\nof one minus the cumulative distribution function (cdf).\n\n3\n\n432101234z0.00.20.4p(z)432101234z0.00.51.0=0.2z0.2=0.84(z)1(z)",
        "lecture": "Con\ufb01dence intervals",
        "page": "2"
    },
    "Con\ufb01dence intervals_3": {
        "content": "2 Video: Definition of confidence intervals\n\nDefinition of confidence intervals We define a confidence interval as an interval ( \u02c6\u03d1\u2212a \u02c6\u03c3 \u02c6\u03d1, \u02c6\u03d1+b \u02c6\u03c3 \u02c6\u03d1)\nthat has a specified chance 1 \u2212 \u03b1 of containing the parameter, and where the positive numbers a and b\ndefining the lower and upper bounds of the interval depend on \u03b1. The smaller \u03b1 is, the larger the values\nof a and b can be for the statement to hold. A common value for \u03b1 is 0.05 (i.e. 5%), which gives a 95%\nconfidence interval. However, we could set \u03b1 = 0.2, which would give a narrower 80% confidence\ninterval. Often a and b are equal, but we have given them distinct symbols for full generality.\n\nWe can express the definition in terms of a probability statement as follows:\n\n(cid:16) \u02c6\u03d1 \u2212 a \u02c6\u03c3 \u02c6\u03d1 < \u03d1 < \u02c6\u03d1 + b \u02c6\u03c3 \u02c6\u03d1\n\n(cid:17) = 1 \u2212 \u03b1\n\nP\n\n(1)\n\nIn this probability statement, the upper and lower bounds of the interval are random variables, since\nthey are based on the estimators and the estimated standard error, which are themselves random\nvariables derived from the sample.\n\nExpression in terms of random variable in fixed interval We can rearrange the definition of the\nconfidence interval in terms of a standardised variable ( \u02c6\u03d1 \u2212 \u03d1)/ \u02c6\u03c3 \u02c6\u03d1:\n\n(cid:32)\n\u2212b <\n\nP\n\n(cid:33)\n\n< a\n\n\u02c6\u03d1 \u2212 \u03d1\n\u02c6\u03c3 \u02c6\u03d1\n\n= 1 \u2212 \u03b1\n\n(2)\n\nBecause this standardised variable is derived from the sample, it fits our definition of a statistic.\nFurthermore, it is composed of two statistics, the estimator \u02c6\u03d1 and the estimated standard error \u02c6\u03c3 \u02c6\u03d1.\n\n3 Video: Method of estimating confidence interval of the mean\n\nof a large sample\n\nMethods of estimating confidence intervals There are two main methods of estimating confidence\nintervals:\n\n1. Under some assumptions about the distribution of the data Xi and the number of samples n we\ncan derive the distribution of ( \u02c6\u03d1 \u2212 \u03d1)/ \u02c6\u03c3 \u02c6\u03d1, which will then tell us the values of \u2212b and a at the\n100\u03b1/2th centile and the 100(1 \u2212 \u03b1/2)th centiles.\n\n2. More generally we can use a type of statistical simulation called a bootstrap estimator to derive\n\nthe confidence interval.\n\nWe\u2019ll demonstrate the first approach by continuing with our simplified example of a normal distribution\nwith known parameters. In the following section we\u2019ll then cover the bootstrap estimator.\n\nExample: confidence interval for the mean of a normal distribution with known variance\nIn\nthe example of sampling from a normal distribution introduced in the last video, we know the population\nn. Because the population\nvariance \u03c3, and by definition, the standard estimate of the mean is \u02c6\u03c3 \u02c6\u03d1\nvariance \u03c3 is known, it\u2019s not a random variable, and therefore the SEM \u02c6\u03c3 \u02c6\u03d1 isn\u2019t a random variable\neither. The standardised variable in Equation 2 is therefore\n\n= \u03c3/\n\n\u221a\n\n\u02c6\u03d1 \u2212 \u03d1\n\u02c6\u03c3 \u02c6\u03d1\n\n= X \u2212 \u00b5\n\u221a\nn\n\u03c3/\n\n4\n\n(3)\n\n",
        "lecture": "Con\ufb01dence intervals",
        "page": "3"
    },
    "Con\ufb01dence intervals_4": {
        "content": "Figure 4: Distribution of time from making a reservation to the reservation time (\u201cpreparation time\u201d )\nin restaurants using the \u201cair\u201d booking system in Japan in the period January 2016\u2013April 2017.\n\nand only contains one random variable, X. This makes it quite easy to deal with, since we know that\nthis distribution is a standard normal distribution, so we can define a 95% confidence interval by setting\na and b to be values at which the cumulative distribution function (cdf) is equal to 2.5%(= \u03b1/2) and\n97.5%(= 1 \u2212 \u03b1/2). In this case the values a = b = 1.96 satisfy these conditions. Generally we set a\nand b symmetrically, so that there is equal weight in the \u201ctails\u201d of the distribution (Figure 2).\n\nConfidence intervals for the mean of a large sample The central limit theorem states that the\ndistribution of the sample mean of a \u201clarge\u201d sample from any distribution should be normal. How large\nthe sample needs to be depends on the distribution, but Figure 1 demonstrates that sample means of\nn = 100 samples from an exponential distribution already appear to fairly normally distributed with\nn. This\nthe SEM as predicted to be the standard deviation of the exponential distribution divided by\nmeans that we can use the procedure above to find confidence intervals.\n\n\u221a\n\nConfidence intervals for the mean of an empirical distribution Up until now, we have considering\nestimating parameters from theoretical probability distributions, such as the normal distribution or\nthe exponential distribution. We\u2019ll now consider how we can estimate the parameters of an empirical\ndistribution, i.e. real-world data, from a sample of that distribution.\n\nAs an example, we will take the population of times between making a reservation and the time of the\nreservation itself in Japanese restaurants using the \u201cair\u201d booking system. The full population contains\n92378 times (Figure 4, left) and we\u2019ve created a random sample of 1000 of these times (Figure 4, right).\nIn real life, if we had the full set of data, there would not be any point in creating this random sample\nof times, but we do so here to demonstrate how well we can estimate confidence intervals. From now\non imagine that the sample of 1000 times is all that we have available to us. It\u2019s important to notice\nthat the distribution of the sample resembles the population distribution, even though it is rougher.\n\nTable 1 shows the summary statistics for the population and the sample. We can see that the estimates\nfor the mean, standard deviation and centiles from the sample are all similar to the true population\nvalues. From the table we can see that the population mean is \u00b5 = 8.30 days and the sample mean is\nx = 8.06 days. The sample mean would be di\ufb00erent if we\u2019d happened to have taken a random di\ufb00erent\n\n5\n\n020406080100Preparation time (days)050001000015000200002500030000NumberPopulation020406080100Preparation time (days)0100200300400500Sample n = 1000",
        "lecture": "Con\ufb01dence intervals",
        "page": "4"
    },
    "Con\ufb01dence intervals_5": {
        "content": "Table 1: Summary statistics of population and sample of preparation times, generated by the pandas\ndescribe function.\n\nPopulation\n\nSample\n\n92378.00\n8.30\n25.65\n0.00\n0.21\n2.08\n7.88\n393.12\n\n1000.00\n8.06\n27.72\n0.00\n0.17\n1.96\n6.92\n364.96\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\nsample.\nFrom the summary statistics from the sample, we have x = 8.06 days and the standard deviation\ns = 27.72 days. Our estimator for the mean is \u02c6\u03d1 = x = 8.06 days. Our estimator for the standard error\n1000 = 0.88 days. The 95% confidence interval for the mean in\nn = 27.72/\nin the mean is \u02c6\u03c3 \u02c6\u03d1\ndays is therefore ( \u02c6\u03d1 \u2212 1.96 \u02c6\u03c3 \u02c6\u03d1, \u02c6\u03d1 + 1.96 \u02c6\u03c3 \u02c6\u03d1) = (6.34, 9.78).\n\n= s/\n\n\u221a\n\n\u221a\n\nReporting confidence intervals When reading scientific papers, there are various ways of reporting\nconfidence intervals:\n\n\u2022 M=8.06, CI=6.34\u20139.78. Here \u201cM\u201d stands for mean and \u201cCI\u201d stands for confidence interval.\n\n\u2022 8.06 \u00b1 1.72 (95% confidence interval)\n\n\u2022 8.06 \u00b1 0.88 (\u00b1 1 SEM). This is a 68% confidence interval, though the confidence interval isn\u2019t\n\nspecified in terms of area under the curve.\n\n\u2022 8.06 \u00b1 1.76 (\u00b1 2 SEM).\n\n4 Video: Bootstrap estimation of confidence intervals\n\nPrinciple of a bootstrap estimator We want to estimate the standard error in an estimator. In the\ntopic on sampling, we have already seen what happens when we sample a mean repeatedly from a\ntheoretical distribution, and that this can give us a measure of the standard error of the estimator.\n\nThe name \u201cbootstrap estimator\u201d arises because it appears to do something physically impossible, such\nas \u201cpulling ourselves up by our own bootstraps\u201d. (Equivalently we could pull ourselves up by our\npigtail, Figure 5).\n\nIn a bootstrap estimator we treat the sample that we have available as a population, and resample from\nit to give the sampling distribution of the estimator. From the sampling distribution we can compute\nthe standard error of the estimator. It feels as though we shouldn\u2019t be able to treat the sample as a\npopulation, but it works because if we have a large enough sample, the distribution of the sample will\nresemble the population itself.\n\n6\n\n",
        "lecture": "Con\ufb01dence intervals",
        "page": "5"
    },
    "Con\ufb01dence intervals_6": {
        "content": "Figure 5: Bootstrapping: Baron M\u00a8unchhausen pulls himself and his horse out of a swamp by his pigtail.\nPublic domain image from Wikipedia\u2019s article on bootstrapping.\n\nBootstrap procedure for finding a confidence interval for the mean We will start with a large\nsample n from the data, which has a mean x. By large, we mean large enough that the sample resembles\nthe population distribution. Of course, this is not possible to know exactly, so the larger the better. We\ndecide to take B bootstrap samples. Common numbers are 1000 or 5000, or 10000. More samples are\ngenerally better, but bootstrapping can be computationally expensive, and fewer samples can also give\nreasonable results.\n\nHere is the procedure:\n\n\u2022 For j in 1, . . . , B\n\n\u2013 Take sample x\u2217 of size n from the sample with replacement\n\u2013 Compute the sample mean of the new sample x\u2217\nj\n\n\u2022 To compute the bootstrap confidence interval, we find the centiles of the distribution at 100\u03b1/2\nj in order from lowest to\nj at k = B\u2212\u03b1(B+1)/2\n\nand 100(1 \u2212 \u03b1/2). We can do this by arranging the sample means x\u2217\nhighest, and pick x\u2217\nto be the upper end of the CI.\n\nj at k = \u03b1(B+1)/2 to be the lower end of the CI and pick x\u2217\n\n\u2022 We can also compute the bootstrap estimator of the variance of the mean:\n\ns2\nboot\n\n=\n\n(cid:80)B\n\nj=1(x\u2217\n\nj \u2212 x)2\n\nB \u2212 1\n\nThe advantages of the bootstrap procedure are that we can use it for any estimator, e.g. the median,\nand that we do not need to make any assumptions about the distribution of the estimator.\n\n7\n\n",
        "lecture": "Con\ufb01dence intervals",
        "page": "6"
    },
    "Con\ufb01dence intervals_7": {
        "content": "Figure 6: Demonstration of bootstrap mean applied to restaurant reservation time data (Figure 4). The\ntop row shows the distributions obtained from the first 3 of 10000 bootstrap samples. Although the\ndistributions are similar to each other, they are not exactly the same, and the sample mean of each is\ndi\ufb00erent. The bottom figure is the distribution of all 10000 of these bootstrap sample means. The mean\nof the original sample is shown, as is the 95% and 80% confidence intervals.\n\n8\n\n050100x*0200400x*1 = 7.19j = 1050100x*0200400x*2 = 7.45j = 2050100x*0200400x*3 = 7.20j = 36789101112X* (days)0100200300Mean = 8.0695% CI = [6.46, 9.90]80% CI = [6.97, 9.22]Distribution of 10000 boostrap means",
        "lecture": "Con\ufb01dence intervals",
        "page": "7"
    },
    "Con\ufb01dence intervals_8": {
        "content": "Example of bootstrap estimator applied to mean We\u2019ll now apply the bootstrap estimator to give\nus a confidence interval for the mean (Figure 6). For each of our 10,000 bootstrap samples, we\u2019ll\nresample 1000 samples with replacement from our sample of 1000. Each of these samples will be a\ndistribution (top row of Figure 6), from which we can compute the 10,000 bootstrap means. Then\nwe\u2019ll plot the distribution of the bootstrap means (bottom row of Figure 6) and find the 95% and 80%\nconfidence intervals. In this case we can see both the 95% and 80% confidence intervals contain\nthe population mean (8.30, Table 1). However, if we replicate the experiment with a di\ufb00erent initial\nrandom sample of 1000, in around 5% of cases we should expect that the 95% confidence interval does\nnot contain the mean.\nWe\u2019ll leave this as a lab exercise for you to implement, though you will find that you get di\ufb00erent\nanswers for the confidence intervals, depending on the state of the random number generator.\n\nComparison of bootstrap confidence intervals with normal approximation The 95% confidence\ninterval obtained via the bootstrap procedure is (6.46, 9.90) days, which is very similar to the confidence\ninterval obtained by the normal approximation, (6.34, 9.78) days. The bootstrap interval is slightly\nshifted to the right, suggesting that the normal approximation is quite accurate at a sample size of\nn = 1000.\n\nGeneral formulation of bootstrap estimator A great advantage of the bootstrap is that we can\neasily apply it to statistics other than the mean. Here is the general procedure for estimating the\nconfidence interval of a generic estimator \u02c6\u03d1:\n\n\u2022 For j in 1 . . . B\n\n\u2013 Take sample x\u2217 of size n from the sample with replacement\n\u2013 Compute the sample statistic of the new sample \u02c6\u03d1\u2217\nj\n\n\u2022 Then compute the bootstrap estimator of the variance of the statistic:\n\ns2\nboot\n\n=\n\n(cid:80)B\n\nj=1( \u02c6\u03d1\u2217\n\nj \u2212 \u02c6\u03d1)2\n\nB \u2212 1\n\n\u2022 To compute the bootstrap confidence interval, we find the centiles of the distribution at 100\u03b1/2\n\nand 100(1 \u2212 \u03b1/2).\n\nThis procedure works well for measures of centrality such as the median, and for the variance. It\ndoesn\u2019t work so well for statistics of extremes of the distribution, such as the maximum or minimum.\n\n5 Video: Interpretation of confidence intervals\n\nInterpretation of confidence intervals Although we have only computed confidence intervals in a\nsimple artificial example, we are already at a stage where we can consider how to interpret confidence\nintervals. From Equation 1 we can see that confidence intervals are a random interval \u2013 whenever we\ntake a new sample, we will end up with a new interval, as illustrated in Figure 1. The interpretation\n(according to the frequentist interpretation of statistics) is that if we performed a long run of experiments\n(i.e. repeatedly took samples) the parameter (the mean in this case) would be in around 95% of the\nconfidence intervals.\n\n9\n\n",
        "lecture": "Con\ufb01dence intervals",
        "page": "8"
    },
    "Con\ufb01dence intervals_9": {
        "content": "How big should a confidence interval be? Should we choose the 95% confidence interval or the\n80% confidence interval? The answer to this question depends on the problem. For example, suppose\nwe have a machine that makes tens of thousands of ball bearings for aircraft jet engines every day.\nEach ball bearing needs to have a diameter of 2 \u00b1 0.0001mm for the engine to work safely. We measure\nthe diameter of a sample of the ball bearings every day. Because this is a safety-critical application, we\nneed to have high confidence (say 99.999%) that the ball bearings are in the range 2 \u00b1 0.0001mm. This\nmight require a large sample size, but it\u2019s worthwhile because the consequences of getting it wrong\ncould be catastrophic.\n\nOn the other hand, suppose we are estimating the number of red squirrels in a population so that we\nknow how much red-squirrel friendly food to put out for them over winter. We might want to leave\nout a bit more than we expect they need, we\u2019re happy to accept a 10% chance that the true number\nof squirrels might be greater than the upper end of a confidence interval, so we compute the 80%\nconfidence interval, and put out enough food for the number of squirrels at the upper end of the interval.\nThere\u2019s a 10% chance that we might not be providing for enough squirrels, but it\u2019s not as catastrophic\nas in the aircraft situation (depending on how much you value red squirrels compared to humans).\n\nUpper and lower confidence bounds\nIn this case, we\u2019re not worried about our estimate being too\nlow, so we only need to compute the upper confidence bound \u2013 we would quote a mean number of\nsquirrels and an upper limit.\n\nAside: Capture-recapture Suppose we want to estimate the number of squirrels N in a population.\nWe can do this with a clever method called capture-recapture:\n\n1. Capture n of the squirrels, tag them so that they can be identified if caught again, then release\n\nthem.\n\n2. Wait for the squirrels to move around.\n\n3. Recapture K of the squirrels and record the number k of these recaptured squirrels that have tags.\n\n4. The estimator of the number of squirrels in the population is\n\n\u02c6N = nK\nk\n\nThis should work if the capturing and recapturing processes are random. If this is the case, the\nexpected proportion of tagged squirrels in the whole population n/N is equal to the proportion in the\nrecaptured sample k/K, hence the estimator. It\u2019s possible to derive theoretical confidence intervals for\nthis estimator.\n\n6 Video: Confidence intervals on the mean for small samples\n\nSmall samples We\u2019ll now consider the distribution of the mean based on a sample of a \u201csmall\u201d\nnumber (usually n < 40) of data that appears to be distributed normally and whose variance we are not\ngiven \u2013 we can only estimate it from the data.\n\nFor example, suppose we want to estimate the mean weight of a population of female squirrels from\na sample of n = 32 squirrels (Wauters and Dhondt, 1989). The sample mean is x = 341.0g and the\nestimated standard error in the mean is \u02c6\u03c3X\n\n= 3.9g.\n\n10\n\n",
        "lecture": "Con\ufb01dence intervals",
        "page": "9"
    },
    "Con\ufb01dence intervals_10": {
        "content": "Figure 7: The t-distribution for 3 degrees of freedom and 10 degrees of freedom, with normal\ndistribution for comparison. 2.5% t critical values and z critical values are shown.\n\nWe can imagine that if we had taken a di\ufb00erent sample of n = 32 squirrels, we would have found\nboth a di\ufb00erent sample mean and a di\ufb00erent estimate for the standard error in the mean. Thus, the\nstandardised statistic (X \u2212 \u00b5)/ \u02c6\u03c3X itself contains two statistics, X and \u02c6\u03c3X, which are, in general, random\nvariables derived from the sample. As we are estimating the standard deviation, rather than knowing it,\nthe normal approximation to the distribution of the mean begins to break down.\n\nThe t-distribution We could use the bootstrap estimator to estimate confidence intervals. However,\nin this special case, there is another option. There\u2019s a theorem that states that when X is a random\nsample of size n from a normal distribution with mean \u00b5, the random variable\n\nT = X \u2212 \u00b5\n\u02c6\u03c3X\n\nis distributed as a t-distribution with n \u2212 1 degrees of freedom, where the t-distribution with \u03bd degrees\nof freedom has the probability density function depicted in Figure 7, which is given by the equation:\n\np\u03bd(t) = 1\n\u221a\n\u03c0\u03bd\n\n\u0393((\u03bd + 1)/2)\n\u0393(\u03bd/2)\n\n1\n(1 + t2/\u03bd)(\u03bd+1)/2\n\nwhere \u0393(x) is a gamma function. We will not prove this theorem here; in Modern Mathematical\nStatistics with Applications Section 6.4 there is the sketch of a proof.\n\nThe t-distribution is very similar in shape to the normal distribution: it is bell-shaped, symmetrical,\nand centred on 0. However, for small numbers of degrees of freedom, has longer tails. This means that\nthe tails contain more of the weight of the distribution. We define the t critical value t\u03b1,\u03bd as the value\nof t in a t-distribution with \u03bd degrees of freedom which has the area \u03b1 under the curve to its right.\n\nFor small degrees of freedom, the t critical values are considerably bigger than the z critical values of\nthe normal distribution (Figure 7). As the number of degrees of freedom increases, the t-distribution\napproaches a normal distribution. The distribution with 40 degrees of freedom (not shown in the figure)\nlooks very similar to a normal distribution.\n\n11\n\n432101234t0.10.00.10.20.30.4pz0.975t0.975,10t0.975,3z0.025t0.025,10t0.025,3Normalt, 10 d.ft, 3 d.f.",
        "lecture": "Con\ufb01dence intervals",
        "page": "10"
    },
    "Con\ufb01dence intervals_11": {
        "content": "Table 2: Abbreviated table of critical values for t and z distributions.\n\n0.100\n\n0.050\n\n0.025\n\n0.010\n\n0.005\n\n0.001\n\n\u03b1\n\u03bd\n\n3.078\n1\n1.886\n2\n1.638\n3\n1.533\n4\n1.476\n5\n1.440\n6\n1.415\n7\n1.397\n8\n1.383\n9\n1.372\n10\n1.325\n20\n1.310\n30\n40\n1.303\n\u221e 1.282\n\n6.314\n2.920\n2.353\n2.132\n2.015\n1.943\n1.895\n1.860\n1.833\n1.812\n1.725\n1.697\n1.684\n1.645\n\n12.706\n4.303\n3.182\n2.776\n2.571\n2.447\n2.365\n2.306\n2.262\n2.228\n2.086\n2.042\n2.021\n1.960\n\n31.821 63.657\n9.925\n6.965\n5.841\n4.541\n4.604\n3.747\n4.032\n3.365\n3.707\n3.143\n3.499\n2.998\n3.355\n2.896\n3.250\n2.821\n3.169\n2.764\n2.845\n2.528\n2.750\n2.457\n2.704\n2.423\n2.576\n2.326\n\n318.309\n22.327\n10.215\n7.173\n5.893\n5.208\n4.785\n4.501\n4.297\n4.144\n3.552\n3.385\n3.307\n3.090\n\nLooking up a t critical value (added after 2021/22 lectures) To look up a t critical value, you can\nuse the python scipy package. For example to find t0.025,10 you would use:\nfrom scipy . stats import t\nalpha = 0.025\nnu = 10\nt_cv = t(nu ). isf( alpha)\nprint (t_cv)\n\nYou can also look up t critical values and z critical values in statistical tables, such as the ones in\nthe appendices of Modern Mathematical Statistics with Applications. Table 2 shows an abbreviated\nexample of such a table. Each row contains t critical values for degree various levels of \u03b1. The final\nrow, with infinite number of degrees of freedom, is the z critical values for these values of \u03b1. The full\ntables include values for more degrees of freedom.\n\nUsing the t-distribution to derive a confidence interval The 100(1 \u2212 \u03b1) percent confidence interval\naround a mean x of a sample of n values with estimated SEM \u02c6\u03c3X derived using a t-distribution is:\n(x \u2212 t\u03b1/2,n\u22121 \u02c6\u03c3X , x + t\u03b1/2,n\u22121 \u02c6\u03c3X)\n\n(4)\n\nNote that we have used the t critical value t\u03b1/2,\u03bd. Here the number of degrees of freedom is one less\nthan the sample size (\u03bd = n \u2212 1). Also, we have divided \u03b1 by 2 because we are wanting upper and lower\nbounds to the confidence interval. It might be that we only need an upper bound, as we considered\nwhen we were estimating squirrel numbers earlier. In this case we would just quote x + t\u03b1,n\u22121 \u02c6\u03c3X. This is\nstill a 100(1 \u2212 \u03b1) confidence interval, since the interval from \u2212\u221e to the upper bound contains 100(1 \u2212 \u03b1)\nof the area under the t-distribution.\n\nTo continue the squirrel example, suppose we want to find a 95% confidence interval for the weight.\nThe 95% confidence interval implies \u03b1 = 0.05 and \u03bd = n \u2212 1 = 31. We would then look up the\nt0.025,31 = 2.040 and substitute it into Equation 4 along with the sample mean and estimated SEM.\nand then use this to generate the confidence interval, which we could quote as \u02c6\u00b5 = 341.0 \u00b1 8.0g\n\n12\n\n",
        "lecture": "Con\ufb01dence intervals",
        "page": "11"
    },
    "Con\ufb01dence intervals_12": {
        "content": "(95% confidence interval, n = 32). This is a bit wider than the interval we would obtain using the\ncorresponding critical value of a normal distribution z0.025 = 1.96.\n\nReferences\n\nWauters, L. A. and Dhondt, A. A. (1989). \u2018Variation in length and body weight of the red squirrel\n\n(Sciurus vulgaris) in two di\ufb00erent habitats\u2019. Journal of Zoology 217:93\u2013106\n\n13\n\n",
        "lecture": "Con\ufb01dence intervals",
        "page": "12"
    }
}